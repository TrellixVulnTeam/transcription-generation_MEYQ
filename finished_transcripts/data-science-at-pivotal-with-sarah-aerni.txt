Transcription: is the principal first I'd like to ask naive question what is data science
 I guess it's not really a naive question is the question I think that's appropriately ask because that means many things to many people at pivotal we focus on data science really being machine learning and statistics apply to very large datasets encompassing actually the underlying understanding of how to process in a parallel Fashion on the using a blank technologies that exist like I'm distributed databases or Hadoop I'm things like Spark
 are you can think of it as part of the repertoire of algorithms essentially that you're going to use so ways for you to interpret or understand what is going on the date of my building a predictive model and machine learning kind of has a lot of different algorithms and I'm lot of people working in that space developing new approaches that allow you to either combine features are attributes that you can extract out of the data about individuals for example in their behavior to predict something or two separate them into subpopulations were there was this two types of data scientist he called them analysts and Builders and make sense of the data and the builders construct software systems and models and recommendation engines does this song
 oh my God I think everyone is a different interpretation of what it means I I feel like it's hard to divorce those two pieces from each other I think anyone who operates in the space does a little bit of Statistics probably hard to not understand kind of whether or not you believe something is sound or I guess test whether or not some hypotheses that he going to spend the day to our true so I feel like you can't remove data science from statistics even if you are on kind of more of the implementation side I guess to Me Maybe that's separating somebody who's a data scientist from maybe like a I guess what I consider machine learning Engineers so if you're somebody who's maybe figure out how did Implement an algorithm at scale and not concerning yourself as much with building a model learning about the data extracting features or insights from it and then we're feeding that
 back into a loop but really more around how do I implement this particular method that I think it's just kind of crossing over into appear engineering space and I think there's a spectrum so I would love to begin to creep towards a discussion of those engineering practices what is pivotal
 sure I'm pivotal is a software company we do have a lot of Engineers those are Engineers that either dedicate their time management course split their time within pivotal Labs itself we have Christmas practice that is both outward-facing so we can salts on with companies that are trying to develop maybe a new application I'm or they need to learn how to do application development of it differently so they can work with customers or of course they can work internally on our own software that we produce I'm like Cloud Foundry I'm Winchester platform-as-a-service as well as our data products including our greenplum database is a distributed version of postgres and then Hawk which is that database sitting on top of hdfs so
 wizard of have Engineers across the board there we also have a machine learning library Mad Lib that allows you to Ferg's ample build regressions on billions of rows of tables setting up this kind of national engineering portion of of what it is that we have at pivotal and then he's open source software tools that we develop as well products one was the greenplum database and another one was a product that's built on hdfs can you describe these two products in more detail
 I'm sure I think probably best to check out to the love that I owe for all of our data stock but as far as kind of the history of pivotal itself we are composed out of assets from EMC and VMware VMware really is where Cloud Foundry for example came from I'm EMC is where we had green plum which is a distributed version of postgres of a sleep worked off of Post breast some 10 plus years ago when it became clear that running these large analytics queries on you know a single node is in trying to function as our data grow two terabytes in petabytes and so are there any server query that we run for example if you wanted to look at a large table and say it's from all the names of all the people ever on earth and then you're curious to know maybe the distribution of those names
 true a table on a single node billions of names going to be quite painful and then said he distributed data I'm in allowed to live on multiple nodes you can ask those queries to the operator on by Aldine On The Run local data so I can count the number of times it has any particular name appear at and I can be aggregated that sort of a sum of some, and that's really where I'm green plums is originating from overtime of course with the evolution of advanced analytics on top of that we work together with Joe hellerstein at Berkeley to create a madlib which is an open-source machine learning library and that is really the foundation of kind of the machine learning side of the house that we have we're all to mately it allows us to build models I'm off of extremely large tables that now rather than no only containing kind of information that you might be wanting to run a spoon considered traditional kind of bi
 Amazon which are course very important to understand what's happening your data that actually build a predictive model for example predicting on how long a patient would be in the hospital using a lot of historical data and how much how much do your Enterprise customers need to know about computer science and machine learning in order to leverage these products really too much so it depends on the expertise level or kind of the the set of tools that they have that they work with so we have people that really just no point Tableau at it and use it that way so I think someone who doesn't need to really understand much except you know he was a table and I want to visualize something in it the other side people were engineering a new product and Ann are potentially trying to figure out how to build a new algorithm Sobe The Other Extreme we probably have a very good understanding and it hurts
 I'm fishing today to sign space you want to use things like Mad Lib and who want to ride sequel queries and although you don't necessarily need to know everything about the underlying architecture because really you're just writing postgres queries and allowing the query Optimizer to figure out what's the best thing to do given the way your data shaped it is of course always beneficial to be a bit faster and certainly any instances that we run into a times where an algorithm may not be already available in Mad Lib and if you want to use python or are and all of its libraries we have the ability to use procedural languages I'm since again bring home is postgres I'm to run R and python in a parallel architecture meaning lot of smaller are in Python jobs we've done it you know remember running millions of smaller models in parallel across to my notes
 I think the way used to be in the high performance Computer World where you may be needed to know how to multi thread a process lock bread-and-butter can you get information across I've got sort of all still removed and and you can operate seamlessly on the data for an Enterprise customer that wants to start using greenplum or another pivotal product
 we have a variety of ways but I'm boarding people and again I think probably best discussed by our product team I'm outside we do a lot of training for the data scientist to enable them by I'm offering training directly on their data if they'd like so we can build models with them we can help run kind of a hackathon on data or data gym where we teach them how to use a tool teach me how to use other tools on top of it and then have them go off and try and do something really cool in a week or a few days to sort of experience at machine learning library and the adoption of distributed data platform spark have enabled these types of machine learning library can you describe how these machine learning libraries take advantage of a tool like Spark
 so I get it if we're just going to generalize to how be distributed compute paradigms work and not necessarily worry too much about whether it's permitted in memory system Mariah Moore traditional mapreduce where it's writing to disk but the concept of being able to just give me data across nodes and operate on top of that I'm some of them allowing you to be quite in control about how data gets moved around and others where it's a little bit more seamless a task or when you're trying to figure out what the libraries in all of these invitations is how to break a problem up into a lot of what will call him very soon we parallel jobs so essentially what you have to mention the other billion
 a billion roses near interested in running a linear regression on top of that there are certain pieces of the linear regression where you're doing computations on each row that can be operated on in parallel across all of your nose meaning for example of the product of a feature Vector against itself on which is one step that can be run parallel you know whether that be kind of splitting it up using spark or whatever it is that you're wanting to do or so of course have pieces were ultimately the underlying chunks are running sequel queries or maybe smoke python code on top of each other they start of shift the date around at a minimum level on two then come up with the final solution
 example restorative you're trying to get the distance to a centroid for every example I'm every example doesn't need to know about another example to compute that distance 03 June Loop I'm in a band you ever get that information by Sir moving those center is around to the kind of that midpoint so it would have been distributed this case between when you would use spark and when you would use a more batch processing framework like still batch I'm in a lot of instances it's just kind of the way things run if you wanted to do some Anthem bit more like transactional and fast then you might be going to to some other sort of like in memory datagrid I think you're doing something as much as possible
 properties really you write to disk all the time I'm going to become as kind of an IO issue something about something that they could be much slower but it is you know it's it's also have to be consideration and probably worthy of testing you know until there's kind of a full understanding about what is faster or another I'm trying to test out and see what is actually faster because of course in any Paradigm you are still relying on kind of the underlined the way things have been engineered to shift and move around the data I'm which can become a network issue as well understood about Shuffle. That you have to kind of trust I'm in so decide you know what what is most appropriate I think you just to find something that that's really useful which is the idea that keeps things in memory after you've done a certain face
 of whatever pipe line of processors you're doing where is Hadoop mapreduce just finishes the the job that it's doing and then write to disk so it's Park you can create these elaborate multi-stage processes which can be really conducive to machine learning if I understand correctly it's probably best to firstly I guess the best source will be databricks as they are you know that the company that sort of outwardly promotes and and develops and works on spark I'm going to educate very broadly on what what to do but at the same time I think the use cases is a case-by-case basis I think what we acknowledge that there is kind of not at the moment A1 tool fits all and it's always important to consider what you're used case at hand is you know if there's something that simply doesn't exist currently in
 if you want to run a lot of different smaller our jobs maybe you're wanting to you know actually just process a bunch of images in parallel and batch next track features out of the appropriate for mapreduce job that really what you're doing is taking you know terabyte of images of a lot of small ones and Binoo recognizing faces on them and then just keeping them there for future use that something where you could write him a produced job and leave it you could have questions as far as well but if it's just started one step and there might be limited utility to forcing it into spark when Barker's better use that moment for something else I'm sorry I don't want to generalize I think there are probably places for a lot of the different types of paradigms out there
 you are the post recently called using data science to save and improve lives of others what is the lowest hanging fruit like what are the problems which data science can definitely help us solve in the near future
 I am Healthcare or across-the-board across-the-board whatever is the most lowest hanging fruit that comes to mind
 alright I need a lot already I guess so I feel you know it the same to you all the time anything based on behavior in this I think of this point almost commoditize it's easy you another tools out there this is almost like a moose obviously the not interesting science probably already there advertisements do you want to guess I'll help Caroline signs near and dear to my heart since I have a background in that and I think you also do so I think for me right now at the focus areas certainly I'm in the United States within the Healthcare System I think is around I'm kind of legislatively driven opportunities to improve Healthcare outcomes to lower costs
 and it's funny because I think when we think about of course from from the nice perspective and trying to improve health Healthcare outcomes you know we've done work predicting whether or not somebody will I'm crashed so they'll be severe physical deterioration of a patient seen one or transfer them into the ICU they're in the hospital I'm so they'll be taken care of in a better way so that's one of these critical I'm better than necessary Leah low-hanging fruit stand in the sense that on the other hand you might be wanting to predict whether or not a patient. Minutes of the hospital which of course is also very important piece but it's something we are really no hospitals are getting dinged for this so they need to know if they're going to discharge a patient in the hospital I'm if a patient likely to come back within a certain. Of time and therefore I will have to pay for those cost because I let I let them go to soon it's it's probably one of the most maybe not lowest hanging fruit necessarily because he could still go back to recommendation engines with even Healthcare
 but it is certainly one of the ones that's that's being driven very very forcefully by and of the requirements and reimbursement gather like a dude aggressive data science on Health Data because of the the state of confidentiality like HIPAA requirements in the back of the difficulty of of truthfully anonymizing Health Data in terms of doing collaborative data science but you know with within any particular hospital they of course have access to a lot of the data and in are able to keep it sufficiently secure I would argue that as to cure Healthcare data is I believe probably Financial Services data is even more secure
 if all Health Data was magically released tomorrow that you know and was anonymized to a successful the Greek scientist would be able to hack on it and do some amazing things
 that's an interesting question I'm assuming that we can solve for that entity resolution problem it actually recognizing patient A&P know to hospitals which is kind of in and of itself the big data science, I would argue that you know there is such sparse information captured on a lot of individual patients that it's actually not totally inappropriate to try and focus in a bit you know on on episodes of care so being able to stay what will happen this patient right now as they are in the hospital I would argue that in today State you know they're so little that we actually know for example about the role of genetic variation in disease for example in and out comes we have very specific examples of cancer can determine whether or not you can take a certain drug what kind of on a global scale
 take me to sample problem I guess if we had date on everybody and you know the genetic level in the world would that be sufficient for us to answer all of the questions no I'm sure not from the article says patient records are also growing with advances in genomics information about patients are we learning from new genomic science
 well of course we can consider that kind of familial history in the sense that we have access to what it is that they have inherited in terms of some conditions that we do know that there are four examples variance that make it extremely likely at 4 for patients develop breast or ovarian cancer over a lifetime I'm in that of course is critical because you might undergo prophylactic treatment to avoid developing that were to lower your chance at least developing that so as that information as I date of grows on example where they're very specific subcategories but there's a very famous article where a researcher Mike Snyder published everything not just his base Gino meaning to the variance but actually seen changes over time of gene expression but we can think about how much of that genome is active
 overtime that allowed him to trace I'm in addition to some other measurements for example watch himself develop or become pre-diabetic I'm in an acknowledging that you know they're there also is a. Or predisposition for him out of her a particular medication that he may or may not react well to so you know how that information we have more patience
 with that type of deal are we can try and make different decisions but hopefully I'll just try and understand you know as we look at lab values for example you know are your low platelets as a result of genetic variation door because of some sort of environmental conditions I think it's really not just the collection of you know your family history but also these things that happened to you over time tracing changes you know is your blood pressure changing a lot or is this just your Baseline I'm trying to act like that dated also critical another article is both consumer wearable and in hospital sensor data is growing in granularity increasing in scope and becoming more pervasive and quote and I'm curious if the volume of data being collected here outstrips our ability to write algorithms to actually analyze that data that's a great question and then I also definitely want to credit my co-authors here got them and Kalia and who contributed to this article and I believe one of them
 probably actually I contribute to that particular quote I think what we're seeing and this is something good to happen for a long time when you think about the foundation in the history of almost everything that we do I'm in the field of statistics and machine learning as it evolved as those data volumes grow everything that we know and understand is going to change because we're going to have access to a lot of different data sets 01 model that's going to kind of fit all if we assume you remove the housing prices well you know it might be that we can include interaction terms about the zip code for Noble San Francisco's housing market different from I don't know say Little Rock Arkansas I would assume but that we need to start Gathering subpopulations what we stayed at overtime are the algorithms we generate
 yes have to try and address the the problem of day to feel the potentially overtime or I mean I understand anomalies within the data as well that you know maybe would have tossed those out but over time is it in the volume of girl baby anomalies actually more meaningful subpopulations I may have all we might want to consider
 interesting Medical Imaging is also changing due to data science and computer is often better than a human's naked eye identifying problems human working together with the computer performs even better than currently in the state were human computer interaction is is fantastic but are we going to see if your computer is simply better than even the human working with the computer like the computer alone will just do a better job that is like a fear that a lot of people are kind of putting out there that we fear mongering sells podcast I don't I don't know I don't want you to claim anyway I think maybe you're referencing
 learning I'm That Kind of allowed us to take away when we consider I think the fundamental human element that existed in machine learning which was this concept of feature engineering Works extracting those variables that you're going to put into an algorithm you know anything about something like housing prices again let's go back to that we're doing a linear regression kind of predict how much a house is going to cost of course you'll sit there and think okay well I care about you know the number of square feet the bedrooms the bathrooms you know maybe other things that you can measure about the neighborhood and in those were things where there was his portion of human intellect that went into it so not just a human computer interaction that you're speaking to it actually the part about designing what is going into it and you know it in the space of deep learning worth of Julie saying no let's not even ask humans to try and understand the patterns that might feed into something
 welding machines actually get down to the pixels and determine what is there be on that trying to sort of have humans feedback I just be the only thing I would hope is that the desire that kind of a is built to serve people and not machine well that's not true with internet of things but still I think hopefully the Fear Part is removed but certainly removing the human intellect disagree but intellect or human error don't you think we understand psychology well enough to start of hand off respect to the computer
 do you really think of human error there or is it a human's capacity of cognitive limitations to be able to actually see certain things and I think I'm pretty sure that our computers can can you know if they can't settle on the same types of heuristics that we can right now eventually they'll be able to the near future that's interesting I look forward to it if you presented called internet if it's driven software is eating the connected world and you had a picture of an oil rig disaster how can internet of things work together with data science to prevent disasters yeah you know absolutely that is just one of the examples and I can be earlier one that I gave with kind of
 original disaster of deteriorating so crashing at the hospital and in trying to prevent that what we have is the ability to allow sensors to a word I say about what's happening around us and I think historically we would say oh you know something's changing with the heart rate let's wait until it reaches a certain threshold and alert I mean of the pressure builds up too much let's alert there might be a disaster coming but really if we have a lot of different sensors available to us rather than waiting until it's reached what we consider I guess going back to your earlier statement about a machine versus a person being able to understand and crate these rules you know we've reached a danger Point statistically at this point it's going to explode soon it said allowing all the sensors that are there predict an event that has happened in the past and trying to understand how all of these sensors together actually give information
 I'm. Can that lead to an outcome and you know I'm not talk we actually talk a lot not just about the concept of building a predictive model to say you know there was a failure and how come any of these sensors come together to predicted but actually going through into the nitty-gritty still about how their does have to be a component of human engineering of features how do you take some sort of a Time series data and turn it into something meaningful to predict an outcome so still that need actually I I look forward to. Them will have a lot to say on this space I've been doing a lot of work and I sent her space right now predicting failures and maintenance issues so it is definitely an exciting space capacity is growing at a rate that is commensurate with the sensor data growth
 and I am sure he MC could speak more to that I mean they're in the storage space and I can speak to it I myself have not paid too much attention to it I kind of Emma a user of this in front of other people to architect I'm sure that you know if we look at how much data has been generated in the past year as compared to all
 history similar to kind of human growth were looking at something pretty and then and of course there are a lot of really exciting things that have have sort of been evolving over long period of time ever going to believe in DNA Computing trying to go to store data in different ways it's not just bits not just binary but I'm kind of other ways to store data I also look forward to seeing what happened in that space But it is of course alarming rate of absolutely the predictive model
 so easy I think I'm going back to that that statement around you know how it is that you transform kind of Time series data or anything into a feature that will go into predictive model on what's interesting is there a lot of tools out there to try and stall for this problem and kind of remove that elements need to understand what it is that we're looking at and extract these features is too kind of exploring the space that try and look for interesting other dimensions to explore sort of left something that I focus on
 this week of shows on software engineering daily is about women in Tech and I'm like do you think this is a topic that to even be discussed explicitly or is it more of a topic where the way to impact the situation of a showcase talking about their technical achievements talking about any controversy or you know female marginalization I think that's an interesting perspective and I did participate in a few conversations I'm in a space in and also on you know even contributed to an interview where other women were interviewed in the space I think of anything else it is important to focus of course on all of the contributions of any individual you know regardless of who it is and what it is
 erase it sort of it relevant I think calling you know an individual out just due to some sort of an attribute of courses it's interesting I think when we focus on you know a woman did this and what that means for women it is very empowering for other women I believe who want to be in that space to then have peers to look toward but at the same time and also a course I think inherently we women tend to also become a little fearful that you know they'll be called and they were called out because they were women so I'm not sure if there's going to be a good solution to the space and I don't ever think that there is anything negative to calling out you know how a GroupMe feel or be perceived I'm to bring awareness just the same way that we try to bring mindfulness to the other elements of Our Lives for example you know Apple brings mindfulness to how much we move by putting kind of a step tracker in our phone
 so I think just for everyone to become conscious and I think in many ways anyone could be guilty of this to remind ourselves that you know everyone is just as deserving of praise for the work that they do respect of who they are and to be respectful of people's differences and and sort of just relying solely on one basis for differentiating individuals can can be dangerous Springfield I can't take this offer engineering as I'm not a surly a software engineer on the data science. I think this is something that I've called out a lot I think there are a lot of very visible women in data science monitor got any one of the idiots when it supposed to point out he was there early on with a set of male peers at LinkedIn so I think
 the space that feels very open still and I think it's important to maintain that but I wouldn't say we have an uneven Plainfield because I think I said this once before data science is specifically about you know in a lot of ways being in interdisciplinary thinking outside of the box taking someone's business expertise and some technology technological understanding and bring you that kind of to a center internal Odd West Diversey and thinking is so critical because coming up with new ways a new approaches and having those conversations were it's not just single-minded focus actually means that it is a very interesting space in which to have a lot of diverse backgrounds you know in any regard also in and what you've been trained and no are you coming in operations research in biomedical informatics bringing those groups together actually allows for a lot more ideas to flourish no longer approaches to be developed
 funniest ostracism or marginalization whatever you want to talk about it of women in I know they receive opinion this is super important because if you end up with a crappy Solution that's just hivemind e it's it's bad this ostracism of the negativity towards women even if it's saying data Sciences of field born out of universe thinking out of of you know that sense some other fields are the same I think it's just it's really bad at that field itself already is bringing together people with some of her spanking and so it's interesting
 since I always feel everything is welcome and it's not to say that people and their women that don't feel Century marginalized I'm I can only speak for myself and of course I think you know any at any respect I think anytime something occurs it's easy to fall back into the mindset of oh this is because I am but it was a really important when you have that moment Tulsa step forward and realize well you know this is me and I need not hide it I need to embrace it and allow it because of course by denying that example you're a woman and then trying to CertiFit in and set the mold you're never going to quite succeed at being something that you are not so instead of knowledge it try your best to celebrate it and focus again on the task at hand clothes off at Academia you spent a lot of time and I'm curious how the intellectual experience with an Academia compares to your
 professional career at pivotal
 so what's interesting is my team is extremely academic in the sense that we all come from strong academic backgrounds as well not happy she's Master's and bachelor's degrees across the different fields of studies and we all bring that together at Thibodaux we still have a lot of the same things that during my graduate school training them were present so we you know we do knowledge sharing we potentially lab meetings you know what it is that we worked on we ask Outsiders to come speak we still go to conferences weary papers you published papers so not sense pivotal itself specifically the data science team still feels extremely academic with that added benefit of having access to tons of data in a lot of really really great technology
 do you think I should be a closer relationship between Academia and Industry
 I mean we have extremely close relationships with akademia they mentioned you know the the Mad Lib library was developed specifically with Berkeley and we have other examples I'm Chris Rea works on top of greenplum database for deep Die For example and we do work a lot actually you know how to carburation with a researcher Harvard Medical School Tax Services in some of their research we have some other ones that are on going I think it is an interesting balance because of course the end of the day industry has to concern itself a lot West whether or not you know their profitable but at the same time if it's so fascinating to me that when we allow researchers to access our environments and no we did have a large thousand know to do cluster the analytics workbench that was open for research and we did have a lot of research organizations working on top of it
 so what you allow kind of young developing minds are researches in a space to think about new paradigms and not have to focus on I have to get this Grand in order to get the software it's so interesting cuz I think a lot of pills are kind of stuck to thinking in a in another Paradigm because of course that's what they have access to and so they engineer where they build algorithms are they come up with ways to process their data in the Paradigm that they have access to and if you open their minds with new paradigms access to new technology technology that all shifts to work in Academia because you can be doing quote basic science
 I did I did a lot of basic science and I think the concept of of it being more noble is is probably up to the person who's passionate about that topic so I would say that there there's a lot of there's a lot of very interesting work that happens at a basic science level that I think absolutely needs to continue occurring within Academia there is a lot of basic science that occurs with an industry for example the pharmaceutical industry but I think those two spaces interact a lot of things that happened I think that are driven by companies that are concerning themselves as being profitable again going back to the example of you know the iPhone or Fitbit or anything that's out there that's trying to track yes you know what is core it's concerning yourself with how how do we become profitable but the things that come out of it I'm researching understanding of movement potentially looking at multiple sclerosis patients and seeing the way they are
 I think what's so fascinating is the way in that space I'm even though it's driven of course by you know being profitable at the same time there are a lot of really really interesting contributions that come out of it too
 interesting Sara errani thanks so much for coming on software engineering daily it's been fantastic talking to you thanks so much for having me
