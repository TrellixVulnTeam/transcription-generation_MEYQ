Transcription: what is Aria is the original creator of apache-spark and a co-founder of databricks which provides data science powered by spark Mateo welcome to software engineering daily
 spark is a parallel Computing engine that runs on clusters of many machines and can be used for data processing ocean behind creating spark basically if we wanted to make it easier to process large volumes of data which you know I agree, and Boston in a lot of commercial applications but also increasing leagues in research so at the beginning programming models for large clusters of of machines are through mapreduce but as more people started using this technology and especially you know people who wear in software engineering necessarily or who you don't have other skills as he wanted to make it a lot easier to work with this kind of data sets
 there is a spark job than a Hadoop job
 spark it actually quite a bit faster than than traditional Hadoop mapreduce for a few reasons one is the ability to do a lot of the computation in math and the other one is that the engine understand more General communication patterns and and patterns of basically dependencies between tasks in the in a job so we've seen on disc with just data dots on disc we've seen her to go as much as 10 times faster and in some applications just because it understands the structure of the job better and if you also use memory and there you know keep parts of your data can fit in memory it's going to be a hundred times faster as well it depends of course on the application with Hadoop mapreduce every operation is actually three operations you got map Shuffle and reduce and so do you have in higher level operations in to do if you actually have to do 3 and operation
 streaming free Mark Sykes Park you have a final degree of granularity and you can simply chain operations together along a directed acyclic graph is this correct yeah that's exactly right so basically math pages many applications need to take multiple math pages steps and it's very expensive to pass data between the steps because you have to basically distributed file system and replicated across steam machines and then later you load it all back and Spark understands you know what the entire sequence of steps and can just push the steel between them in memory Primitives rather than Hadoop stew stage disc-based mapreduce Paradigm why wasn't Hadoop created with an memory processing power yeah I think that's a good question I think that there were two reasons for this so first when when I do Penn and mapreduce came out memory sizes you know
 machine and an $4 cost wear a lot smaller so it cost of memory has actually continue to fall very quickly with with Morris and suddenly you don't know a lot more data sets can reasonably in memory than they did back then so that's one of the reasons that the hard way back that wasn't like you know good enough and cheap enough to be able to do this but the second recent applications back then we're just simpler and they what is batch processing applications where in many cases you know you didn't need to do too many map is your stepsons overhead of it wasn't such a huge deal because you just let it run overnight and you know in the morning you got back an answer and that's all he wanted and again people move to words applications that they want to be much more interactive like there's a person sitting there asking questions and they want to get an answer back you know a few seconds later
 what does real-time data coming in and you want to act on it right away is a better or more appropriate tool meant to be a generalization of so you can still do all the things you could do with math pages in spark but you can also do other things other types of applications that are harder to do to express efficiently with mapreduce so that way it's meant to be a superset of Mariah Peters also contains a lot of other components dough and you know these are things that you say my map interstates are fine example Hadoop also comes with a distributed file system hdfs and you can't replace that with spark spark you know doesn't have a file system it isn't it it's only
 engine but you can connect Sparks hdfs and use the opposite ends of a gradient of processing speed what are the parameters that can be tuned to explore the different sides of his gradient so basically in spark you can control so you can run these these jobs that you know that. Process someday and depending on how often you on these jobs you can get either something what you want you know you have a large data job a single set of operations that process is the whole data at once and that's a batch job or you can hunt smaller ones continuously as data comes in and you know each time new day.
 can you update the state and memory that's tracking whatever your application wanted to attract and that's how you can have my streaming application that keeps updating State as new date arrives so I'm so basically you can use spark and both ways the engine supports are running really small low latency jobs it also supports are in a large jobs that offer many hours has to deal with things daily and restarting in the middle of the job and so on so it's kind of this job site that's the main parameter datasets I think that's pretty hard to tell actually things
 yeah I think that's how it is exactly you know companies and organizations that use that basically use only spark that don't don't use my mapreduce at all even you know for the lightscale batch processing and we see this as specially and ones that just recently started you know building say big data pipelines are parks and for them it's easier to just grab and and do spark for everything. Also companies that have map it is all ready for some jobs and then they bring in spark mostly for new applications which might be on on smaller datasets why they need more into activity but the Lord response jobs are you know how you guys are 8000 nodes running kind of just as a single Gloucester running in a single jobs and the largest jobs you've seen our jobs are multiple terabytes of
 in 4 weeks so by do actually I forgot if it does by do or Alibaba but one of these companies and basically take to transform into features for machine learning algorithm and a processed in one-plus petabytes of data over the course of a week just going to have to do that I heard the podcast where you mentioned that is I think I already see podcast that you said it was Ali Baba and you said the wrong thing it took 3 days do you have any estimations on the financial cost of that kind of processing so yeah I mean it's expensive so I think that was like oh I forgot how many nodes in the wind. Cluster but you know when you get like a thousand keeping them hunting you know where is thousand of dollars
 so it is definitely expensive it in these companies like you know generally they said because they do many different applications using the same data and you know it's me basically that many parts of the business at 10 use that they tied together yeah I can't really speak to this but I like how efficient do you think the the processing queries that though they were running to do that image processing where I could they have saved a lot of cost just buy like improving the at the way that they were the way that they were processing those images I don't know too much about this specific application but in general it depends on it depends a lot on the application so basically you you need to do some for this particular one image processing is actually very cpu-intensive and there are really good libraries out there that user
 modern CPUs efficiently that use multiple cores or vectorization SSC things like that so you really need to use one of those libraries in your spark job they got the best performance otherwise if you just like right at you know what kind of naively by hand you can throw lots of computer is already but the efficiency for computer will not be very high so I think my guests for this is. Do you know that so many of these libraries that called me they are using one of these but I don't know the specifics in and other applications you know especially if there aren't really great standard libraries for what you want to do there's a trade-off between do you spend a lot of time and you ain't gotta like buildings are super efficient Library which cost you you know a person like human time or do you know you had something quickly and then you can on it and see how well it really does so actually the people die
 this is more valuable especially if this is like a one-off sing. They're not sure they want to do it again so they focus on on getting something built quickly and then if it works well and there's a time this again everyday then they focus on optimizing at minimum viable migration if a company has just a batch framework and they want it they want to implement some sort of streaming into their workflow how painful is the minimum viable migration in terms of cost and developer time I think that depends a little bit and why there is there batch job they have is is written in spark or it's raining in mapreduce or some other framework so if it's written in the AP either the programming interface for spark streaming is very similar to the interface for the batch jobs and you can offer to use a lot of the functions you have in your batch jobs and apply them to us
 I'm in Staten is very similar operators and so on so you did you know how the streaming model how it works how is that what slightly different about it but you can I use a lot of the same card and in fact you can you find tell it like for instance like split the stream Interstate 10 minutes windows or something and they're on my batch job on each window that's that's very easy to see if they're coming from a different family than they need to actually basically right there computation in spark and it is the good news there is it possible to use for example all of the inputs formats for Hadoop which are all of the other interfaces for her due to access storage so you can still use the same day and the same form you got a deck and with a bit of work it's possible to use a lot of the map and reduced function you lied as well but that I think takes more engineering cuz you have to learn all of spark and
 yeah but we definitely seen people do that process is that a competitive differentiators for Spark versus Storm or Samsung
 yeah I think so I think I specially you know for people who do you know what they say you develop your analysis as a batch job and then he wanted to create a streaming job or say that even you you do the analysis interactively so like for instance from the Sparks interactive shell like you don't like the python interpreter or something like that and then you can type in some coding and you can take that code and move it into a year I went to a job that you aren't they are directly so that you need to think about it and it's not just the migration but it's also maintaining the code after making sure you got the same answer from your batch job and your streaming job and you know fixing bugs and both of them and so I'm offering you you know it it's pretty hard to maintain two separate systems and keep them in Cinco and I slightly different storm and or or even apache-flink if you want to go there
 what do you think would be the most interesting compare comparator to to discuss with spark princess to look at so what is it in terms of being a unified framework to do both you know batch processing interactive processing and streaming spark is the only one of these systems that can actually do all of these things in some of these engines you can use the same engine as a fork for battered streaming but actually you can't combine the two and a job when I get a different day. But in spark you can actually combine all of them in a single applications of example some things you can do in spark that are really hard and the others are you can take the code for your batch job and run you know basically almost the same code on a stream and NPR streaming results or you can
 when is streaming application updates I'm staying to know keep track of some State and memory like what are users doing on your website and then start running interactive quiz on that stage in the same in my mind I just disconnect like our jdbc server and start on in Christ so that's that's unique to spark because the engine understands all these and they're all designed to interoperate they follow kind of the same a p I and II difference comes in on the streaming site so if you look at the streaming side the model for steam processing in spark is so different from traditional stream Parton singing in in some interesting ways so traditional stream processing engines have these continuous are operators where you push data CEO and The Operators that are long-lived at they have state you know where sociated with them and you keep pushing to post to dislike fixed graph of operators and they updates I'm staying Thursday
 so you know that. Models I pretty simple to understand but it's actually pretty hard to do operation only because fault tolerance becomes really difficult when one of these goes down everything Downstream and Upstream is affected and you need to pretty complicated method to recover things like Dynamic planning like scaling you know it increasing the number of nodes in your class there are things I had become very difficult as well because it's a fix graph and also throwing in new Quasar these interactive quais as I mentioned it is not really possible because you decided on this graph of operators in advance so she different is this sequence of Base short jobs mapreduce like jobs that share data and memory between them so it means we can adapt to changing workload patterns match
 easily. Because you know if I you know if we see that it's a lot more day. You can just laundry job with more tasks and dynamically change you know the level of parallelism and so on and means it's also easy to throw in the other jobs. Duke interactive Quasar like some ad-hoc queries that you hadn't anticipated before and you got actually much stronger fault a couple a guy and he's because it's the same like deterministic fall take away that you got with batch jobs commercialization of spark what year what is the describe the product offer in a date of bricks as a service platform that makes it very easy to learn spark and and in general to build a big data infrastructure as a team so the thing at providers that provide cluster management is very easy
 blind-spot clusters you know have multiple car stereos for different users scale them up and down elastocrete and so on and then it provides this this interactive collaborative workspace on top where multiple team members can connect to one of the sparkle stories ranked where is a shirt like the the outputs they produced and Asher visualization as well so is this interactive notebook like environment with this feature is built on top of it affected uses of spark that you saw was people who are writing spark back ends with web app front end that was weird was that based on separation like you were surprised by how many people doing this let's just build a company around it
 Dennis part of it yeah I think early on when we we started divided we just didn't yeah we we we we hadn't talked too much about this but about this you guys but it makes a lot of sense and basically people do you know people who creates I could domain specific application like Siri is an application for understanding I don't know like you know videos viewers are for understanding like ad clicks on a social network or something like that and when their application needs to our new craze it starts to respond last night and it actually can you deposit a lot of data interactive leaks so we wanted to enable things like that it's not the only thing we do that I mean we also just wanted to make it very easy to draw Restaurant Sparks so you can just you know build a job and diploid and not have to worry about managing her class steroid configuring at doing that for the hardware and so on but I definitely
 in about this how does databricks integrate with Cloudera
 which Kardashian the product isn't asking about the product to her about the company platform where I I have some some of my date on the Cloudera platform and somebody on the date of bricks platform is there any point of integration or is it are they kind of mutually exclusive if you have cloud in Amazon ec2 which is why we are you can easily connect to any dinosaurs that sparked understand including hdfs clusters and hbase and all the other storage on gender and Busters and pulling out the data that they wants to work with star interactively into database so you know isn't doesn't lock you in
 specific form of Stone Age It's meant to work with any common form of storage that's available in the cloud so you can just bring in the data from all of those and combined it because I've been doing these interviews with big data companies in we've moved to develop this kind of ecosystem where you have these different like big data service providers and it's really nice that it seems to be like they're all copacetic right they all integrate with each other well in this like nice collaborative well it's not like this no animosity landgraab type of situation it's it's kind of it's It's not like completely the truth about it but there's a lot of interoperability I think all the vendors of these platforms would prefer if you could remind more more
 Austin to their platform and some of that happens because they have for instance like you know they they have their own save storage format for day that I can file format or something like that and they each have kind of their preferred one but because they're all shipping you know this open source software and and there are very well-defined interfaces to to access their stuff it's pretty easy to use anyone's software taxes owed of anyone else's data and I think that's really great for you know for users of the software and it is basically leads the vendors to do you know that they want you know I got Bizness by kind of locking you in but they'll get it by actually having you know I better product with something. You you really want to buy it so I think it's it comes a lot from the open-source aspect of writing this never happened with safe commercial databases because like the whole purpose of a database is like it's this complete you know in clothes
 system that's like totally you know it only works with that same vendor more or less
 manage Big Data stack you know you've got confluent for Kafka databricks work for some spark cardio what is the end-user the current end user experience for for somebody to me that is managing all these different systems at a company and and how do you how do you project that to change into the future energy needs now is actually fairly complex it's even actually if you have even just a single vendor but if you have a distribution that comes with many components in at like if you know is one component for stream processing 1/4 bad you know when you have hives for Mater Dei that stuff like that so it is pretty complex and I hope that this stuff gets simpler but now basically you know you need to learn how to how to manage each of them this thing we're doing
 thanks but I think this will be a general trend is we we kind of sad for from the beginning and that allows us to do a lot more of the Management's kind of for you so you know which basically with all these cloud services not just ours but you know how many of the other is there someone else who has arms people who are keeping the system up and running all the time so you just focus on on using it and and you know figuring out like you know whatever happened caitians you want to build so I do think this will be the main way to simplify things is you if you can offload the management of it to one of these Cloud vendors and then the question is how how these are going to integrate with each other and that's a really good question I think you know there is just because people asked where there's a lot of integration happenings but you know that that remains to be seen
 Sky's tutu to work for clients
 well the vendors Idaho do panthers really don't date they don't actually provide apps for clients and basically want to be in that business because it's it's a business that's like basically it's it's it's very easy to her to compete with that business so it's very hard to differentiate yourself and the way they traditionally worked as they work with psych each customer Zone it organization and they train them to do this kind of management but that's pretty tough because you have to learn all the stuff that changes for the software as a service companies like if you look at Amazon or a database or if you know any of these kind of companies directions you know people do you know the number of customers and so on his you know as much smaller and it's much more efficient because they have a lot of built-in software that makes it easy to manage you know multiple
 which of these are they got economies of scale basically from from that so how does the spark open source ecosystem compared to Hadoop Seco system Thursday they actually overlap a lot of software that works well with her dude can also work well with spark Warehouse plug and so strong calibers interact with the open so I actually some things that are different in spark and some of them are because we wanted to do things differently based basically based on what we thought you know how it worked well in her do and why didn't so one of the main things that's different is spark as a project includes you know this this this general computing and Jen but it also includes a ton of the library is built on top like sequel or machine learning or stream processing that end in herd
 these are completely separate software projects and the benefit of this is you know it means that all the libraries work together in are in a nicer way there's like opportunity to integrate my to make a change. Doctors many libraries at once and makes them consistent and they're also got released on the same schedule so you download one after artifact and it has everything you don't let no one weighs about having like a distribution of sparkling the only reason you need distributions of Hadoop is because Daddy's different projects that release stuff on their own different timelines and someone needs to actually bring together like a consistent set of Devon and make sure they work together kind of like Linux packages the same reason you have Linux distributions so this is one thing that I think you don't makes it simpler and it does make the project bigger and like there's like a bit more management you have to do have it but I think we made it work pretty well and we got a lot of the the benefits of this model
 the second thing that that's Park did. I think this also happened in her do but I think we spending a bit more effort on it is we also have a very standard interfaces to plug in new data sources new machine learning algorithms for example and new episode of 3rd party packages that make it easy to bring in stuff from outside any stuff that we don't want including the standard Library guys are pretty clear waiter to add it in there and so everyone who wants to write one can I can see how to log in and this is similar to like the way you have an NR for example you can easily install any package and there are tons of people lighting packages that that are very easy to use Docker has been in the news a lot lately because it adds a layer of usability over a Linux containers what are the synergies between spark and containers
 Ocean Technology like the Linux containers that doctor uses a good question yes so we do see spot deployed on find Container base platform so we see people lining at on on darker or more generally on some of these Frameworks that do you know distributed systems on top of Darker like measures and kubernetes and so on so I think that's the main way it's being used it's it's pretty nice it's also pretty nice for a local development so I don't know a lot of people use use doc right there you know that the package likes a spark and like a bunch of libraries they depend on for local development
 yeah I think I think basically it's probably not a difference because usually spark arrestors are designed to have multiple applications at multiple users already and they can already do they can already see resources between them to some kind of costume manager but what they offer is You Can Dance park on the same cluster that you run a very different set of work clothes are like say you also had web applications are you unlike your build system your build server like Jenkins or something like that and you can you know move machine diaper when you have machines. Org Idol that are not doing Sparks. You can assign them to other things so I think the main benefit of this is how many different types of things can run on top of that man can now share resources are because of work on making Rich standard libraries and he's like recent clue things like
 is clustering sliding Windows machine learning talk more about the development of standard libraries basically we we decided pretty early on in the budget. We wanted to have they which standard Library that's built into it so that all these things are consistent with each other because I was mentioning before when we looked at the model in her duplex people who are buildings really great libraries but they were building them separately releasing them on their own Cycles you know some of them are incompatible with each other or what I cards to install alongside each other and that made it very hard for users to you know to quickly pick some up so we started streaming I think Bobby like about seeing years ago and then machine learning and graph for about 2 years ago and we've been continuing to add stops since then and
 the main focus is to have a really good API that's consistent across all the libraries and to Adam sings that we are sure that we want to support long-term so like we don't want to add something into the library and then decide I go there's no one who can maintain this or no one's really using the sand and we have to remove it so we basically the one thing we focused on wisdom is stability so maybe we are in some cases we are on the side of caution like we only put an algorithm is that are very commonly used and that we're pretty sure we you know week we can continue to have later but I think overall the models works well every way seen a lot of people start to use these and you know if if if there are new things I want to do that aren't in the library is it's also easy for them to release something else along side at the libraries provide standard templates for like 1 day. I should be liking and how to block it in
 requested feature why is are so popular for use with spark language for statistics and and data science so it is just it's talking a lot of a lot of schools are most courses on statistics you know she will teach you like a little bit of art if they have some awesome Computing elements to them and so a lot of people know at sanitizer just an amazing ecosystem of our packages of built-in that you can you can just download a news so it was very natural to you know what they try to use a tendon is also well known for like you know it it it it it runs well on a single machine but it is can't really use multiple machines or even multiple cores on the same machine very easily at larger scale
 support for it is still early on but but you know we see people Beginnings to use that and also a lot of people contributing patches to to make there are support better and more and more functionality and that's why I'm excited you know to see how that continues with spark are are are these the same people that are building web front-end in the web front-end Communications are in the R Us with spark be different from those the web Finance are more like they're complete applications I created and save Java or python or something like that the other ones are more for exploratory like data analysis so you have some questions you want to ask and you know you wanted to build a model to predict like you know fired or something like that and you your time any different approaches and all his very good at prototyping
 science then I can to end applications the ones I've seen most commonly asked for geospatial data so libraries for working with geospatial data and same thing for a Time series so these are a couple of things that aren't in the library already but that would make a lot of sense to include and then there's always you know like there's always new things we can do to her adding more functions are to improve usability or performance or things like that so that's quite a few of these you know that are coming out and we are working on them yeah you created Spark
 yeah I think so basically you know I don't think there was anything too crazy but there were a couple of things that were interesting sorry we definitely assume that the user is who wants to go to work with you it would be great I said they'll be more and more users and they'll want to do it more and more quickly so don't want to do a lot of stuff interactively or in an exploratory way and lots of like all that spend three months to build this application and then it's going to be an introduction for like a year ago so people wonder what this means is that people will one higher level programming interfaces that you know do a lot more stuff for them under the covers so that focused on high-level API languages like python early on and then are now that you know are accessible not just like hardcore software Engineers but also
 scientist or analyst whose main job isn't the right software it's it's you know that they have sex and domain knowledge in a specific field so this is one of them and then we also did you know assume that it in terms of hard work they'll be a move two words you know faster Random Access style memories and it's not just itself which is it you know what people talk about a lot to expect but also Flash and some of the new types of non-volatile memory announce like they're CDX point in telling Micron Technologies for something like spark with a lot of your data can be in a you know and end in something that's much faster to access than disc and then you have to manage that space and figure out you know what what needs to be in there how do I move it across between machines and so on
 databricks has a concerted focus on the data science area and I can really smart because data science is obviously a field that exploding and I'm curious what are your thoughts on like how big is the field of data scientist gets in the future because right now it seems like you know there's there's a there's a lot of people maybe are underemployed or they don't have the feelings that we have the technical skills to get a date it's like you know they're qualified technical intelligent people but their jobs are kind of like disappearing and so did you like people that are well-suited to learn data science in the future so what are your thoughts on the data science space yeah I think most people who look at this is a very rapidly growing sort of a job title and job and you know we we do see a lot of people start
 we also see a lot of companies beginning sort of data data efforts were they would need data scientist you one of the things to it to keep in mind also is it it's not like a completely new singer in some sense a lot of the people that previously were considered say analysts or like you no data analyst this kind of position are you don't do the same kind of stuff we call a data scientist now so in some ways it's also just a new label for something that people were already doing but I think the reason that increases because more companies can easily store data and data about their business and it's it's it's a competitive advantage to use that for something so it increasingly you have to do that you know we want to focus on and them is it you know in addition to the two Engineers which I obviously you're also very important uses of spark is because you know
 data scientist need like basically they need easier to use tools to work even with stuff on a single machine but definitely worth anything with distributed across it cost a day they will not have the time and patience to wait to actually you don't spend a lot of time to build something complicated because they often have to answer a question you know within a few days and then move on to the next question or the next project that they're doing so that's that's why we want to make sure that what we're doing is good for them and the cloud models on databricks works really well for them because they don't need to install software manager can figure it all the stuff that's very time-consuming and it's not like what they primarily our expertise and a job so it is very complimentary that way is a sign of pent-up consumer demand
 yeah I think what I think I excelled is used for a different stuff so some of the things you do in Excel it would not make sense to use spark for them it's just for like basic simple stuff and stuff on fundamentally on smaller data sets that actually fit in you know it in in your wine Excel spreadsheet but I think it end and ends in South Carolina that is actually you know that's true this I think the people using that could easily benefit from you know something like that that's faster for example even on a single machine something that actually uses multi-core isn't and you know can deal with data that's bigger than you know. The memory of the machine and stuff like that so I think that's those are the most interesting ones examples for House Park is being used in consumer domains like drones or virtual reality
 in consumer demand so I think the best examples I've seen there have been like this kind of Fitness tracking applications are whereabouts. So that's kind of the most common one I've seen so we work with a few companies. How to use that for that and you know basically the idea there is like they you know it smarter to collect data across multiple choice and then tell them like hey you know you know people like you sure that we think it's better if they like each this type of food or if you know we noticed that you know you walk along this way when exercising but it might be better to go along that way things like that or even just telling you I can't compare to other people you're like in the bottom 40% in terms of like I don't know how many
 Stars you go out and stuff like that so just to tell you how you compare to the others that's the main thing I've seen ya example talk about how they might hypothetically Building architecture so let's say they've got all these consumers that are wearing these Health devices and this this data is currently being being sent to to the servers in this company how are they going to build you know how are they going to build this spark system that ingests that data and does useful things with it yeah so yeah I said it the first thing is to decide where to store today. So it died several options there so you can do for instance you can just write an Amazon SD if you want a cloud service to the story than what you will be very easy or if you want you could you could have a Hadoop cluster or you could have some
 Cassandra which is more of a you know it's it's also good for serving life crazy so is it does not useful if you want to play the day that the dew point where he's on it in addition to just throwing some logs so that's the first thing and then I think most people would begin with some some exploratory data science so then you sit down in front of a python terminal one of the you know like a notebook like interface like the one that we have in database where you you know you you dinner subset of the day. Maybe into memory across the cluster and then you start slicing at in different dimension like trying to answer say whatever question or hypothesis you have like for example if you think you know you can say you can you want to compare each user to other users in the neighborhood and tell them like if they're very different
 salvation in terms of the activity pattern or something like that so you know that's that's like wanting you try to do and once you actively using are you typing some commands you get answers you know you type a new ones based on them and once you come up with you know something there like you know say you want them a model of like what you know whether this user play for is like one type of Activity one another once you built that the final step is to actually make a kind of a production application and you can write a streaming application or you could hide a batch wine and in you know just like on it every night and maybe email people in the morning or something like that and hopefully in this case you'd be able to take a lot of the Cody Rhodes during exploration and we use that code to spend some time to the hard enough maybe Optimizer 2 or 230
 resident RAC podcast you mentioned something off hand about spark having BitTorrent built into cluster management is bitter in taste is not in question management but it is used for some of the communication operation to like one of the operations is vodka swear you have a value on life to master node and you need to send it to all the workers and sometimes these values are really big like for example in machine learning the value is there parameters actor for your algorithm and that's something that can be like many likes a 10 million floating-point number so that's like a few you know like tons of of megabytes basically and we actually use a protocol similar to BitTorrent descended across all the notes in the crash there cuz you don't want them all just waiting on the master to push that data to them so yeah
 I've seen a few like a few thoughts on it but yeah I off the top of my head I'm not sure but if you want I can try to find them it's yeah it's creepy I think for analyzing the Bitcoin blockchain and like the transactions is probably the best you know play should try to use that stuff after Big Data week is going to be Bitcoin to see how people are using it so I'll see if anything has changed in the patterns that people use to access Big Data since the Inception of mapreduce so yeah I think several several things change so I think that
 how to use this data because it went from like all we have this batch job we have to run every night like say this index of the web you know we just want to run this application to or we collected lots of data and we have many questions and we wanted to ask those questions as quickly as possible and expose it to you know as many users as possible within the organisation who might get there and prove you know down but I still like whatever they're doing for their job so that's one thing I do number of users has increased and I think also the the the level of timeliness that people you know I looking for has has increased as well so both in terms of doing things in a streaming fashion and and just answering these interactive quiz faster and not having to wait you know 20 minutes to get back the answer for her gray but getting it back in 20 seconds and moving on to the next one
 yes I think we are there is a lot of stuff you know still lots to do and and Spark and that way you know what I think all of us are excited to work on but you know I'm I'm really excited to see you know what to do to see the girls of the of the standard library and really put together sort of the the most comprehensive library for it you know doing data processing in parallel and I think we already have like I think the largest start Library that's out there today but I think there's still a lot of very cool algorithms and functionality like I mentioned time series data geospatial and so on that you know people are figuring out how to do at large scale and that'll be cool to include in there and then the second thing I'm excited about is you know price on and our users and you know making this kind of computing infrastructure available tomorrow and
 types of users that traditionally like would never have touched you know mapping do send it would be too complicated for them to do it so do you know the things I'm I'm excited about so what are the moche that I know where it were running low on time will be the clothes off but what are the motifs in the show that I want to build up is this idea that people should be willing to go out and work on their own projects and if they see a need in the world person technology afraid to create it so do you have any tips or strategies on how to psychologically prepare yourself to to write something that that is on the front like me some what revolutionary bold yeah sure yet so let me let me let me mention a couple of things so the first thing is you don't know with the
 what does you know increasing I prevalence of Open Source software but also with the really great sort of community Tools around it like it hard and you know darker have another package index for various things I expect packages are packages and so on I think it's easier than open source software and have other people and begin to use that's all that info it's possible for people to discover it at the same time though you know if you want people to use that it's very important to have you noticed art with something that actually solves a need soap and often the best way to do that is if you have that need yourself and then you obviously you know that what you're doing is useful and to also start by making it very small and very simple so that everyone can understand that and can kind of begin to use that sucks you know when I look at open source Library
 something you know especially if it's something for work when I'm like oh should I find an existing library for this or should we write our own version of this of the stink it's it's always really nice and I look at it so I call it it's a pretty small Library it's like you don't like 5009 or something I look at it I can kind of understand that and I think you're even if we use this you know how long do standard enough to be able to get up to speed quickly so that's when when spark started it was a long time you know a lot of people came in and said it's a little bit skater deployed is like totally different like in distributed computing and Jenna night if it crashes you know my by production application is not going to work but this is like small enough that I can understand it and you know I'm going to give it a try I think this is one of the things that that helps Park and a new battery stand you sometimes new projects try to do too much at once
 and you know and and and basically and become scary for people to adopt them so you know I think these are the things you don't make sure you don't make sure it's easy to understand and and make sure people can find it still guitar Boise weather services
 no I think that's that's pretty much I really enjoyed chatting yeah but thank you so much for coming on software engineering daily project so keep up the good work
