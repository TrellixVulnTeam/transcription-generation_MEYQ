Transcription: biology research is complex the sample size of a biological data set is often too small to make confident judgments about the biological system that is being studied during Jason Knight PhD research the RNA sequence data that he was studying was not significant enough to make strong inclusions about the gene regulatory networks that he was trying to understand after working in Academia and then at human longevity Inc Jason came to the conclusion that the best way to work towards biology breakthroughs was to work on the computer systems that enable those breakthroughs he went to work at nervana systems on hardware and software for deep learning Nirvana was subsequently acquired by Intel during this should we discuss how machine learning can be applied to biology today and how industrial research and development is key to enabling more breakthroughs in the future
the main lesson that I took away from the show is that while we've seen phenomenal breakthroughs in certain areas of Health like image recognition applied to diabetic retinopathy or applied to skin cancer the challenges of reverse engineering are genome to understand how nucleic acids fit together into humans this kind of challenge is still pretty far out of reach and improving the hardware used for deep learning will be necessary to tackle these kinds of informational challenges
are you ready to build a stunning new website with wix.com you can easily create a professional online presents for you and your clients it's easy choose from hundreds of beautiful designer made templates use the dragon drop Editor to customize anything and everything at your text images videos and more Wix makes it easy to get your stunning website looking exactly the way that you want plus your site is mobile optimized so you'll look amazing on any device whatever you need a website for Wix has you covered the possibilities are endless so showcase your Talent start that Dev blog detailing your latest projects grow your business and network with Wix apps that are designed to work seamlessly with your site or simply Explorer and share new ideas you decide over 100 million people choose Wix to create their website what are you waiting for
make yours happen today it's easy and free just go to wix.com that's wixx.com and create your stunning website today
 Jason Knight is a staff algorithms engineer with Nirvana Jason welcome software engineering daily your background has a lot of material that I want to cover and you and I have had a number of conversations over the past month or so and I'm hoping we can delve into a variety of some of the topics that we've talked about in our various conversations your work in machine learning started when you were a PhD student you were looking at RNA sequences explain what you were doing during your PhD she was on both sides of the fence the theoretical and going to more applied on the theoretical side the ideas that a lot of Statistics classical statistics in in the machine learning statistical machine that came about from that has these kind of large sample assumptions where you assume that you're going to get more data
 overtime are you have a large enough medidata that you can make these kind of asymptotic arguments to say well you know when I get an update then all these nice things will happen unfortunately with with medical data biological data typically the sample collection processes is rather expensive for her difficult and also the the domain itself the problem and it's just such a complex High dimensional space that your sample size relative to the complexity the problem is typically much smaller than you would like so these asymptotic arguments don't hold as much water and so the theoretical work was basically assuming you have a small amount of samples what is the best you can do with that and what are the kind of theoretical guarantees you can make especially given that it's human lives on the line SMC when you're making these kind of predictions of drug treatment outcomes
 search like that that was the theoretical side how can you work with small samples in this complicated domain and so there's kind of a variety of techniques that's not an uncommon scenario there whether you're talking about human health or
 something that's far outside the the realm of human health there are plenty of problems with small sample sizes where it nonetheless we would like to make predictions about them what kind of techniques can you use in that kind of domain rights of the kind of two sides of the approach that you're taking one is it's simply using a statistical techniques primarily Bayesian techniques that have really good optimality properties regarding their sample efficiency so how much your how much information extracted from each sample and there's even a theoretical argument said Bayesian reasoning is optimal and in some sense after these approaches and you can knowing you're doing the right thing with that information and combining it in the appropriate ways
 Give Me A Reason about the uncertainty that you get as a result of that says that's another big pieces be having an ocean of the uncertain you get is is critical right because if you put into dating to get a results then you will the next question to ask is how good is that results are how much should I expect it to matter to the patient or how should I bet someone's life on it so that's that's the article or less that part the other part is that down biology in in really any of these two means like you mentioned there's others lots of examples you have typically lots of domain knowledge if it's not in your purse a for example biology we know all this information about biological Pathways and hierarchical in nature of a nucleic acids in cells and tissues and organs and all these things so how can we take that prior knowledge is what we call it in
 code that in a way and coat it into your models that constrains your analysis so it improves your reduces the uncertainty with the limited amount of day do you have when you have that car situation where you can only come to a very uncertain judgement like you come to a conclusion but you are so uncertain about it what is the practical application for that is it just something where you look at it and you say okay we can't necessarily act on this but we can use this result as a clue to explore something else but the prediction of algorithm is just one part and it's the DS or do you have is almost as valuable as a patient self because when you receive a I mean essentially it is the same thing as if you're talking to a doctor and he tells you to do some treatment then your factoring in in his credentials and and accolade
 she has an in and out what recommendations he has in terms of whether you know what you want to follow his advice and whether you might get a second opinion that's the exact same thing with the new machine learning model is that if the uncertainty is too large then you either need to collect more data your model doesn't match reality and so that the data isn't informing or training the model well either change the model collect more data or maybe you're asking the wrong questions you were modeling Gene regulatory networks and you were using Bayesian statistical models with Advanced Markov chain Monte Carlo techniques can you give me kind of a summary for what exactly was the data set you were working with and what operations you're performing on it and what tools you were using right so the largest project to his expression data for RNA seat State as what
 also to the cell you can think of it as a machine or even a state machine and it has a free-floating RNA that is ready to be translated into protein and if you measure the amount or quantity of these RNA strands that are floating in the nucleus you can get a sense of what the current state of the machine the cell is and so what lyrics Modern sequencing techniques you can measure all of these are free-floating strands at once across either Boca population of cells are actually read it some single cell measurements as well and the idea there is then given a set of these measurements for how well this one was cancerous patients the doctor notices of IHOP's are a tumor biopsy
 tissue does this say sequencing analysis to get these expressions and now you have a measurement to the internal state of the cell and you went to determine from that measurement what type of cancer is this what type of treatments we should be using for this particular type of cancer because it's an umbrella term for for many different diseases and that's where the whole Precision personalized medicine initiative comes in as the idea here is to be able to use this High dimensional data is expression measurements to make predictions on what type of diseases says with these 20,000 measurements for each sample and said the basic idea is to build a hierarchical Cisco model of a word that means is you you in the process of measuring these cell environment variables and expressions you
 is it a set of processes that are undertaken to actually drive these measurements and you can model each of those steps statistically underlying physical processes and in doing so you can
 Cody the noise in and all these are there a Transformations that are happening to the real data and then bite and Ferb backwards to the real estate of the cell from these projections if you have the cell's internal State and so that's where the Markov chain Monte Carlo comes in and said these models describe the forward process if you will die from real free-floating RNA in the cell to the measurements you take and you want to go to the back you want to go backwards in quantify the uncertainty you have as you go backwards and so mighty Carlo is one way to do that go from data to basically in front of the distribution of what you think the real estate of the cell West music those measurements in 2015 what were the tools that you're using for doing more money Carlo
 relatively the same as it is today there are some kind of domain specific languages an difference in such as Jags and Stan was kind of just recently coming out these are packages where you can kind of right this the statistical models and DSL and then they have their own custom purpose and Friends engines that you plan to your data and a new start for Matt and then it'll run to mcmc for you unfortunately some of the specific goals that we were trying to get at specifically that kind of genetic regulatory Network topology if you will it's a very discreet highly Pete energy landscape which is traditionally can a snail for mcmc techniques and so we had to experiment with
 some tacos from the physics literature has specifically driving from sling call the wing Landau method method for moving around in these high dimensional highly Pete's energy Landscapes without getting stuck in local Minima are you minimize the amount of for you get stuck and so there's a very limited so it kind of remembers where it's been in tries to kind of push itself to do new things you can actually make analogies with human beings in like if you love to travel to Paris France in send you you have to think to yourself I love Paris but maybe there's other cities that are even better out there so you kind of have to counteract your desire to go to Paris with the fact that you've been there a lot this is this Explorer explain how can I trade off as well
 you are building a data-intensive application may be involved data visualization a recommendation engine or multiple data sources these applications often require data warehousing glue code lots of iteration and lots of frustration the exactive studio is a rapid application development Studio optimized for data project it minimizes the code required to build data-rich web applications and maximize your time spent on your expertise go to exactive. Com SE daily to get a free account today that's e x a p t i v e. Com SE daily the executive Studio provides a visual environment freezing back and algorithmic and front end components use the open source Technologies you already used but without having to modify the code unless you want to of course access e k means clustering algorithm without knowing our
 or use a complex visualization even if you don't know D3 spend your energy on the part that you know well and less time on the other stuff build faster and create better go to exaptive. Com SE daily for a free account thanks to accept it before being a new sponsor software engineering daily it's a pleasure to have you on board as a new sponsor
 okay so I want to keep moving through your work history as an electrical engineering student and then you did a PhD in this basic computer science and biology intermixed stuff with a lot of emphasis on biology so I think of that is moving up the stack from electrical engineering and then you moved up a stack even further when you went to work at hli which is explicitly focused on biology and human longevity and then after that we'll get to Nirvana where you work where you are working on chips for deep learning and this is sounds like getting back to your roots electrical engineering but touching on hli human longevity Inc explain what hli does their goal is to essentially amass the world's largest database of whole Human Genome sequence
 and in so doing collecting estate evasive is when they got heart disease how did they respond to this treatment center Center and it's combining that database with machine learning scientists and researchers like myself I've been trying to untangle the represent the structure they are in in in Enfield models predictive models to make use of the state and that feeds back to their kind of clinical work which is they have this the health nucleus is what they call it where
 you can pay a lump sum of money go and get a full body MRI scan get your whole Human Genome sequenced will do some expression measurements as well and decks can a whole bunch of other things and it's like a complete modern-day physical then if they notice anything or you later get sick then they can cross-reference that data and then use all the data in the background to make state-of-the-art dictions on treatments and different disease etymologies and stuff like that folks in Silicon Valley who talked a lot about longevity like Peter teal or Craig Venter who is part of the hli runs hli or Larry Page
 did you look really turned my understanding of debates around human longevity and Death on on their head they seem to characterize human death not as much as an inevitability but as just a set of symptoms that may lead to an expiration date on humans those symptoms you know can be isolated and perhaps dealt with what are some of the fundamental aspects around human health that your opinions evolved on or changed when you were at HL I actually was pretty gray and he's a researcher UK and he made a great case for why aging should be
 sitter to Disease an index should be considered a disease States in Chile and such a logical way that it's it was hard to refute that I just never really thought about it explicitly before but after here and I was like oh that makes complete sense in so that really within when I saw it I guess they shall I can sense it resonated with me in terms of what does not a whole lot of better things to work on I believe this is also one of Elon musk's your central challenges though this when he decided not to work on it to you I guess and I think it's it's an excellent I Daysha skull and
 the problem is biology that the deeper you look the more twists and turns she leaves for me inside of it is it's just such an amazingly complex and fascinating system biology in school for a while before switching to computer science and I love the fact that biology is such a more humbling field than computer science computer science is always liked you. We'll find a solution to this we built this system we can build another system on top of it it'll be better it'll be faster and biology is this slow arduous dissection of something that we just don't understand
 yeah definitely a humbling is a good way but it's also a shame because there's you can essentially it is designed outside of human hands then there are lots of things you can learn from it and applied us back to computer science even like the distributed can a modular systems I mean that's biologies bread and butter so we're really just playing at the kids table when it comes to orchestrating complex process ease through my very noisy channels in a robust systems in the face of these new complex and noisy domains work at Nirvana which is a company that's building a platform for deep learning explain what that means what's a platform for deep learning
 left but they July is his I basically that the complexity of biology was just is I thought that having large enough data set would be enough and maybe there's still lots of great people working on it but I I feel like the what's needed it now in the field is this more stronger machine learning Technologies so that's why I was really trying to the Nirvana because they're really pushing the the boundaries of of what you can do in terms of enabling infrastructure for data scientists and so it's that's why I want to be a part of this in between the full stack was and continues to inside of Intel so they're developing a deep learning specific Hardware on a to enable in new types of network topologies that have never been done before and then on top of that I saw for infrastructure to
 unable to use experimentation of these new larger models and then also the cloud infrastructure to abstract away the details and allow people to use Hardware without having to install it in their chassis themselves let's walk through some of these aspects of what you work on your Bona Nirvana has something called neon which is a reference deep learning framework for high performance on Hardware at the bottom of the stack we have actual Hardware that is running computation at the top of the stack we have our application Level code that's spark or tensorflow or python or whatever you talking about we're in this stack between the application Level and the hardware level does this neon reference Frameworks it is going to be at the same level as a tensorflow or
 somewhere around there so that the user of the data scientist with directly into import neon and their python script and then use those Primitives in Belding blocks to build up their deep learning typologies what exactly does it give them originally neon was the same as its performance so still in benchmarks by Facebook and others the performance of neon on gpus is still unmatched even buy the 10 videos on Kodi in an libraries and that's from some serious optimization work at 10 my some others here how did I that I claim no ability to myself and then now is where we're moving further
 into the future we realize that deep learning is moving so fast that it's It's Quickly becoming apparent that the the main workflow for for data scientist in the future will be less and less about building new network topologies from the the core building blocks that they're doing rather it'll be more sitting together pieces of existing architectures and reusing allowing data scientist to move up the layers of abstraction moving into the future and end in really enabling data scientist to do a seamless way is this one thing we're looking at here I'm so looking at how I can build composable systems for deep learning that enable data scientist to me
 higher up the abstraction in and you're also seeing is from your tensorflow with their recent announcement of the layers API integration of Caris but we think that can be taken even further so really excited to see what we can do in terms of making data scientists and powering them that much more because we we do customer engagements as well so I was looking up to be part of one this was starting rise I join actually and releasing firsthand how the interaction of the data scientist are teaming and the customer work together and in the way that these deep learning models are beautifully built in any abstractions you need like all that feeds directly in to see what we need from T20 primary source also we dog food a lot internally when you talk right a data scientist from Facebook for example they import neon and
 somehow makes their code more performant what is it doing is it is it changing the order of their bike code instructions or what exactly is it doing to prove that the performance of their code it's essentially a compiler and so it does all the kind of things up I would do but the focus of the tensor Matrix linear-algebra type of actions rather than typical llvm-clang where it's rearranging memory loads and stores here were looking at the memory usage patterns and reusing memory buffers for intermediate values that are no longer live in the competition fusing kernels together which is similar to fusion and functional programming languages like Haskell and their vector
 Grayson's which prevents or reduces the amount of memory loads in and saves he need additionally memory layout optimization so the way you actually layout of tensor in memory is very important for performance because of cash locality how you're pulling it in and doing that the tiling of the The Matrix multiplies on the back end so you can get is a really great performance winds are really bad performance losses if you're not considering that carefully it's a whole slew of techniques along those lines this is such a new field for me these lower-level Optimizer thing he's like I did a show about Apache Arrow a while ago which is this really important project that basically just reorganizes your you're in memory representation so that you can share data more easily
 between distributed Frameworks you know like you you don't enough today maybe you don't have the same layout for the way that your data is represented Hadoop versus the way it's represented in Spark versus the way it's represented in in your python code and so you can get really good performance mileage out of making better data sharing or just like you said they delay out because it's just you can be more performance so there's also some kind of trade off there is that you know this is nothing new but there's there's always a trade-off between kind of how much you can compile and how much flexibility you give to the user and this is the same as in programming languages as well where users more flexibility with things like dynamic-languages and Ruby on the far end
 but you you was actually give up performance when you when you request a require that flexibility and so actually deep learning or so and see this where you have to use Dynamic topologies where are the topology of the neural net actually depends on the data and put itself and so it makes compilation difficult because it supposed is changing between dating and so we're looking at different ways of of allowing data scientist to have this flexibility while also getting the performance at the other end so that I think that's another area where I think I can really make a big splash because this is something that is not really being handled right now you have kind of the two extremes with tensorflow on one hand and torch and Pie torch and Schoenherr and eye lab on the other
 and nothing really going to give you the best of both worlds with something like that Julie I work through jet compilation you can essentially regain a lot of flexibility will also retaining much of the performance if you make the right set of trade-offs there's also Nirvana engine which is Hardware that's optimized for deep learning it's an Asic which is an application-specific integrated-circuit A S I see that is custom designed and optimized for deep learning when Hardware is catering to deep learning what does that Hardware do that's different again the trade-off I was mentioning that the flexibility and performance straight out and so that's one access you can pull on or when you throw you can tag on to regain gain a lot of performances is
 specialize in your hardware for for particular sets of operations within limits then you can retain some of the flexibility but but really maximize performance another is that because of the ability for deep neural Nets to to retain pretty much all their performance near apology with reduced Precision then we're also using pickle Flex Points which is a different way of encoding the data in actual the hardware and doing the competitions is hardware and so you just go lower from fp32 which is what most people use on gpus today to see FB 16 wishes with some people have been using lately on Nvidia Hardware but not a whole lot of work there but but
 that's the way the future but the problem is fp16 you start getting a problems with training and so flexpoint essentially allows you to retain all of the integer of 32 while having a single shared exponents I'm on all the elements in a tensor and so we've done in statistical analysis and validation and essentially the tensors the exponents having nexplanon each weight of the neuron that is somewhat redundant because you can essentially found them I'm kind of in an exponent regime and share that exponent on all the elements so that's another kind of optimization that were were applying and we have a sport that on the software sign says another reason to have the Nirvana grass to abstract distance away from Frameworks and they don't have to worry about them so what's the connection between neon the reference
 deep learning framework and in the engine the Asic that you custom built and optimized for deep learning so you got your Hardware on the bottom and in the middle is where the Nirvana graph project comes in sooner chronograph is basically llvm inspired architecture for solving this mini to mini problem where you have many front ends your Frame Works instead of computer languages like an LVN case any of any backends hardware targets you want to support all connections and so we some sleep in many many problem with one intermediate representation which is what we call a chronograph ir and so that is similar to the llvm-ir its representation for representing Matrix computations
 reasonably low level 2 mini user we don't expect users to write this themselves this is what the framework am it's like neon admits Nirvana graph ir and then that is taken by a compiler and then subsequently lowered to it you're given us a instruction set we've got a show coming up about llvm show luckily the this is like just in time I sort of understand what an intermediate representation is and then gradually getting an idea for why lldm is a big deal but could you just shed some light on that explain what an intermediate representation is and explain it a little more detail where you are drawing inspiration from the llvm world llvm being what was originally called low-level virtual machine is his but it's turned into this ecosystem of compiler tools that's widely used in places like Swift and rust and I'm sure a lot of other places but yeah
 Color Run on this llvm analogy in the intermediate representation definition someone who wanted to ride a new computer language or new hardware then in the past what you have to do is essentially just write it all from scratch yourself or for the the language designer for for the harbor designer you wouldn't have to find all the languages you must support find other different compilers and then figure out what their internal structures look like and there are some shins and match your your back ends firmware to each one of those sources daunting kind of proposition lvm was started as the PHD project to basically it was the most successful Alone by introducing the
 extraction which is a Hardware Independence representation of computing that slow enough level that's pretty much any language turn complete language can map onto but it's it's high enough level to Ace Hardware independent so then you can you can have any back and compile are so any front-end language like Haskell Julia rest imagine actually there's probably hardly any languages that don't have an lvm back in at this point PHP year something that I've got all these languages now all they need to do is just go to llvm-ir which is this Hardware Independence assembly code if you will and then now you can reuse all the existing
 do I have to write them all yourself so it it's basically a win-win for everybody as long as he can get those abstract the finding the right balance of intermediate representation is is the challenging part over and it if you want to write a new framework it really focuses on Diana's ISM or recurrent net Co specifically or neural network architecture neural memory architectures beforehand you'd have to consider writing all these Cuda kernels yourself or interfacing qudian in an fpga accelerators that are coming out which is difficult to say the least where is now you can park at 1 interview representation and then you get all these back and forth
 and same with Harbor vendors as well to have this idea of it and it's ok Google actually the tips of oil does Summits released the exelate projects open source and so that's kind of their take on it from the other vantage point of the framework author and they want to have multiple accelerators that is basically EXO a ecosystem is match up words necessarily only downwards where is were focused on really buildings ecosystem both for lots of framers and lots of Anakin's flow in a moment but let's talk a little bit more wrapping up the career chronology so Intel recently acquired Nirvana where you work what
 are the synergies between Nirvana and Intel Hardware Synergy sir I definitely parents of it until is unparallel for its expertise in Silicon fabrication and I will talk Rafi chip design high performance chip these kind of things and so we've we've already gained a lot of benefits from having access to their experts across the board to compare you know how we work with external contractors for part of our ship design and then we compare notes with you know what Intel internal people would say about the same thing in Austin Times they're there quite a bit difference in until his car lot of mistakes and such that external contractors Macon and so it's it's been really great they're having all
 supports and of course access to the the Fabs I was just saying they were very excited about thin on the software sign in Hell really has a long history of doing excellent job of making things run fast for everybody on Intel architectures that's one thing I didn't appreciate the Intel he said that's part of their like really great strategy for Intel CPUs is that yeah they make great news but they also have two soccer teams behind the scenes that are kind of quietly making contributions to the Apache HTTP server in genetics or whatever it is just all these open source projects that they make little tweaks here in the air to optimize for Intel architectures and and then lo and behold when are some Hardware
 reviewer looks it performance across wear clothes that people care about then Intel's faster. Only because I heard was great but because they've done at least offer optimizations behind the scenes essentially and so I think that that kind of fits with what we're trying to do here is just make deep learning faster for everyone and an able to have a new phones on your way from artificial intelligence that were seeing really take hold and I mean it's in everyone's pocket every time you talk to Siri or Alexa OK Google I want to understand a little bit more about where we're at in terms of the chip industry where are these different players are cuz I know that you know there was this big shift in the chip Market the way the chip Market worked as a result of mobile and seems like we're going through another one of these massive transformation
 because of the machine learning specific chips and as I understand the history the GPU companies stumbled into a gold mine basically because they initially made these GPS for gaming and it turned out that Wall Street and every other high performance Computing Outlet wanted to do the same sort of Matrix calculations that views were originally intended for four games and then this machine learning Revolution happened and GPU companies begin reaping even more rewards because again these are like Matrix calculations you're doing can you give some more color on the history of high performance chips and where the different players sit today question you know there's there's a lot of
 fishing Hardware also follows the kind of cyclical bundling and unbundling cycles that software don't know if you could quite was basically like it has kind of taking that in and added another dimension of change so it's actually he saw this kind of proliferation of of add-on device cards and I guess the 90s and early 2000 so you can buy a sound card from Sound Blaster and to do cars from Vudu and a t i and all these companies in and and and really as Moore's law made it just cheap to throw more transistors on a single guy then you see a lot of this a lot of these silicon IP
 is Richard traditionally discreet things since you just bundling more and more on to the processor itself or the North 1st and North Bridge and then the processor is cell and so sound card Industries you know largely subsumed buy just the single Northbridge chip on your ear your motherboard and then that's all moving a system-on-a-chip systems in but it is the one factor that seems to fight against that is win win win something like gaming you have or now with the flirting you have just a strong enough pull from industry to continue to want more and more dies face that it makes sense to have the separate card for these specialized applications and then now that it's becoming so exciting for the applications it's actually
 yes even like growing at such a great and so yeah it'll be interesting to see what then you have to Overlay this with the kind of cloud and Edge device like like you were talking to Peter Levine him at the end of cloud and it said that whole discussion also needs to come over lay here in terms of how much steep learning computations going to happen at the Edge versus in the cloud and there's a whole training vs. inference side of things how much training are you going to be able to do on your Edge device how much training do you want to do on your advice so I probably just could have Illustrated more questions than answers but why so exciting working still cuz there's so many factors at play yeah and there's so much it at stake in a good way of the possibility is an opportunity
 Indy Prime flips the typical model of job search and makes it easy to apply to multiple jobs and get multiple offers and eeprom simplifies your job search and helps you land that ideal software engineering position candidates get immediate exposure to the best tech companies with just one simple application to indeed Prime companies on indeed Prime exclusive platform will message Candice with salary and Equity up front so if you're an engineer you just get message by these companies and the average software developer gets 5 employer contact and an average salary offer of $125,000 so if you're an average software developer on this platform you will get 5 contacts and that average salary of offer of $125,000 AD Prime is a hundred percent free for candidates there are no strings attached and you get a signing bonus when you're hired you get $2,000 to say thanks for you
 Andy Prime what if you are a software engineering daily listener you can sign up with indeed.com SE daily to go to that URL and you will get $5,000 instead if you go to indeed.com Jesse daily it would support software engineering daily and you would be able to be eligible for that $5,000 bonus instead of the normal $2,000 bonus on indeed Prime thanks to indeed Prime for being a new sponsor of software engineering daily and for representing a new way to get hired as an engineer and have a little more leverage a little more optionality and a little more ease of use
 so you and I were both at the tensorflow dev Summit and it was pretty cool there was a lot of futuristic stuff on display and it I thought it was a great encapsulation of why this field is so exciting you know you see people presenting stuff like pee wee built a really good way to identify skin diseases when using all the machines and they do as good of a job as a dermatologist or you know we've got a great way of identifying diabetic retinopathy and they're all kinds of residence like that as well as deeply technical presentations I was there on a press pass which is has been unexpected benefit of doing software engineering daily and I got pulled into this little press meeting that included Jeff Dean and that's just totally surprised I didn't expect that to happen but there were other people on the tensorflow team there and we got to ask questions to the team
 so I asked Jeff Dean If There were Hardware limitations for deep learning this is in retrospect extremely naive question I was just thinking like oh you know I can't wait to distribute everything across like a spark cluster and like it no problem you know we could process all this data does it really matter and he was like oh yes of course they're Hardware limitations and he said you give a specific example of training Google translate you said they can only use one sixth of the total Corpus of data they had which is still obviously a ton of data but can you give me a better picture for why there are Hardware constraints and deep-learning amazing a question but you know why can't Google just distribute all of their data and process all of it is it where it where are the hardware limitations of them
 training procedure of a keyboard language is backpropagation is the cast of gradient descent so the idea is that you've got this large training Corpus and you've got this large function approximator with tens of millions or even tens of billions I think it is the latest paper I saw of of parameters that you need to train and and you basically initialize them randomly all these millions of parameters and then you slowly mesh them in the direction that you want them to go the key is is that you noticed them in by using small batches batches of your data and the problem is that traditional learning a rose with a grad student on his laptop or desktop and doing these mini batch updates one at a time sequential
 using about a hundred examples at a time in the promise that if you use too many examples doesn't rain because your your gradients are noisy NAFTA going to push you around in the energy landscape but if you use too little then that are too noisy and you don't get enough signal from your data in your kind of just spinning around in circles if you will see the promises that now okay you've got a Datacenter of of servers in a great each one of these can individually process these many batches but then I have the question of how do you combine these together and that's a non-trivial problem in the difficulty comes from a number of factors but sometimes it just doesn't work like you you can't just throw more capitation at it and it distributed fashion and have it have it so it converts to the right answer and that's one of the
 regarding Trinity's algorithms is still very much of a dark arts and then we don't have a whole lot of theoretical guidelines and terms of why some hyperparadise work and why some don't when you try to distribute this and we don't have sterical guidelines for why or how you should come by in this together there's lots of great work at about the research group sign different asynchronous and synchronous communication techniques
 the optimization itself of deep learning networks is enough that makes the decapitation challenges that much more difficult because you can't just throw parallel Hardware at the problem it's a one way as you get individual cars that are faster so that's obviously when everyone's trying to do cuz that's the straightforward answer but the research answer is that nobody really knows what the right approach is for doing things and distributed setting for training like to have a lot of questions they wasn't able to get to it how do you use tensorflow at Intel and Nirvana so we like I mentioned earlier dog food as much as possible so we mainly use neon customer engagements in a customer's from Healthcare Finance Automotive
 biotech government startups Fortune 500 companies and quick please with neon and how it enables their data scientist get up and running we may not have all the bells and whistles is tensorflow it's over for for certain custom gauges we leave it on the table for using tensorflow when when a certain you know this case requires it but I enjoy using the feedback loop of dog food has been really helpful for us I see so so you were at the tensorflow dev Summit Church is looking at the the scope of machine learning and deep-learning rather than specifically looking for applications of tensorflow one reason I was at the summit was now that we're moving more to come
 developing the chipper are getting it out there soon and and wanting customers to use it then she is part of Intel we want to support users where they are and enjoy seeing where we want them to be over the long run and so so yeah if you want to support tensorflow and another Frame Works as well so so that was what my discussions were there was people was what does this look like an extra layer like I mentioned earlier it looks like there is is a pretty nice story so I think we're all happy with how things are looking for the future near full circle as you have made the transition from doing basically biology Research into the lower-level chip develop
 I meant framework development are you becoming more optimistic or have your opinions changed about the opportunities for deep learning in biology more optimistic in fact I was engaged nucleic acid sequence prediction problem here for a customer engagement and seeing what can be done with the right Hardware a software and data sets on biological sequences was was some exciting beyond my wildest dreams really I was actually skeptical going into it because traditionally biology just as scorns anyone who tries to come in with too much hubris on on a challenge and I thought our our targets were really delicious but lo and behold at over the
 the project we made significant and relatives in really exceeded my expectations so I think I think if anything deep learning and biology have is almost a match made in heaven at at some levels of of of kind of the more fundamental a nucleic acid can a prediction sequence prediction because you have these three high-dimensional inputs and you want to learn a whole lot of things about them and and and was images you know it's is clear to see this is a picture of a puppy with nucleic acid sequences it's much more difficult to say you know what what is this telling you you know when we have a lots of ways of approaching a problem from classical even text alignments models models
 pretty clear to me that deep learning is a fits that sweet spot really nicely of sharing its own representational features and building at the abstraction levels which is exactly what's needed for the sequence analysis in and that keeps directly into your variance analysis and personalizer precision medicine so I'm really excited to see this space evolve into in Kirby's like a deep genomics out of Toronto that's their target is is this exactly so I'm really excited to see what they're up to over the next couple of years sequences and trying to understand what those sequences actually mean is that basically like looking at binary of a computer program and trying to guess what the higher-level code actually means you got four
 falafels fright actg just reading off the the hex dumped of Life instructions and and you're trying to interpret the years and how they interact with each other across several different domains of time and space and in an organization and yes yeah it's it's exciting right because it's it's your literally reading a blueprint in and is a of Life yeah because if we give the better we get at understanding this and week where we can build higher-level tools and let the biologist actually be doing productive work without learning spark makeup done shows with Biology people where they like yeah and then I learned spark and you are biologists you should not have to learn Spark
 and also gets really exciting when you consider that generative models as well so deep learning a lot of most people see is the kind of prediction classification type style of it predicted to do the generative adversarial networks and variational autoencoder have shown some really great work in terms of he would have generate novel proteins are sequences more on the small molecule when you think about DNA or RNA dial me in an RNA that does this has these properties and then you just kind of generate example strands and then you can sit the sizes and mass and then test them against it
 Chrysler things in your class biology laboratory so I think the sky is the limit here to start to get really weird I guess it is pretty weird already pretty pretty exciting already it's so hard to predict what's where we're going to be in 5 or 10 years it's like impossible 1 prediction as I think he had a biology so I'm really sad about that but you can imagine that for every domain that's out there like they have these core problems that they haven't been will solve know which computers and possibly they can have a new tactic that attack in the mail and so so like biologist one microcosm of exam I mean it's a big very profound but it's like as
 learning propagates into all types of Industries it's like in the world but machine learning is also going to eat the World Behind it as part of software and deep-learning probably going to be a large part of that will be large piling so so yeah I think those applications out to Industries well they're not as sexy as artificial general intelligence in AGI which most people don't really fully realize it the question answer box that we really want out of OK Google or Siri or Alexa fully like require is Agi a good amount of utility out of town but a G I will be the last Donna to follow like it'll definitely propagate to these industries first but that's just my meeting list Tucson
 directions alright well really fascinating stuff I could talk to you for a couple hours longer but I think we'll wrap it up here Jason thanks for coming out software in Cherrydale it's been a real pleasure getting to know you and I really enjoyed the conversation actually if I could make a quick mention that we are hiring here at Nirvana and tell if so anyone is interested in joining us here it's it's a really great culture hiring in positions in the Bay Area in San Diego and elsewhere so look at a job site intel.com and yeah cuz we are looking forward to know enabling data scientist for the next generation of deep learning
 few quick announcements before we go software engineering daily is conducting our annual listener survey which is available on software engineering daily.com you can click on the survey link the survey really helps us understand our listeners and gives us data that we can show to advertisers that help get us better sponsorship deals also the software engineering daily Community has started working on minor ranker this is an open-source Newsfeed platform we are trying to democratize the idea of a news feed so that the only news feeds in town or not this early Twitter or Facebook or any other centralized Newsfeed we'd like to make it possible for anybody to make a news feed so you can check out the minor anchor project at the minor record.com you can check out an implementation of minor anchor at software daily.com
 you can find links to all of this stuff at software engineering daily.com there you can also find a link to join our slack group to follow us on meet up for future meet ups and other information so thanks again for listening
