Transcription: applications built in the cloud are often serving requests from all around the world a user in Hong Kong could have written to a database entry at the moment just before a user in San Francisco and user in Germany simultaneously try to read from that database if the user in San Francisco is allowed to see a different database entry than the user in Germany that database is not strongly consistent consistent databases work such that two users who read the same entry at the same time will receive the same result weekly consistent or eventually consistent databases are suitable for applications where transaction ordering is not important for example photo sharing apps and e-commerce shopping carts bank accounts on the other hand off of need to be strongly consistent
cockroachdb is a scalable survivable strongly consistent database Alex Robinson is an engineer at cockroach labs and he joins the show to explain the data model for cockroachdb and how it maintains strong consistency
 spring is a season of growth and change have you been thinking you'd be happier at a new job if you're dreaming about a new job and have been waiting for the right time to make a move go to hired.com se daily today hard makes finding work enjoyable hard uses an algorithmic job matching tool in combination with a talent Advocate who will walk you through the process of finding a better job you want more flexible hours or more money or remote work maybe want to work at Zillow or Squarespace or Postmates or some of the other top technology companies that are desperately looking for engineers on hired you and your skills are in high demand you listen to a software engineering podcast in your spare time so you're clearly passionate about technology
 check out hired.com SE daily to get a special offer for software engineering daily listeners a $600 signing bonus from hired will you find that great job that gives you the respect and the salary that you deserve as a talented engineer I love hired because it puts you in charge go to hired.com se daily and thanks to hired for being a continued long-running sponsor of software engineering daily
 Alex Robinson is an engineer at cockroach Labs Alex welcome software engineering daily we previously did a show about cockroachdb and today we're going to be talking about it in more detail so I want to talk about cloud-native databases more broadly first so we're in this era where people are talking about cloud-native and people have different definitions of what that actually means I think they are those different definitions are directionally similar red love to start off by getting your definition of that term Cloud native Paso you're right that they called natives means different things to different people but but yeah it's usually along the same lines that the general gist of it is that it's a system that runs well and modern Cloud environments and environment is one in which process is often are around for a long time and you don't really trust the hardware that you're running on
 yeah when it up your ass was knew you couldn't rely on a VM to be up and running for months at a time or years at a time like you can with your own server in a Datacenter and so this concept of cloud native came around as people started building their systems to run well and in in infrastructure where Hardware isn't very reliable so system has to be something they can handle failures very smoothly ideally without a lot of operator intervention and that means that the system has to have some way of healing itself and recovering as notes fail or is discs fail or is is Network links fail and endnotes can't talk to each other anymore so this is kind of fun in your system has to handle failures be very automatable cuz it's usually run in these increasingly complex environments we're having operators manually do everything is Impractical right I finally because you have to handle failure is the system has to be distributed in some way any process that can only run on a single machine at a time is almost by definition not cloud-native because any sort of failure is going to take
 I'm completely most of the shows we've done about cloud-native whatever that means we're often touching on these runtime systems like kubernetes at but maybe I'm using that misappropriating that turn runtime system I just actually kind of made it up on the fly but work till you talking about the containers and these containers are things that are not very durable your expectation at this container can vaporize it any instance at any instant and with databases it's a little bit different you're not exactly making the same assumptions will help us understand like what is different about databases in a cloud native environment versus databases pre Cloud made of and I guess separate question I want me to throw two questions a good ones but talking about the Cloud Atlas runtime like a kubernetes container in a kubernetes environment or maces environment versus whatever the system of runtime or system of operation that a database is
 running on what's different about these these two ways of managing steak or managing user session or whatever I'll start with the first question and I think it's important to note that running something in a container it doesn't have to be any different from running it directly on a host the difference usually comes in and have the tools that you're using treat those containers cuz if you're running a process a database directly on a machine I'm trying to get the process and it'll usually keep running until you either kill it or until the Machine Sales and the same thing is true if you decide I'm going to run that process in a container on that host instead and tell you or a tool does something container or that host that process is going to keep running inside the container exactly the same as if it was running directly on the host so something inherently different about running a process in the container or the difference really comes in is that the tools that people use to manage containers are often a little more eager about killing them moving them around and things like that
 so the difference between running a database in a container and arbitration system vs. running it directly on a host is that there are all these disruptions created by the tooling so maybe you're trying to upgrade all your machines are the conversion of communities running all your machines you have to take the process down in order to do that that's introducing some down time that wouldn't be there if you're running correctly on the host but the only real difference is it your process has to handle a few more disruptions so if you want to get distributed process that can handle failures this comes almost for free so if you have 3 replicas of your dad for example in the case of cockroach and kubernetes decide to take one of them down for some maintenance then the other two can keep serving all your traffic is fine and when the other one comes back up it'll just rejoin the cluster and start operating as normal again so that's where this fault tolerance ultimate ability becomes really important in these environments and sew a database that wasn't designed to have multiple active replicas at once so that might have a single active at a given time is a bit tougher
 brightness apartment could you have to have a process in place to transfer which replica is active at any given time when these maintenance events are happening when I first was learning computer science and programming people were explaining disc versus in memory to me and they would often say yeah when you write to disk that means if your computer dies your shuts down or whatever that information is going to stick around even though your computer shut down and ran out of power whatever you put it back up still got whatever was on disc and in memory means this is something you shut the computer down if it loses the in memory things I think even then that was an oversimplification of reality and today that would be even further from the truth but we still use these terms like on disc or saving to disc will use least you know turn like durability and I'll be frank with you I don't really know what these terms mean anymore
 is there a simple explanation to this or is it is it's just like Simplicity that is an opaque layer over the reality which is like really complex layers of cashing in distributed systems and stuff well I komplex not just in your head it is it is a complex Concepts and in reality is while they're different layers of your ability but what do you think I was in memory system so there are there that are in memory databases but that still persist everything the desk so that they can recover and so how those systems work is that they expect to be able to do everything out of memory and the whole working set of data has to fit in memory but they are regularly logging all the all new write to disk so that they can recover if something bad happens and they have to restart for some reason so that the data is durable but the system is still in memory system where is zebras have a database that isn't an in-memory database that does require disk to store all of its day. Cuz it can't all fit in memory that are not necessarily durable because they don't flush the wreck
 disc golfing so there are systems out there that when you do it right there like knowledge that right to you before that date I safely on desk and if they then failed your date it would be gone even though you were told that it was there was written to desk and then there are you in further levels of this where now database can choose whether or not to fully sink that record to disc before acknowledging it and certain types of Hardware failures well maintained that data even if you didn't think it but other types of Hardware failure to complete power failures might lose it where is rebooting the machine want and so there are a lot of different levels of durability durability also comes in the question when you talk about a system doesn't offer true consistency cuz you could right now key to one of the kids and and one of them will clobber the other cuz you go into the same key and so what those rights will succeed but the first one might not be durable than the other one and clobbered it in a consistency race they're all sorts of ways to Define durability and think about durability depending on what your building
 you care about okay great thank you that's great disambiguation I want to continue to talk a little bit of history and set some context for people who are unfamiliar with this space we definitely have a hangover of the past where we have an association between like the database that you choose and this idea vendor lock-in so there are number of providers who in the past they looked at the idea of a database as an opportunity to lock in a Enterprise customer the Enterprise puts all their data into this database and porting that data to something else or porting it to something else that is that the organization can manage themselves without using proprietary software is difficult or impossible and then since then since that era of time and if you're talking about like I guess it's 90s mid 90s early 90s early 2000
 kind of moved a little bit away from that where there's sort of it is a Competitive Edge for a database to provide less vendor lock-in what's the state of kind of vendor lock-in in the marketplace and how does that manifest in like engineering features
 locking is a really tough thing to avoid everyone is worried about it these days much more so than they probably used to be but lock and answer the next roll wraps around cuz cuz locking is also natural effect of adding more advanced features to a system if you had some some cool new feature to your database that other databases out there don't support and your users come to rely on that that's fun even though it's something that's absolutely in that win for those customers it's a feature that they rely on that makes them building their own business easier but it makes it harder for them to move out of your system to go see your competitors don't offer it so it's tough to get rid of walking completely empty and trying to do so would probably be a Fool's errand but there's definitely brought her levels of compatibility that can be strive for so cockroachdb as an example we are using the postgresql layer protocol and are trying to mimic as much as their syntax in as much of their features as possible so that it's very easy for someone who's using post-credits
 do to swap out their back end and put tacos to be in its place it's a lot of work to do so because he has been building up for decades adding more and more features more more language corks and more things that need to be supported and sequel is a standard standard query language for most databases but it differs between all sequel databases there are we are you still around in my sequel that won't run in postgres where they won't run an Oracle and vice-versa for any pair of databases you can imagine you can come up with incompatibilities and so simple kind of mirror this idea of locking pretty well and that it's at a place where locking is small and easy to change data bases in it only some of your career will have to be Rewritten and only partially but true compatibility in complete lack of locking this is just not possible in the end of space debris
 gramatik code sonar helps development teams improve code quality with static analysis it helps flag issues early in the development process allows developers to release better code faster code sonar can easily be integrated into any development process performs Advanced static analysis of C C plus plus Java and even raw binary code code sonar performance unique data flow and symbolic execution analysis to aggressively scan for problems in your code just like battleships use sonar to detect objects deep underwater Engineers use code sonar to detect subtle problems deep within their code go to go. Grandma check.com SE daily to get your free 30-day trial exclusively for software engineering daily listeners
 code sonar analyzes your code and it delivers a detailed report the code shown are user interface provides all the information that you need to quickly understand the reports follow cross-functional pass understand cross-references quickly navigate between files and visualize large pieces of your code go to go. Graham attack.com SE daily to get your 30-day free trial and a leash the power of advanced static-analysis thanks to grammatech for being a sponsor of software engineering daily
 in I think it was like to that was it 2005-2006 around that somewhere around that time no sequel started becoming really popular and there were a number of reasons why I think two biggest reasons I come to mine is one JavaScript was getting really popular so people were looking at Jason and they're like well would be cool to have a database that looks like Jason okay cool we get this document database mongodb it's no Sequel and then maybe the other reason is it's schema-less you know so it has you can easily add fields to some subset of your entries in the database and you know it doesn't really complain as much and that was that was useful because you had all these startups they were getting started because of the clouds popularity of the cloud so you either this or there was just a huge growth and start up since we had to start if they were moving fast and they want to change their schema and so that made no sequel much more appealing what were they
 like the upside in the downsides of this like this kind of Boom in Cloud native usage of no sequel just give me your perspective on I guess it's been laying off 10 10 years roughly kind of sense bisbee's nosql databases started becoming really popular what give me your perspective on that history
 I think it's kind of interesting because the systems were first created in order to reach greater scale and greater availability now these systems are coming out of Google and Amazon in Facebook who were building the not cuz they wanted schema-less database but they were building the system's big table or Cassandra Dynamo they were building these because of the greater scale and availability that they could gain by getting rid of the sequel interface and moving to something simple but then it is very interesting with these Systems Green brought her adoption the marketplace for the opposite reason now for the reason that the the lack of a schema made it easier to move fast and then change your system more frequently I think there's a lot of value to both sides of that coin now when you are small and when you're a new system in your training free calling your database makes it hard to change your schema you're going to have a rough time and you probably going to watch a database
 traditionally sequel databases have not made it very easy to change your schema while you're still serving traffic I think there's there's real value to both sides of the coin of whether you have a schema or don't have a ski but some of that can be some of the pain can be lessened if your database supports very painless schema changes that can be done in the background without stopping any user traffic it makes it much much easier for start-up to deal with working with a sequel database there that requires a schema this kind of a kind of a tangent but there's some interesting parallel here to the the story of containers containers were the technology behind containers in Lenox were built out so that companies like Google could get better resource utilization and make it to isolate prophecies but then to the market was due to their packaging features the doctor added them and so it's this one idea containers the game that was created for one reason for gain popularity in the marketplace for another
 what are the features that the no sequel databases kind of don't solve as well what are they lacking and if you're if you're a modern startup you've gotten going you've got this is no sequel database that's just giant that you know you started with no sequel database to get no users in your moving fast and then over time to look Ho Holy Smokes and I'll wait we're actually now using a nosql database at scale what penalties are you going to pay for for being all no sequel
 call the biggest bouncy is just the lack of trust you can have in your database you can't always be confident that once there is correct or that what's there isn't sink because I'm almost almost equal system sacrifice consistency in order to provide that greater scale and availability and so if your system is running on a no sequel database you probably have a lot of work around in your code in order to deal with your data sometimes being out-of-date and that's that's by far the biggest pain that you're going to have to deal with you might also be upset by the lack of indexes so you going to have to manually maintain your own indexes in some cases depending on what system using and what kind of indexes you want and whatever you start manually maintaining your own indexes to allow for faster reads you're probably going to end up with some incorrect as soon as indexes is well and so it's really just this lack of consistency in life of correctness that it's going to be by far the biggest problem for large no sequel to plan it's okay perfect this is why I wanted the conversation to go
 when do we need
 consistency or correctness or whatever term you want to use to describe that why do we need consistency and reiterate why we might lose that if we're entirely using nosql databases
 nothing inherent about no Cycle Systems that require them to be inconsistent cockroachdb internally has a nosql database now inside of it that is consistent but most out there on the market aren't consistent so we'll talk about it then they are the problem with inconsistency is it it puts more responsibility on the developer was using that database to be able to to be able to use that data giving away cuz whenever the developer wants to use date of the database that the sink is it okay if the stator is slightly out of date or is it okay if my first right succeed but then my second right never happens cuz the process crashes now which is especially important in a cloud made of environment where process could be taken down anytime so the developer has to think about a lot more when operating against an OC full system even if it's always okay to lose the right or to read old data or amp for four copies of the year to go to thank you. That's always okay there's still a large cognitive overhead on the developers of the system have
 I think about that every time that used the database for anything when I talk to Ben Darnell who works at cockroachdb one of the co-founders it sounds like you didn't really like this term eventual consistency which is the term I've talked to different people about on this show that term eventual consistency is there anything wrong with that how do you define that term eventual consistency it's to find almost exactly as it sounds it means if you do a bunch of rights to your database and then stop using it the database will eventually be consistent so all the copies of beta will eventually be caught up and agree with each other but because its eventual I mean that any given point in time it might not be consistent so it's really the saying there are no guarantees you cannot guarantee your data is going to be in any consistent or correct State at any given time it just will eventually be consistent and so it's really not offering you a feature so much as explaining the lack of a future
 yes indeed and why are the bugs that we might have as a result of eventual consistency why is it so hard to think through how those bugs might manifest
 call concurrency is is inherently tough for people to think about and to deal with when you're writing a program locally you can start a trust that you know what all your data is going to be now if you do it right into some memory location and then another later on you want to read from my location it's easy to reason about what's happening in your program it's tough to reason about this kind of delayed data store where what's at reading might not be correct and so it it's just not something not a way of thinking that comes very naturally to people and a very often leads to bugs in production
 well I mean I think it's basically like a dining philosophers it's like any it's a race condition that you can imagine in in computer science basically like if you make a big series of Rights and they end up writing to billete those Rights happen at the same time in different parts of the world and they end up writing two different replicas of the same database you know it's going to take some Delta some. Of time for the conflict in like okay how do you okay so how are we going to sort out how these rights are linearizer serialized whatever time you want to do if you need to time order some rights that essentially happened at the same time around the world and you need to make some Global resolution on what order those rights occurred in those different database nodes are going to need to resolve the different rights that occurred and in the meantime if other people are reading
 from that database you may not have guarantees about the consistency although I guess that's that's not a great example because there's essentially no still like objective thing you could say that like okay this one occurred first they all occurred exactly the same unix-timestamp but you know even if they were like a millisecond off and these occurred in different places in the world you would have some. Of time where there would be some cement consistency I guess I kind of think about it is trying to write a concurrent program that doesn't have locks or doesn't have a time okay that said if I want to build a consistent database what sacrifices do I have to make what am I going to do
 bought the biggest sacrifice going to have to make if it's a well-built system is latency so any consistent system that has multiple copies of a piece of data is going to have to talk to it more than one of those copies now if there are three copies of a few state of at least two of them have to agree on something to guarantee consistency and that means that in order to commit anything in the system you're going to have to pay for a network round-trip between it now at least two of those copies of the data where is the system that you don't worry about consistency could commit a right before it's talked to any other nodes so that's a fundamental sacrifice that you have to make in order to provide consistency in a distributed system is it there's more communication cost that you have to wait for when making any sort of right but beyond that there's not much inherent that you have to give up the system might be more complex to implement but it doesn't necessarily have to have worse through but it doesn't necessarily have to be slower and in any way other than Network latency
 it doesn't have to have lower scale or or anything like that the only thing you really have to give up it is the network communication cost
 and I think your ass isn't does not like a sister of trade-offs that's like that adds up to something bad it's just addicted to trade off and that's why there's no you're working on cockroachdb which is a database that makes a set of trade-offs that give you strongly consistent Sequel and I want to dive into that to give people an understanding of why there are so many different databases in the world I think it is just a case in point of every different database is making some different trade-offs so what are some of the key differences that cockroachdb does implementation wise to provide that strong consistency
 so as I mentioned we have to store multiple replicas of each piece of data in order to provide availability in case of failures that's almost a requirement in order to be available in a cloud made of environment so we have multiple copies and in order to be consistent we have to keep those copies in sink they have to reach some sort of agreement I'm every right to the system whether or not the right is accepted and so for every copy of data or every right to a given piece of data we have to reach consensus using what's call the consensus algorithm when we use the raft consensus algorithm it's fairly similar to the facts about algorithm every right in the system has to go through this consensus process where the three copies of data are effectively voting on whether or not to accept the right so every right has to do this which is in stark contrast to what no sequel system might do to accept a ripe
 so that's the biggest difference but once we have that low-level Foundation that allows us to do consistent rights to a single a consensus group we build on top of that but whole network of a large number of thousands are tens of thousands of these consensus groups so we split our data up into a large number of ranges so we break the data up into chunks in each trunk forms at 7 consensus group that makes agreement on any given rights so if you're writing a single key that will go to a single one of these groups in that group will agree on whether or not you're a techie if you're writing multiple keys in a single transaction then we have to coordinate those groups at a higher level to make sure that all you all those rights either atomically do or don't happen so there is a lot of code in Logic on top of this can sepsis protocol in order to accommodate full circle system
 okay I want to go from the ground up and talk through the architecture of cockroachdb so we'll get at a certain point to differ discussions of that consensus model be discussed let's just start at the lowest level of cockroachdb if I'm storing some database entries what's going on at the lowest level
 call the very lowest level of any database is writing the actual bites to disc and so it's a very lowest level of cockroach were using something called rocksdb rocksdb is what's going on is it in bearable key Value Store embeddable meaning that it's almost like a library that you linked into your process and it's this embeddable key value store that's been open source by Facebook and was originally Open Source by Google call leveldb so Google level DB which is effectively how they restore their bites on disc for their systems of record in their databases and I open source test Facebook and proved it to work better on Modern desks like ssds and we are riding our bikes to disk using Rocks TV so we don't actually write our own code for dealing with discs when we going to write a given key value pair two discs we pass it down into Rocks TV and ask it to persist it for us
 okay so now we understand the database entries at the lowest level let's start to talk about the replication model if I have this entry that's written in rocksdb on desk what's going on replication wise like if I if I write occurs and it's you know it's written in the lowest level with disc where else is it written when is it written why is it written so between those points actually there's a layer of Version Control and multi version concurrency control we keep your multiple copies of each key on my desk so I can go to that you would like or we can move up the stack what would you like interesting need to elaborate on the lowest level and then we'll move up
 right so while we write individual key value pairs to disk for any given logical key in this space we maintain multiple copies of it and what's known as multi version concurrency control so if you write to key a a time one will write that copy to desk and if you write to TA at time 10 will write that copy separately so that we still have the key at time one and this is important for things like supporting reads that are in the past so it enables transactions to continue operating concurrently even though and they might be reading writing to the same key the Reed can work on it devalue that's in the past if the timestamps work out that way and this really helps a lot with with high concurrency workloads
 tell me about the stack to the replication when a key is being written to disc on one node any right that we do gets passed through raft so in order to be written we have to propose a command to rafts and it's wrapped consensus algorithm internally will work out on each machine whether or not it needs to be written to desk so one of those notes will be the leader of the rap group that leader will write it desk locally then pass on the T value pair to the followers of the rap group The followers will then write the right that key to disk and respond to believe you're saying I wrote it and once a majority of the nodes have written at the desk the leader will consider it committed and then do what's known as applying that write to disk so we're really storing the state of twice once in rafts internal log where keeps track of what it's been working on and then once in the actual date of store once Rapids it has decided that the that the right has been committed
 okay so I think if I followed you correctly we're still just talking about a single box and now we can get to distribution which is on top of the replication once you have some replication you want to distribute it it was a correct process is right okay so let's let's talk about that a little bit more I guess explain the distribution layer in more detail so we propose goes through this process where is a user wants to write PA that goes to the leader note for the rap group and that leader node packages it up and in what's the Wrath command command to its rap log and this log isn't this sequential ordered set of commands that all the rap nodes have the exact same graph log
 eventually come to the exact same route log every command is in the same order in the same position in a log and that's what they used to come to agreement so for a new command wrap leader will write the new Commander it's log send it to the followers and if everything's going well they will write that same command to their logs and tell the leader that they did and once wrapped knows that command has been committed to a majority of the notes then each one of those replicas can take that command out of the Trap log and apply the command to the actual database so the raft log is stored separately on disc from the actual date either database and so it's a separate step to actually say now that the command is committed or going to apply that state to the real system so that reads will see it and the other rights can write over it
 your application sits on layers of dynamic infrastructure and supporting services data dog brings you visibility into every part of your infrastructure + APM for monitoring your application performance dashboarding collaboration tools and alerts let you develop your own workflow for preservability an incident response data dog integrates seamlessly with all of your apps and systems from slack to Amazon web services so you can get visibility in minutes go to software engineering daily. Com data dog to get started with data dog and get a free T-shirt with full observability distributed tracing and customizable visualisations data dog is loved and trusted by thousands of Enterprises including Salesforce pagerduty and zendesk if you have not
 tried a dog at your company or on your side project go to software engineering daily. Com data dog to get a free t-shirt and support software engineering daily our deepest thanks to data dog for being a continued sponsor of software engineering daily
 okay so the fundamental level were talking about an event log of changes that the different nodes that are storing a certain piece of data have to come to an agreement on two different replication logs for nodes that for different knows that are concerned with different types of entries or is there some Global replication log or Global event log that every node is concerned with
 so every node has its own craft log and every rap group so remember we break the state off into thousands of Chunk so the entire database is broke up in the thousands of different rap groups each rap group has its own logical raft log there is no like one Global log write each rap group has its own log that is stored on that note
 so what so what I understand is that there's the different likes pans so like if you had let's say restoring every musician in history and you had this alphabetical store for of all the different musicians and it's like okay the musicians named their age there discography and database was so big that you had to break it up into multiple section you got a through F you got G through n and showing through Z get these three different sections and you break these up into spans so you have each of these the span these entries bands in this is like I think it is you would describe as a sharting and then you replicate each of those shards and sometimes there's like you know Shard one and two will be stored on the same node and then you have another note good stores Shard 2 and 3 so if I even know that subscribe to Shard 1 and 2 is there a single raft log that has all of the transit
 dissociate with charge 1 and 2 or is it like separate raft logs for each of those charts that question makes sense actually kind of blurry at The Logical level they are their own rap logs but given it a single machine could have tens of thousands of these shards on them it would be impractical to have a separate actual file on disc for each one so logically each one is a separate but we actually store the raft logs and Rocks TV as well because rocksdb can gracefully kowalewski super graph logs into the same files on disk for us when we're reading it back out will read them back out at the pershall road level and we don't we don't ever need to read them out together
 read them out at the purse shark level so will read will read you have showered ate through a strap logout independently of showered and she threw and when we read them back out or in use them for anything even if they are stored intermingled in the same rocksdb database but you have them like said the same file but they're like segregated in different places in the same file so you can just read them in a linear fashion you don't have to not like all the reason to make gold corrector okay interesting okay so we talked about some of the sharting in the replication stuff we kind of get it there's like there's a raft protocol that manages the different sharting and replication layers different charting replication nodes eventually will get the scale and talk about like failures and stuff assuming we have some time
 put on top of this replication and distribution layers you've got a transactional key-value-store layer and gear before we talked about that let's define the term database transaction can you just explain what a database transaction is and then motivate the discussion of the transactional key-value-store layer that's on top of this replication distribution layers transaction in a database is effectively a group of work that you want to do you all at once so it could be a group of Rights it could be a combination of reads and writes what you want this to all be grouped up into one single entity that you were all completes are all doesn't and that's why I didn't any of those transactions are typically what's known as atomic so they either every operation in the transaction finishes for every operation failed and so transactions
 any sequel database are considered acid transactions is a very common acronym for Atomic consistent isolated endurable which I really just properties of these transactions and how they interact with each other with a core function at the core definition of transaction is in atomic set of operations that that are done together on a database which might offend be like separated those two transactions no problem like those two operations no problem like go to the store buy some milk in those two things separately may not be no there's not much ambiguity but if you say go to the store and buy some milk there's more perhaps more room for ambiguity Cypress probably not a great example but people who don't know what a database transaction is and why there is complexity associate with it go look it up I guess
 would you say that is that is that right or do you have your baby be a better example that defines the transaction the complexities that can result of a database transaction the canonical simple example is if you have two bank accounts can you have to transfer some money from one to another you need to take money from one and add it to the other if you do these operations action you know that either the money will be transferred or it won't but if you do this individual operations you might take the money out of person Hayes account but then you fail to put the money into person pees account if something happens in between them and then you're just taking money away from people right transaction there something that you would want to synchronize you want these two things to happen all at once or nothing is that True Value Store layer what's going on there what why do we want that
 well the the raft consensus algorithm that I was talking about earlier gives us no consistency and atomicity and durability in all of this but only on rights that take place within that one Shard because the stars are communicating with each other and so in a sequel database to be a sequel database we have to support operations that. All the data in the system at once and not all the time will not always be in A Single Shard infected almost never will be so in order support transactions across the entire database of a user we have to have some sort of higher-level mechanism on top of raft in order to make those transactions atomic
 okay so what happened specifically at the transactional key value layer of cockroachdb what's going on there will win a transaction comes in we need some way of tracking the status of that transaction and so when a friend action starts we create what we call a transaction record for it and that record just gives it some ID at the time stamp for the transaction and defines what status exam so whether it's pending and progress whether it's been completed or whether it's been aborted and that allows any consumer of that transaction to be able to tell what's going on with it as a transaction continues any rights that happen within that transaction get associated with that transaction record so if you start a transaction and write the key a that right you did. Be associated with that transaction so that reads won't start reading it until that transactions been committed cuz that and that would be a violation of of isolation within the database if you ended up not committing that transaction
 so you can do a bunch of work and then when you're done doing all that works whether it's been a bunch of Rights or bunch of reeds you come back and you flip that transaction records to indicate that the committee is done and at this point it's all of the things that written are considered committed as well and reads will start having to return them okay on top of the key value layer we finally get the sequel we all know what sequel is this distract now I don't forget standard language does it take us through what happens on a query throughout the stack so we all know why we watch sequel right like if you want you want okay let's say you were in postgres you're using a postgres database and you decide you know because it's like ability to survive know all kinds of catastrophes and stuff which we haven't even really gotten into but I've heard about all these useful things but I've got all these queries in postgres well you know where compliant cockroachdb is comply with
 grass sequel it's just the same query language but just different doing different things under the covers so but with cockroachdb you need to be able to implement that SQL interface so we all know what's equal is take us through what happened on a query through these different layers we've discussed you got sequel key-value transactional key-value-store replication and distribution like the lowest level on disc maybe you could just like map out and you know you don't feel like dictate the entire like codebase to me but just give me a description of what goes on in a query
 12 minutes equal Curry comes in we're essentially just mapping it 200 series of key value of operations and sew a query comes into US typically as plain text and so we have two parts that text into a more structured query we can do some semantic analysis of that of that query in order to optimize it and pick out what sound does an execution plan so we take that text and turn it into some sort of execution plan which is just a series of key value operations on the database and so every logical row in a sequel table gets Maps down to one or more key value Pairs and so
 or say it say you want to do just now a scan of your entire table will take that query parse it turn into an execution plan that just does a key-value scan over the entire key range for that table and returns all the data back and so there's just this layer that knows how to use our key value interface and answer to do the rights and read the card by Sequel and then map those key value pairs into the more logical sequel rose that are then return to the user so if you know how the other line KV store works then the only thing you need to know to understand the sequel database it's just how you can map sequel operations to evaluate operations
 okay fair enough so you have this database where the data is distributed and replicated and you want to maintain consistency so maybe I'm mistaken but I think that I like intuitively it feels like there might be more Network overhead in this kind of database when you need to in every time the other read or write you want to kind of make sure everything is going on everything is is is consistent again I'm not of a probably not freezing this as well as I could
 is Network overhead an issue is that something that you need to minimize during a database lookup or a right and is that complicated so
 for any given query the Cockroach TV process to receive the query becomes what we call the Gateway note for that query so that's you know that's going to issue most of the key value of operations for that query so if you do this naively there is definitely a network overhead to it so the simplest way that you can Implement sequel on top of the key value database like this which is what we did an issue is to have the note that receives sequel Perry go ask for all the data to be sent to it so it has to go reach out to all of the different nodes in the cluster that have the relevant he value data and read it all back before doing any operations on it so this is the one of us sequel database on a distributed system the V2 or the advancement of that it is to start pushing down as many operations as possible to where the data lives and that's what we've been working on for the last year or so where any part of a sequel query that can be executed closer to the data we're moving closer to the data
 so that means yes we're sorting data closer to where it lives were doing filtering closer to where it lives were grouping things and aggravating them closer to where they live and then moving running these things in such a way as to minimize the network traffic so that we only send back the date of that really needs to be sent back and it's just one way I mean that there is as you're as you as you said a lot of work that needs to go into minimizing the number translate in any distributed system was handling a lot of data like this I don't know that the consistency aspect as a whole lot of it that okay or no sequel system still has to send all the data to all the machines for any given the right it just isn't happening right away I'm still needs to have that level of background replication during operation to ensure things are in sync so the actual quantity of 8 across the network does not fundamentally have two different database it might be more of a lazy opportunistic consistency resolution thing as whatever
 banana minion Network congestion or delays wouldn't slow down your queries as directly as in a system where the communication is required okay cool no signal systems typically don't offer a large analytic queries that needs to move a lot of data around on Reed's where it's legal system exposes an interface that makes it easy to read your entire database if you aren't careful about what you're doing a sequel system is inherently going to have to to be moving more data around on reads as a result just goes it offers functionality for reading and processing more data and given query
 cool okay so the zoom out a little bit when a database grows there is something that needs to happen called rebalancing and if we're talking about this database of musicians and We've Got Musicians a through f g through n we've got em we got to go through Z and their sharted some number of times you say the charted 3 times and I'm sorry there should be broken up into three shards and you know you need to replicate each of those shards so that you've got some fault tolerance there but if your list of musicians grows like a big Rose you know by but a large order of magnitude then you're going to need to break it up into smaller spans like maybe now you need to break it up into a through d and e through J and so on just like let's say you need to break it up into into for four different Shard instead of three different shards
 so a rebalancing needs to happen what goes on during a rebalance in operation
 yeah so again this is an area where you can do something simple and then you can overtime right on it to make it more and more sophisticated to handle more and more work loads better so the basics year is that whenever I give the range it's too large say has more data than we're comfortable with having any one of these shards we have to periodically check and see how much data is need showered and decide if it's too big to split that into so that can happen locally and we don't have to go too much external communication for this we can just say this to The Shard it's too big I'm going to split it into somewhere in the middle and now I have two shots as you're missing your example of going from three to four more than once you have that extra Shard you might want to move it around the cluster and so we have a separate subsystem that is periodically examining the balance of data amongst all of the notes in the key and the subsystem has to know things such as how many shards are on each node and how much of the disc is used on each node in order to ensure we have a proper balance and so
 periodically examine examine these things and say no it's 1 2 & 3 have 50 of these shards each note 4 only has 20 obviously that's bad balance because Note 4 has far less than the average amount of data for the cluster and so it's going to say okay we're going to add a replica for one of these shards on to Note 4 and then remove a replica of one of those shirts from one the first three nodes and this is just constantly happening in the background trying to drive all the nodes towards having somewhere around an average amount of data but that's the basic version we're starting now to get more advanced in into the space download as well so you can imagine a situation where one of these shards has a very large Mouse Float on it say all the all the hot spots based on the user workload and you could get better through put in the system if you broke that showed up in the pieces and move those pieces of a different machine so that you weren't overloading the machines that it was on and so on
 working now on splitting up these ranges not just based on size but also based on the workload that's coming in and rebalancing them around the cluster as well based on how much traffic each of these ranges a serving kind of balance out not this time I stayed at each node has but also how many queries it's processing close to the end of our time I would love to know what are the hardest problems that you had to solve or that you are in the midst of solving while you're building this Cloud native database
 who's all there are a lot of problems that we're trying to work on here but the biggest one was really just production izing this raft protocol is consensus algorithm and making it work when all sorts of things can go wrong and also when were running so many of them so we collaborate with the STD database on a raft implementation and that CD puts the entire database into a single rap group where as we're running thousands or 10 2003 the more arrest groups and it's hard enough to do just one for trying to run so many of them has now added even more correctness concerns performance concerns and scalability concerns and so relieved it's probably the fundamental biggest problem that we worked on over the last couple years it's just production eyes and consensus and scaling it to this point and testing and imagine
 OK Google came out with their like their Cloud spanner product which is the their production eyes cloud service version of cockroach DB of Congress of spanner which which covers DBS is heavily influenced by now I know this is not like it's not a direct competitor I'm in the reason I know that I mean it's it's it's something to competitor but the reason I know it's not like like oh my God these roads are business type of competitors cuz I remember when I was talking to Ben and our interview about cockroachdb I asked him about this issue before Cloud spanner came out and you it just seems like he was not really concerned that this was going to be like a big deal and I don't completely understand why that's the case but I'm willing to believe it can you explain how you compete with Google Cloud spanner in your own eyes or or how it's maybe you're totally different product in this is just a bigger space than I realize and what's going on there
 why are you guys able to maintain such confidence in the face of a of a cloud service that originated this idea
 yeah I'm in spinner was certainly the inspiration for cockroach DB or at least the 4Runner for cockroachdb but there are a couple differences that make it make them now why they it while they are competitors and sometimes they're not Cutthroat competitors fighting to the end or anything like that cloud spanner is fundamentally limited by the fact that it is a cloud service on Google's Cloud platform so it will only work for you if you're willing to run your your work loads on Google's Cloud which already eliminates a very large number of potential database users right the world does not currently run on Google Cloud platform and even though they may eventually now hope that is the case it won't be for a very long time and so Google spots Banner is already limited to people running their also it only works for people who have truly bought in to running on Google's Cloud as well for any hybrid users or parsley and Google's cloud and parsley elsewhere they will need some sort of database solution for their their elsewhere weather
 turn on data centers or another Cloud so you got it from the start when we're not it's not like if Google Cloud spanner is succeeds the Cockroach TV will have no users but there are also more subtle differences in what they support So Cloud spanner or spanner in general was built within Google which has a very different development environment from most other companies out there and so spinner doesn't support sequel in quite the same way that you might expect so it's been our only supports sequel for Reed's you cannot use sequel to actually modify the data in your database do you have to use a custom API to do so and that might change eventually but for at least the foreseeable future any app that is currently uses the sequel database will need to modify itself or be modified in order to use spanner is it back end because that's the only way that you'll be able to do writes the last big difference between the systems is some subtlety around how transactions work so
 when you're doing a right within a transaction in a typical sequel database if you didn't do a read within that same transaction you will read back the date of that you just wrote so if I write a couple keys and then do a scan over and the entire table in a single transaction and most databases I would read back those couple tees that I wrote button spanner you don't read back to your own rights until after your transaction has committed which is another settle difference that will break a bunch of existing applications did that might want to just poured over directly there are some differences between the two systems spanner is obviously being run within Google for years so it's very very solid system very reliable and have a great SLA underhood service but there are there are some reasons that people can't go directly to it
 understood know that explanation makes a whole lot of sense and I you know I look forward to reporting on the space more it's very interesting sometimes hard for me to delve into it the level I wish I could cuz it's some of the lower-level intricacies are still things that I don't quite yet but I feel like I'm making progress and I think the listeners probably feel that way too so thanks for coming on the show I really appreciate it talk to you is real pleasure alright cool
 spring framework gives developers and environment for building cloud-native projects on December 4th through 7th spring one platform is coming to San Francisco spring one platform is a conference where developers congregate to explore the latest Technologies in the spring ecosystem & Beyond speakers at Spring one platform included Eric Brewer who created the cap theorem Von Vernon who writes extensively about domain-driven-design and many thought leaders in the spring ecosystem spring one platform is the Premier conference for those who built deploy and Ron Cloud made of software software engineering daily listeners can sign up with the discount code SE daily 100 and receive $100 off of a spring one platform conference pass while also supporting software engineering daily
 I will also be at Spring one reporting on developments in the cloud native ecosystem I would love to see you there and have a discussion with you join me December 4th thru 7th at the spring one platform conference and use discount code SE daily 100 for $100 off of your conference pass that's SE daily 100 all one word for the promo code thanks to pivotal for organizing spring one platform and for sponsoring software engineering daily
