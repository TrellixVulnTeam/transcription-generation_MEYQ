Transcription: data modeling is the process of creating relationships and rules about objects so that we can decide how to store them in a database data modeling defines how we store in query are database systems as well in today's episode we discuss data modeling and Cassandra with John Haddad an evangelist at datastax the company that works on Cassandra and has Sandra support the distributed nature of Cassandra create some unique rules around how we should model are Cassandra tables if you are new to Cassandra there are a few episodes in the past that you may want to listen to before this episode I would do a couple of episodes with Tim Berglund who is also of datastax by the way I want to thank Caleb Meredith and Ben Johnson from the software engineering daily Community who helped the preparation of this episode and if you want to contribute to software engineering daily and help me prepare for these episodes you can go to
offer engineering daily.com you can click on the link to collaborate that's also the road to becoming a host if you want to host some episodes on software engineering daily and you can also find links to the slack Channel or my Twitter or my email address I would love to hear from you if you're a listener I want to know what you are interested in hearing more about or hearing less about so it that after a quick message from today sponsor we will get to this episode about Cassandra and data modeling
this episode of software engineering daily is sponsored by Braintree payments mobile app development can be complex but integrating your payments no longer has to be plaques with Braintree your business can accept nearly every type of payment from any device with just one integration learn more at Braintree payments. Com / SE daily braintree's API support Android iOS JavaScript and many other languages the documentation for Braintree is clear and elegant and if you're into creating payments into your app for the first time you can call Braintree customer service and get assistance with your integration don't get frustrated when you're trying to integrate payments and monetization into your app go to Braintree payments.com SE daily and monetize your app today Braintree payments.com SE daily
what's going on with this episode of software engineering daily thanks for listening
 John Haddad is a developer evangelist at datastax John welcome to software engineering daily hey thanks for having me today we're going to talk about Cassandra data modeling but let's start out by just talking about data modeling in the abstract what is data modeling so basically you have your kind of business environment you have a bunch of stuff that you need to store and you really I guess you could say to your two things you have a bunch of data that you need to store and then you have the ways that you're going to clear it and they're very tightly coupled so the way that most people kind of learn data modeling either in school or if they've use relational databases at work is they learn this idea of like these normal forms and that's what most people are kind of familiar with that's what they come into the world with but it's really not the only way to do things like third normal form is very cool from like a correctness perspective but it's not always the right way
 go off your you know if you're building an application that lets say is tracking like billions of records with you know hundreds of millions of users or something today tomorrow this tricky things and it's yeah that's kind of the that's been nuts and bolts of it and to give a prototypical example that we can use throughout this episode let's say they were building a database for a company like Netflix and payments and stuff what are the different data types that we need to model in our data model sure if if we're coming at this from what's a that traditional you know relational third normal form perspective those nouns that you just use typically become tables and a table is something that you can insert data into and you have records and you can select from it and you can put arbitrary predicates and say I want all the movies that satisfy
 this criteria and then you go through this denormalization process where you say maybe I have multiple movie genres and so then you end up with genre table and then you also have like a table that kind of lynx movies on Rose but that's not the only way to do it so if you were to like let's say come into the Cassandra World instead of doing this kind of third normal form what's think about our data first we actually think about her use cases first and so in the Cassandra world the difference is you would say how do I intend on clearing my data and so you would end up with questions like requirements that maybe I want a query for all the movies in a particular Shawna and so you design your database to be able to answer those questions efficiently and that's what's the matter when you have issues of scale so the third normal form thing is great but it doesn't particularly scale well and so that's kind of where Cassandra fits in
 okay so just to underscore that what are the main differences between How We Do data modeling in SQL mySQL database and Cassandra okay well basically it starts with an A relational world that starts with that third normal form where we're going to say I have this data and this is the way to structure it so that I'm only storing one copy of a particular record or piece of data and that some authoritative thing and then Cassandra world the thing that's a little bit hard for people to understand it first but is really the only practical way to solve the problem is you end up with multiple copies of your data so maybe I have movies by genre and I also have movies by year and to be as efficient as possible I'm going to store two different tables and Cassandra I'm a say this is my movies by year table and this is a movies by genre table and I have all the information about the movies duplicated into both tables and the
 reason why you do that is because when you start to hit you know scale and that's it to solve scale problems when you have scale issues it's much more efficient to do one query I took fetch data out of tickets for Cassandra and then to do what's a dozens of queries are hundreds of disks and that sort of thing starts really matter I'll see your day tomorrow is actually directly linked to your ability to scale rather than being linked to your data requirements and your you know what your Purity as we get closer to talking about building day tomorrow explicitly and Cassandra we should talk a little bit about somebody principles around Cassandra and in Cassandra one thing that's important keep in mind as we don't need to worry about minimizing the number of right so this ends up with weight of a duplicate data and that's not a problem to explain why that is why is it why is it not a problem to just have as many rights week
 sure so when one thing that's very very important to kind of you know say right off the bat is Cassandra was designed for applications which have very very high right workloads and so as a priority the goal was let's make it so that when we double the number of machines that are cluster we double both the read and write performance and that's not something that you typically get with other databases so for instance in our movies application if we're writing let's save a million ratings a second cuz that's something that we you know such a company like Netflix might have to deal with and all of a sudden you know they get more customers and and they're becoming more popular they don't want to have to do major architectural changes in order to you know be able to handle all those rights so what they just in there their case what they can do is just double the number of man
 kids and they can handle the extra rights and the way that this works is well let's money back a little bit before I get back sooner works in the relational World whenever you do a write the database is constantly doing reads so the I might do a right in the database will go okay is there a primary key constraint violation are there indexes that need to be updated are there XYZ like a list of things that need to happen and that's why whenever you add indexes in relational your performance kind of goes into the toilet and most of the time people get rid of them as a scale you don't do that necklace and the world so and they Cassandra world what happens is whenever you do a right it just accept the right writes it to a commit log and write 2 as an in memory structure called a man table and then it's done and that's all that happens over the course of the right so it doesn't even check to see if there's a primary key violation
 and so this seems kind of scary at first but you got to remember the problems Cassandra was meant to solve her ones that are basically you know they're usually more like time series problems we don't have primary key violations because you're just constantly putting in new day that it's coming in from sensors or if it's ratings then it's okay that you overwrite the old rating and that's effectively what will happen and insert I can effectively turn into an update if there is a primary key you know we'll call it a violation but it doesn't actually work at you it's not mad it just over rights that data Heather principle of Cassandra that I want to touch on and this is related to Cassandra being just it's a distributed system out of the box and you one thing you want to keep in mind in Cassandra's that you want to spread your data evenly around the cluster why is that important why is that going to fit into how we build our data model
 one of those things that's actually kind of easy to get wrong and whenever people do it it really you know it really makes you have to step back in like fix your data model if you're let's say you were you were to take advantage of you were to look at your Cassandra clustering you to say okay I'm going to do a bad day tomorrow and you may not actively make that decision probably won't but if you did accidentally put a bad day tomorrow if maybe you would decide to say I'm going to use Cassandra Zach you and Cassandra is a terrible Q it's not I would never use it as you are not as it exists today it's awful for that show in EQ you typically have a list of things right like I put something in the queue and then later I take that thing out of the Q once it reaches the head and the problem with that is a q built with that mindset is effectively limited to a single list of things
 and the problem whenever you have a single list of things are very few number of things whatever you're doing reads and writes on them the way that Cassandra works is that data is always going to live on the same machine or set of machines to even if you had a 500 node cluster if you had one q and Cassandra you would effectively be talking to one machine or maybe if you've you know if you have replication turned on which you should it is on by default you'll have a replicated to like three machines so it's kind of like you got 497 machines that are being wasted and what you need to do is you need to make sure whatever you're doing your day tomorrow you need to make sure that you know if you're doing rights to three machines once you get to 300 you're going to have that same nice distribution of data because it means that you're taking advantage of your cluster so it more efficient you don't have 497 machines is wasted
 right and listeners who are unfamiliar with Cassandra baby thinking okay so I still never said why I care about the fact that the system if I'm if I'm working in an SQL database I just need to know that you know it's it's got this Rose and Andy's call Imagine This is how many access it they may not have to think so much about the spirit systems component of it but in Cassandra it's important to pick a good partition key and this is baked into how Cassandra Works to distribute system can you explain what a partition key is shirt no problem so within our data model how much like a relational database we have just a primary key and the primary key uniquely identifies a row in the partition key is part of the primary key in a partition key effectively allows us to point to a particular location INR Cassandra cluster particular server
 power set of servers and we say okay to partition key is a run through a hashing function and that determines the servers that that data lives on so if we have you know if we pick the simple one like an integer 1 2 3 4 5 we run that integer through the hash bash serve hashing function the hashing function generator hash by which points to a location in the Cassandra we call it ring because it's effectively you know most people think of it as a circle as a distributed database share nothing. Appear system and those are typically represented as circles and appoint so location in that ring and that's where your data lives and that's one of the reasons why Cassandra is very efficient air at reads and writes there's not this scatter gather operation of what we do is we typically have a row and we don't know what we're looking for but were either no or writing because we have the partition key or we know we need to read
 and you know we haven't forgotten key there so we can always go to a single server to do are so even if we have a at a 500 note cluster we don't have to talk to all 500 nodes in order to get a result
 okay so the partition key is the first element of the primary key show the primary key is composed of multiple things could you talk a little bit more about this key party like this explain why picking a good primary key is so important maybe you could talk about how and how we would pick a primary key in the Netflix example sure that's a great example so let's suppose we wanted to do something like select ratings as a good one because there was a lot of ratings for a given movie and there's a lot of great ratings for you in person so maybe we would say I want to be able to select all of the ratings for this movie right and I want to know who rated it because maybe I am doing something with machine learning perhaps you know are you skates is I want to build a recommendation engine for movies
 and what we do is we would say okay or partition key if we're going to be getting all the movies that's are all the ratings for movie we would say I want to use the movie ID as my partition key and then we had in this new thing called a clustering key and it clustering key lets me put multiple rows inside a partition so they can create a model has an extra kind of layer inside of it it's got this partition so it goes key space which is basically Caskey more or you know what database in the relational world and then we have tables which you know is very very similar to tables and relational functionally there a little bit different as I mentioned but they're the same idea and then we have the partition and so we might have a single partition of ratings for given movie and all of the ratings will be stored together and you'll basically have my user id user ID
 are the rating that was in that particular movie and then on the flip side you may also have okay for a given user I want to know all the movies that they rated so in that particular case you would say the user is my partition key in the movie is my cluster in key and that allows you to group all of the movies that a user rated into a single partition which again live on a single server Okay so we've talked through a couple ways to handle these these two aspects of Cassandra where I kind of want to we don't need to worry about having lots of Rights and we want to spread the data around evenly and we also want to read Rose from S View partitions as possible so talk about why this is important why is it important to minimize the reading from a few partitions as possible because it seems like that could potentially conflicted
 the desire to spread data around evenly sure so any any given machine is only capable of satisfying a certain number of requests or doing a certain number of operations and so if you have an option of either doing a thousand queries to all around your cluster and selecting you know data from everywhere and having to deal with different network outages potential Network outages and you know extra discs if you're still using spinning which I hope you're not or you know any any of the 500 things that could go wrong like in the case of our movies and since it's really easy to say you know what I just want to get a list of all of the ratings and all the users that rated something so unfortunately it's one of those things that there's no like hard-and-fast rule a lot of it is going to come from
 just understanding your business cases or you're in your business rules that are around your data are there are cases where you would say it's a lot more efficient to do let's say 10 queries than one because maybe maybe I can't store every permutation and every single query because if I did then you know it would result in me storing 100 or 500 times the daytime in there there is a point where you know rights we don't just say go ahead and do free rides everywhere I like it and it becomes a little absurd and so it it's it's a bunch of trade-offs it's that's really what it is distributed systems you know every system is about trade-offs but distributed systems really really feel it because it's Christmas trade-offs a while ago I'll perfect yeah exactly it's it's one of those things that that people want to pretend like there aren't trade-offs and they're always are
 you know it we can't just we can't just have perfect up time and it was distributed systems cuz there are network partitions and so we have to decide what's what happens there are we down we up like what are we going to do the same thing with our day tomorrow we you know we either say we're going to store multiple copies or in a query it more efficiently or we're going to do more queries which require more CPU and network but Less storage and that's that those are kind of the knobs that you have to turn in figure out in order to minimize the partition rate we need to fit the model around are Aquarius you said this number of times we want to fit them all around the queries in the Netflix movie database example what are some queries that we should think about supporting
 I'm well I think that the first couple that I've mentioned so far I think are you know which we can start with the basic ones which was I am I need to grab list of movies right or one movie at a time and then we say I want to grab movies by genre on a grab movies by year and then you know do you want to grab movies by genre and ear cuz you you would need to construct you would either need to do multiple queries to satisfy that or you need to kind of do some clever data modeling and take up a little bit more space you also end up with questions like who rated this movie which is our movies by user table and then you know the who rated this movie in urdu are machine learning and those are you going to always ask yourself what are people like what what do we need to be able to answer
 so this problem is definitely a little bit tricky in the movies when is actually really interesting because that kind of you no ties into a side project that I've been working on and what example data models that I'm building out is this exact day tomorrow so I have a project called the Cassandra dataset manager and effectively what it is is a way for people to be able to learn Cassandra data modeling by example because these types of things these questions that you're asking right now the same questions that every single user goes through when they're learning Cassandra how does the project teach different ways to build that model
 so the the goal of the project is to I'm really I'm going to jump out on a limb here and assume that you've heard of Apt or yum and save or something basically yeah it says it's reasonable all those tools allow you to easily install applications and so what I realized what we kind of do you know Patrick McFadden who's the chief of Angeles / datastax we talked for years now about how cool it would be if you could just install data entry Cassandra cluster for their purposes of learning and that's effectively what Cassandra dataset manager I'll just call the CDM that's what city I'm at 4 so you would type CDM install movie lens and you get it goes out and it grabs it and it describes the data and stalls in in your Cassandra Foster and so if you were like
 the first thing that you do after you download Cassandra is ask yourself like okay now what this is probably the right answer and so along with each data set you can effectively take a look at a bunch of resources that are available like there might be blog posts and there might be Jupiter notebooks and there might be Zeppelin notebooks or I think you know there's a there's there's just a million things that we can build around established datasets that make it really easy to teach things and then that way the people understand the data model can build really really good day tomatoes and kind of explain how they work and then people can build interesting stuff on top of it I have spark notebooks where where though they were the last one that I was so you can effectively start with a bunch of canned data models that are done you know in a way that will scale really well and then build a bunch of content around it so that people
 learn how to use this this database because it it is a little bit tricky but the reward is absolutely massive what are the what are some more complex queries that we might want to be asking on the this data set that would not be answerable in the ways that you explain so far we would have to do some short pants or yep and that that's it that's a really good question so I touched a little bit on the machine learning if you were too let's say I want to look at your entire data set and generate recommendations for every user based on every movie that they've ever rated that's really good example because if you've gone like you know if you've got a 50 hundred 500 no cluster and you're going to go okay I need to do analysis of this you know it's it's a lot harder than let's say in the relational world or maybe you could pull everything out and just do it in memory and who cares
 but fortunately so hit me back up a sec effectively what we have the problems that Cassandra doesn't solve very well or one that are ones that are more on the analytic side what's let's get the average rating of every movie in the database based on all the ratings you know how it looks get the genre breakdown on or do some top hey type query leaderboards those are definitely not Cassandra's historical strength and that's okay because fortunately no and we've got a ton of really good open source software especially in you know the Apache Foundation tools are spark which has gotten over the last few years have been very very helpful and complimenting other tools like Sandra unfortunately spark works very well with Sandra so the things that we don't sell particularly well in the Cassandra world
 we usually can look to spark and say well you got an analytics problem there this will complement your your database you know very nicely I saw on YouTube When I Was preparing for a previous interview I did with Tim Berglund about sparking Cassandra simpler query that we might be wanting to make on a table in Cassandra let's say we want to create the ability to query or movies by category so like comedy or drama or action movie how do I make a table that will result do I am I going to have to make a new table in order to have this query and how would I make that table
 sure so that's a good that's it that's a great example because there are a number of finite number of categories that were going to have like we're not just adding you genre is right it's just there's a last on whatever and the reason why that question is so tricky is because it's going to affect your partitions in the way that your data is spread around so if you were to have that the tricky part about this kind of ties back to a question that we talked that that you brought up earlier which was how does the partitioning affect your Foster and so if you were to pick John or category as a partition and you only have 50 categories will then you might run into a problem because you're going to have these really really heavy partitions are going to have like you know most of the movies on them in the movie database is you know it's obviously an example and and we can fit that in memory on my laptop and that's not a problem but
 you know the real world when we've got these humongous dataset and all the sudden now we go hey I have a billion items in this one partition like that's not going to work for a while and so what you do there is he typically figure out another way to partition your data into smaller sections so maybe in the movies example you would say I'll break it down by year and I'll have one partition for each genre and year so I can look and go okay I want to get the you know everything from the 80s that was in comedy or maybe I Partition by you know the first whatever and number of letters so if I'm if I'm anticipating lots of data in there maybe I'll say I'll just start with the first three letters and the reason why that's cool as you can quickly jump to any section in your list of movies and it becomes a lot easier to manage
 so let's say I instead of movies I'm I'm indexing TV shows and I wanted in the end ice TV show episodes and I want to add an ocean of time to my data model so I can know how recently and episode came out how should I add the notion of time to my data model sure so if we were to do 150 I think the most straightforward way to do this is you have a partition for each TV show and then we come back to our cluster and call him and that's where we have our notion of time so if we were to use what's a date as like day Terror door or day time it was aired we effectively now have TV shows expressed as time series so if you think of that if you think about your TV shows expressed as a list with the ordering of the list going by time and we're only a pending to the end of the list whenever we get a new toy
 the show that's effectively what we get when we have a clustering column that's done by daytime
 that's how you were gone.
 As an engineer you want to minimize scalability issues when there is high traffic to your application you need your content delivered with 100% availability cash why is the content delivery network with the fastest Global throughput every application needs a CDN whether you are in gaming or add tack or social networking or e-commerce if you're at scale you need a CDN go to cash wise.com that's c a c h e f l y. Calm and use promo code SE daily for a 14-day to Terry be free trial 65% of cash why customers use cash flight for software distribution because cash why has the fastest throughput globally for large files try out cash flight for free today is a 14-day free trial up to 2tb bandwidth and if you use promo code
 Jessie Daley you can help support software engineering daily if you want your app to be successful can't have delays in downtime so check out cash fly c a c h e f l y calm and use promo code SE daily for a discount would support the show now let's get on with the show and thanks to cash fly for being the newest sponsor of software engineering daily
 and now they were on the notion of time Cassandra is widely used as a Time series database and you mentioned that earlier why is that is very it can be very tricky and the big problem with time series is that it's typically very very right heavy workloads and so it's basic it. That's where all their databases kind of struggle is when you say I've got 99% rights and maybe I've got a million writes a second happening and they're just happening forever that's a that's one of those problems that becomes very expensive to solve if you're looking you know towards the say Oracle it becomes ludicrously expensive because now you're now you're putting everything on a sand
 and you've got you know tons of tons of extra stuff that you have to buy in order to make it work and you know it is ultimately it does fall apart Cassandra does really well here because we as I mentioned before a little bit like could have been passing Cassandra is built so that as you double the number of machines that you double your throughput so if I'm if I'm doing time series a big use case is like sensor data okay so let's I have my weather station data and that's just coming in or maybe I have like some data recording you know earthquake seismic activity and I want to get more accurate with that now I have double the number of sensors out there and so I could double the number of readings coming in and that's that's one of those things it's really really tricky because if you haven't planned for that if you didn't know you had a bigger San
 huge problem but fortunately Cassandra World we're doing everything around commodity Hardware so you would just double the size of your cluster and what do you know you can handle twice as much time series go from a disc perspective and from a right perspective
 and that's that's really why people use it is because generally people don't write less data they just keep writing more
 so if we were collecting temperature data for example if they were in a warehouse and throughout the warehouse we have 10 Internet connected thermometers what are some different tables that we would need to create the store this temperature data how would we create a data model for this type of application didn't like are we going to need to write like a Time series for each of those 10 thermometers on every database of the Waimea seems like that data could grow out of control really quickly
 yep and it definitely can so one of the things that people do that's pretty common is you have one maybe you have one partition for each sensor reading right but as you know as your kind of alluding to there's that information by itself isn't particularly useful so usually what happens if you end up aggregating it up multiple levels and you may say okay like you know I have my sensor data for weather stations but one weather station sensor data is generally useless like that the sensor may be broken or you know it only gives you the that particular location and so what you would end up doing is storing multiple copies were you maybe you'd say I have my California sensor data or my Seattle sensor data and you may have a gate into thing there or
 Play Partition by day and then that way I can say what was all the sensor readings for this particular day and that's very efficient and then the other thing you can do which which kind of you know that you brought up there you can that David can get out of control and so one of the things that Cassandra's also really awesome at and in this is like you know for a lot of people this is like the killer feature is you can TTL your data on insert so I can insert data and go you know what like after a week or after a month I don't even care about the raw data because I sliced and diced it a hundred different ways and like got everything I need out of it just get rid of it and there's not like a job like Ron or something that you have to run in the background automatic like to project for you or do anything like that it's just part of the normal Cassandra process that it will Purge your TTL data for you and see how you avoid getting out of control
 so if I wanted to store time series data from a rolling a 10 minute window or weekly window and then constantly garbage collect the rest you saying that the best strategy for that is to put a TTL on every entry effectively yeah so you put a TTL on the entries so if you if you were to eat a few Wanted only the last week's worth of data whenever you put your data and you just TTL it for a week and Cassandra takes care of the rest show what if I didn't want to throw away any of the time series data I want to store it though and is there a way Cassandra can compress it does are built inability for that so basically out of the box your let your Solutions are effectively they just let it sit there you could there there are some true
 so you can do like you could create a table for each month let's say so I'll say you don't have my July data and I write to my July data and then August comes up when I go okay I don't need my July data I can effectively take a snapshot of that table and you could put it somewhere else but Cassandra doesn't have a a means of say I want to Archive destabilizer wicker read only thing it would be very cool if it did someone wants to write that right now that doesn't exist so typically what I recommend people do is take this is where again where Spark comes in handy it's nice to be able to just say okay I'm going to take the stable and archive it in park a form and you know you put it on just some hdfs file system you know you can eat a person hdfs itself or S3 or you know Microsoft's cloud has their thing their file Block store I think it's called
 it's you know you end up getting like crazy compression because that's really one of things I park has great at and it also end up being faster to query so you end up with a really cool means of archiving old data and also making it faster to work on
 Okay so we've talked about some more productive examples, to have a more advanced discussion so I don't know if we have some insight on this but let's say I'm working at Facebook which is where Cassandra was originally developed and I think when Cassandra was developed Facebook basically had a bunch of MySQL tables or maybe you have some more insight on that but how would you make a migration like that how would you I mean because I buy Facebook or you can talk about some broader extrapolate Bowl example of migrating my scale the Cassandra but this is sort of like the data model transposition perhaps I mean how would you how would you orchestrate that migration so as far as Facebook goes I'm I don't know what they're what their Cassandra usage is
 right now I know they they purchase Instagram and Instagram is a big-time Cassandra user outside of that I'm really not sure so I think it'll have to be a little bit more vague but let me hit me back up a sexy one of the things one of the ways that I whatever you I know Cassandra is because I did several of those migrations and I I migrated away from a combination of mongodb in neo4j to go into Cassandra and the thing that I've learned in like the whatever 15 years of programming that I've been doing professionally is to like try and nibble at these problems and this is something that so it's so hard to like have self control when it comes to doing a database migration because most people are like everything
 and it did never goes well and so if you're going to you know move out of the relational world or you going to move away from Mongo or Neo whatever it happens to be and you're going into a system that you've never touched before the most important thing that you do is that you learn this new system and the only way to do that is with production usage but it's not like just throwing everything in the production and hope that it works because you know odds are you probably didn't get the data model hundred percent right and at least one of your use cases so what I advise people do when they're going to migrate from another database to Cassandra is to pick something kind of small and they prototype and you put it in prod and you see how it works and you get used to monitoring and backups and you know what does what's the impact of a t t o and what's a tombstone I mean there's a whole bunch of questions that that sort of just like come up and then you learn about the system and
 you know you move towards bigger and bigger projects and that's really the way to go this is very similar to the discussion around migrating to microservices from a monolith where everybody who is come on the show has said if you want to make that migration you start with some small non-production impact service split it out from the mall list and then and then start feeding traffic to it if everything goes hunky-dory then you start migrating other more production important components of your monolith to microservices and this is philosophically the same idea where you start to move parts of your data model to Cassandra
 does a baby this is too far for me but does graphql fit into this conversation graphql as this server intermediary between users on the front end to a requesting data and heterogeneity of databases that may be underlying that server we had a couple episodes about graphql and Falcor this conversation so it could and it could definitely make that process easier so I'm I'm also a big fan of microservices and everything that you just said philosophically is exactly the same and I think in general whenever you're trying to make changes it makes sense to do things a little bit at a time and keep things stable rather than trying like you don't pull the table cloth off the table I just hope everything is okay it's it makes things a lot easier from an operational perspective and from a developer perspective
 to work on these things incrementally having something like Falcor and graphql in front of it he is one of those steps that will make that process in your life a lot simpler because you as a if you're requesting something like if I'm a mobile developer and I'm requesting something it's nice that I don't really have to care what your doing on the back end and it's one of those tools that helps kind of you to make the back end a little bit more of a black box and I definitely worked on some projects for people intentionally expose what's happening on the the the back end and it's very obvious that you're using one particular type of database or another like maybe
 you know you pass a fragment of a sequel query into the back and in order to ask for something and you know it it's that's just I mean that's just a bad engineering design for number of reasons exposing what you're doing on the back end is definitely going to make it harder to make changes they're not to mention you know the security risks associated with just whatever give me a date of the front end wants
 you know you mentioned it this mongodb neo4j database you had the you migrated to Cassandra's you have some experience with this I don't want to talk about that or some other migration I just love to hear about some more of the up close issues about how do you and what are the changes you need to make to the data model to perform at type of migration yeah so I'd love to talk about that because it's it's it's one of those things that has a there's some hidden costs and you may not be aware of them when when you make them unfortunately so the the interesting part about this migration about moving away from Neo and moving just moving away from graph and Jen is you find that when you use something like a graph you end up with or or relational for that matter you typically end up with this really really tightly coupled back end and then
 one of the challenges of moving away from you know either monolithic services or moving away from something like a graph for relational in general because what happens is you end up having all of your data kind of interconnected and all these like type couplings and these interdependencies and that's really I think the pain of moving towards either microservices or Changing Faces like even if you had a monolith Cassandra like the playmate you still have to move everything out of relational a graph and go there our mongeau and go there mongeau is not as hard because you know there's no joins so you're you're you're okay there that wasn't a big deal for us but moving away from a graph is you know inherently very difficult because you're you're breaking these connections but you spent so much time building and there can just be a lot of dependencies there so one of the thing what are the reasons the cautionary tale for me is if you're going to use the graph date
 how to be aware of the potential problems from migrations or from just changing your data model and the backpack can be a little bit tricky I'm in the same the same advice goes with relational
 so when you were moving from Mongo or the graph database was the Wither relationships among the data what did they no longer fit the graph representation of your data or what exactly was the
 the motivation or they're so so our data model was actually a pretty terrible data model to use a graph database on it's just that the name of the company I was at happen to have graph in the end it's name that someone decided to use a graph database I'm I wish I was joking about this but yeah it was one of those ridiculous decisions bad I really didn't make any sense so we just had a bad model for graph so effectively we had is we made the trade off of none of the advantages of graph with all of the you know the trade-offs in the other way and you know if you're going to use the graph you know I actually love grass I think they're really really cool but for us it was just totally the wrong choice so migrating out of it was really easy because we just had lists of things it was like give me a list of all the musers on this message
 Mabel you know as we talked about already that's a trivial Cassandra clearing that's an easy day tomorrow but asking the graph like you know give me all of the people like how do I say how do I know you what say we had never met and I asked to grab how do I know you and just find a path or find all the paths in like four degrees of separation to answer that which is really cool but we weren't doing any of that so yeah what's the most challenging data schema that you've ever modeled
 you know let me think about that for a second I did some really kind of tricky stuff with a
 a large so one of those this is actually before my Cassandra days I had A7 level taxonomy that I worked on for project it's still up but none of my original code is there I'm sure for answer bag.com and answer back was like a Q&A site that was similar to do Yahoo answers and we were relatively close in size of time and there was some tricky things that we had to do like to be able to look at any top level category and see all of the questions in all of its subcategories and like sorted by rating and filter by a handful of different things and it was just really really tricky because you don't work we were on slow Spinning Disk so for us like minimizing the disc seats was a big deal because you're you're effectively Furby seek you're paying you know she had a minimum
 5 but more realistically like 10 milliseconds and you know you got to get your data model like perfect there especially when you dealing with tens of thousands are hundreds of thousands of requests per second that could come in so you kind of have to get really really like creative with your cashing and you know you start you actually start to design your relational data models in the same way that you design Cassandra day tomorrow. That was actually why one of the reasons why I considered a tamale made so much sense to me is because I had built I jump through enormous Hoops to fix relational data models that weren't scaling and you know I start designing all these tables to answer specific queries but then I've got like still all the overhead know the trade-offs of relational like it doesn't really cluster at all you know that there's a whole handful of things that that you tried off and I really didn't see any of the benefits and so when I saw
 looking for Sandra I was like oh so these exact date of models that I built over here in the relational world to scale will work plus I get all the distributed awesomeness and like the fault tolerance so that was kind of like that was my my like life Journey from the relational world into the world of disturbing systems nice listener questions to wrap up with one is how does client-side cashing affect data modeling
 so I take it that's a good question and I like any reasonably good question the answer is of course it depends which I hate but it always is that way so the the the tricky part about client-side cashing is you it it depends on what you're cashing so if I'm doing something like I'm cashing one row and I'm I'm or not maybe not one row but I'm cashing individual Rose well in that case it may not be worth it actually do cash and so it's like okay I can cash one row or I can go to the database and cash one row but my database just kind of scales for me anyway so what's the point of cashing like usually put cashing in front of relational databases because sharting relational databases is hard so let's just put cashing in front as a Band-Aid and you typically don't you don't really need that dick
 and it's on the systems that I've built with Cassandra I've skipped cashing all together and I just go straight to the database and I have more efficient animals but there are times where maybe you're like you know I want to cash some like hard to compute thing like you know I've got some admin panel and you know maybe I can ask maybe I don't know all the questions that the user is going to ask but I do allow them massive flexibility and so maybe it takes 30 seconds to compute something and I just say okay I'm going to cash that and I just deal with the fact that maybe my cash is stale sometimes and that that's kind of that's where you end up but like I think you know I I I keep mentioning I have to keep mentioning this you know because since this isn't a database is optimized for rights there's
 very few cases in like iot where you're going to cash your data because if you need a bus that cash as soon as it's changed like if I go while I want to cash my check
 you know the last ten seconds of data will you if you get to steal cash read it doesn't really help you out at all so I think a lot of what happens is for high right volume stuff people kind of dropped the cashing
 and you know if you've got if using Cassandra for more of the high availability side of things rather than like it's scalability attributes then maybe you'd put some cash in front of it in order to make it a little bit faster but you're not going to gain a butt ton but you're not interested that's a very interesting question well John this is been an awesome conversation it was really a pleasure meeting you at where we meet. It's been really cool talking to you
 thanks to sinfonico for sponsoring software engineering daily symphonica is a custom engineering shop where senior Engineers tackled big Tech challenges while learning from each other check it out its symphony.com SE daily that's s y m p h o n o. Com SE daily thanks again Stefano
