Transcription: a dashboard is a data visualization that Aggregates metrics in a way that we can quickly understand in a modern software company everyone uses dashboards from sales people to devops to HR each dashboard represents a query that must be updated frequently so that anyone looking at it is getting up to date information the data that's being ingested and turned into a dashboard might be getting updated really quickly in the case of Time series or log data some of the queries might require joins between disparate data sources so building a dashboard is not necessarily an easy task how do you keep the dashboard accurate how do you keep the query latency of every dashboard down
Tom O'Neill is the CTO of periscope data a company that makes popular dashboarding tools in this episode Tom explains the data engineering that underlies Periscope data we explore topics such as cashing columnar data and redshift we've done many other shows about data engineering including shows about how did engineering works at companies like Airbnb and giphy you can download the software engineering daily app for iOS to find all of our old episodes and they're organized by category we have a recommendation system that will give you recommendations based on what episode you listen to and upvote within the app you can upload those episodes and find new shows that you might like based on your listening history because with 600 episodes it's kind of hard to find the episodes that appeal to you easily we have the app helps with that we also have a search engine within the apps you can search for topics them
appeal to you the IOS app is an open source project it's the first project come out of the software engineering daily open source Community there are more projects on the way the community is growing and if you want to contribute yourself you can go to github.com software engineering daily we've got a slack Channel That's hopping and we're working on an Android app IOS app recommendation system a web front-end whoever you are you probably have something that you can contribute to software engineering daily open source app ecosystem and we would love to have you were trying to build a new way to consume software engineering content and it's all it get hub.com software engineering daily thanks for listening and let's get on with this episode
square is a complete omni-channel payments ecosystem where developers can create a powerful Commerce solution using square around at your favorite coffee shops and Retail Edge trusted by millions of businesses for its Simplicity and reliability but as a developer you can unlock the potential of a full payments and business management platform with squares API at software engineering daily. Com Square there a PS4 online payments taking in-person payments with square Hardware 3 custom app that you build an even inventory and employee management and sales reporting businesses of all sizes use squares API to take payments securely online in-store and on the go with a single solution and integrate with back-end systems that they already use check out squares developer
for all the tools that you need to get started today take squares Commerce Solutions beyond the coffee shop and power payments for any type of business by going to software engineering daily. Com Square it's a really flexible platform for building a business with payments and everything else that you need to get start with a business and you can check it out at software engineering daily. Com square that suffering daily.com square and would support software engineering daily so thank you square to being a new sponsor of software engineering daily
Tom O'Neill is the CTO and co-founder of periscope data Tom welcome to software engineering daily thank you for having me go through the discussion of what Periscope data offers in terms of the abstract product category which is dashboarding tools visualization tools and then we'll get into the engineering of periscope data what makes the product unique and we'll talk about the company a little bit starting at that high level product perspective a dashboard is a visual representation of data what role do dashboards play in a modern technology company that's really about the culture you're trying to build if you want to build a data-driven culture the only way to do that is with data and showing at data around the company and so the more you can get data out of sort of faction servers need to the hands of decision-makers
how to fast your company will evolve to make better decisions and dashboards are created just for internal use or can it be part of an externally facing product feature of both for sure in fact in Periscope we allow our customers to embed dashboards they building a product in their own products so that as they do their internal internal analysis they can make that publicly accessible through their own portals okay so how are dashboards created they really start from a business need like somebody and management or leadership wants to know how's there what's going on with the churn or where new revenues coming from or what to invest in and that generates a bunch of work to figure out like what is the spike and adjusting metrics or figure out new metric to record and analyze and then from there you build a dashboard which I think the day it's really a story much more than a graph so there are sales people there's marketing people there's HR people in a technology company they're all getting more data Drive
but do these non-technical people or at least traditionally non-technical people do they know how to write a query to turn their question into a dashboard more and more so they do in fact we sell them to very tactical teams I companies and one of my favorite stories I've been really customer there a video game company and we sold into the parking team who was primarily building analysis with Excel and one of those one of those marketing analyst should have fell in love with sequel on the product and over the past couple years has become an expert analyst to know runs their data team and so through that sort of exchange you can really elevated from just being able to do pivot table to doing really Advanced analysis so how old is the what's the difference between how a salesperson or a marketing person and an engineer what's the difference in how they want to use a dashboarding product it really varies
vitalsource write a salesperson probably wants to look at Salesforce data I'm working person wants to look at their final in the campaigns and and how leads are tracking and Engineers are going to want to try to look at product metrics or server metrics and end of the day they all might use the same visualisations but the way you get from the raw data to that sort of ready dashboard is very different
 alright so give me a high level product review for what is needed to actually build a dashboarding product one of the later stages it starts with ingestion are you up to get the data from somewhere because no date is born in a warehouse and so once you build the ingestion part of your data platform you get it into some kind of Warehouse in as many good options for that and then once it's in the warehouse you do a lot of modeling and Analysis and that's where you convert basically unstructured or or raw should have columnar data into things that actually matter to the business right what is a user how do you define a subscription and then from there I you do further analysis to answer specific questions and that yields your visualization snow specializations become a collection on a dashboard
 and what kind of visualizations do you need to offer for the stator you can actually offer a pretty small set so that the hardest part of data is turning something really complex into something really simple and so by limiting yourself so only using like bar charts and line charts in area charts and things like that it really helps make the narrative understandable to a wider audience where is if you invent really crazy visualisations to invite specific you might not get your message across now there's a lot of different data sources to pull from these days you've got basic sequel tables you've got Amazon S3 the consider that a data source you've got apache-spark got all these different sources who's there a lot of work to be done in building connectors to all these different data sources if you want to make it easy for your customers to ingest the data and put it into a visualization how much it in the way of connector
 you have to build the connector ecosystem is is pretty crazy it's certainly dominated by a head right there is your corsico products your course ass products want to connect to and then a very long tail of all the show to Brandon's house products exist and of course all the other one off data sources and so what we do is we Supply connections to the core most popular Tequila source and then support Nick assist him we're Partners can plug in data from any other source
 so tell me more about about how that ecosystem works that the data connector ecosystem sure so ideally you don't want to write like the connection to Salesforce more than once there's not true value in the world for a hundred companies to write the same connection because the Salesforce API is pretty static and so I think the interesting part is once someone writes that code how can you reuse that and get that data into all the places where it makes sense to do analysis
 okay so that's how you got a connector built you've got a visualization tool going how do you test that the visualisations that you're presenting are rendering that data correctly so testing in analysis is actually really hard it's much harder than in normal software engineering you can do unit tests on Mexican validations and data validation check to make sure there's no nails or no or without a column is unique but making sure that the actual data rendered in the output is right really takes like human validation and sosuke Villanova code review for my secondary analyst to make sure that the assumptions you made in the decisions you made in your in your sequel makes sense so there's been a lot of generations of these data visualization tools bi tools dashboarding tools I want to ease our way toward the conversation around what differentiates Periscope data
 why is there so much churn in this in this area I need 5 years ago or 6 years ago Tablo was all the hotness you 3 years ago maybe it was something else and then now I think I'm sorry I read a lot about Periscope data being being quite popular why is there so much turn sure yet so there's there's to have the technology in a people the technology being that these massively parallel systems are not possible to adult attractions in the Snowflake and the other is that professional David teams didn't exist until recently as soon as we have these people that are trained and able to operate such complex systems to do more complex analysis okay and the broader technological ecosystem that companies that are getting started today in many ways they are reflections of what a Microsoft or Google had in their infrastructure and in their Tech stack maybe 5 years ago 6 years ago so you know you're about to Google infrastructure for everyone
 that's typically referring to Google infrastructure 8 years ago for everyone today and I think that probably extends to data analysis tools and everything so Periscope data was started in 2012 What did the what did the state-of-the-art data analysis tools look like back then I think you were at Microsoft then and you were looking at this ecosystem you're saying will the internal tools here are so much better than the external tools maybe this is a business opportunity very much so the the tricky part with the internal tools is that they tend to be very specific to internal data sources and internal work and said that the challenge was not to come out and invent a new kind of pivot table a new kind of dragon drop product the challenge was to come out it's okay we know what's possible we know it's possible to analyze terabytes to petabytes of data very very rapidly using sophisticated tools how do we do that for modern businesses instead of just internal like search and add sources can you talk a little bit more about that color
 these are the internal tools at being an Google where are based largely on sequel rights equals is a natural language for doing work or analysis and those tools were very much like some of the tools you see today with these massive clusters of compute and the separation of storage compute and Saudi the challenge wasn't could we invent that the challenge was we knew it could exist and so how do we make it possible for modern businesses to leverage that scale of technology to do the analysis that we knew was coming because even if even if the only way I could buy two terabytes today like petabytes is going to happen tomorrow
 yeah and I think it's around this time I'm trying to put myself back in 2012 I guess this was around when people are getting on board with a Dubai and people are still getting on board with the Duke today what if you wanted to if you're a data analyst or a marketer data-driven marketer and you wanted to get a question about the biggest data answered you would have to go to the data warehousing expert and ask them for the Hadoop job results for X so that was like the the early I mean we think about that now and it just it probably sounds pretty archaic to a lot of people cuz I think a lot of the data infrastructure is become a little more soft serve 4 people in marketing for example and so did the company's back then like the the startups back then for example the type of companies that want to take advantage of a periscope data today did they have the data volume problems or did you just see
 Lowe's on the horizon and that was you know what kind of made you think they're going to need some data analysis tools that can do this kind of clustering and so on that's a good question if we looking across our customer base data volume doesn't correlate with company size or age as much as you might think it would we've seen like 5 person video game companies with billions and billions of rows of data Plumbing in every day just because they're operating a popular game and it's instrumented so thoroughly that the event streams are really really dense and of course you're older companies who have an instrument did and even more popular product all that well and you're just dealing with like a replica of their production database which might only have like a hundred million roses in it so can you give me a little bit I don't typically ask about the company backstory the origin story but I think it's particularly interesting in Periscope data tell me a little bit more about the origin story black when you were at Microsoft and I think your co-founder was a Google give me the diff
 clean that scenario and when you guys ended up starting Periscope data sure yes a periscope was not an idea when we got started we were we were College roommates and we graduated we went off to Google Microsoft and it was a Sunday plan that someday we start a startup and three and a half or so years later I'm a cup under Harry calls me and says hey Tom I quit my job I'm moving with my backpack in my girlfriend's to Southeast Asia let me know when you're ready to move down and so like I was a pretty strong like okay yeah like now is the time and about a month or two later I'm driving down from Seattle to San Francisco and he and I get started and we work through a bunch of ideas turned out to be terrible ideas for these Solutions in search of problems Lucian is hurt your problems ideas and we we come to an idea when just before Periscope that worked and it was a mobile app and it was pretty successful at many thousands of users and of course a lot of data associated with that and we want to know what are users were doing
 and when we go to the tools at the time they were very like they drag and drop Brighton measures Rangers and dimensions is like what what's going on here we just want to write some code and see our data and so we met at Periscope for ourselves in the model of what we had seen at the Google and that I should be going to take off because some friends saw what we are building okay can we have this in our business no more like no no no it's a typewriter to worry about it and yeah that this happened a few times and folks are like offering to pay us for the side project on it okay this is real I let's do this and so that that was the Genesis of periscope and it took us probably a year-and-a-half after realizing the side project was clearly super valuable to turn it into the V one that ended up getting like product Market fit and really taking off
 Amazon redshift Powers the analytics of your business and intermix. IO Powers the analytics of your redshift your dashboards are loading slowly your queries are getting stuck your business intelligence tools are choking on data the problem could be with how you are managing your redshift cluster intermix. IO gives you the tools that you need to analyze your Amazon redshift performance and improve the tool chain of everyone down stream from your data warehouse the team at intermix has seen so many redshift clusters they are confident that they can solve whatever performance issues you are having go to intermix. I o s e daily to get a 30-day free trial of intermix intermix. IO gives you performance analytics for Amazon redshift intermix collect all your redshift logs and makes it easy to figure out what's wrong so you can take action All In
 nice intuitive dashboard the alternative is doing that yourself running a bunch of scripts to get your diagnostic data and then figuring out how to visualize and manage it what a nightmare and a waste of time enter mix is used by Postmates typeform udemy and other data team who need insight into their redshift cluster go to intermix. IO / SE daily to try out your free 30-day trial of intermix and get your redshift cluster under better analytics thanks to intermix for being a new sponsor of software engineering daily
 data visualization I think that their perspective is that this is mostly a design problem this is like your building stuff on top of d3.js and you're just making interesting visualisations
 but actually in between the data source the database and the front end visualization that you can just load up on a web page there's a whole lot of Middle where or what I don't know if you want to use the term Middle where I can use the term middleware data layers intermediary data layers cashing layers that are necessary to get good performance in the thing is performance in a data visualization is really important because you don't want to be seeing the data from 10 minutes ago you want to know what kind of monitoring problems are existing right this minute so query latency is a huge deal because every data visualization is essentially a graph a graphical representation of a query and so query latency is going to be your bottleneck in many cases so if I have a dashboard and that lets you say broadly speaking not talking
 Periscope data specifically let's just say abstractly I have a I have a sequel database and I have a dashboard that is pulling from that sequel database and that dashboard is slow to load what are my options for speeding up that query is it is a few different places where I could be slow right it could be standard like web servers though that's easy enough to fix but work is really interesting is if the temp today The Courier running to populate the dashboard is itself slow it's it's a Corey that takes 10 20 30 seconds the really the way to do that is to either to Hardware or through pre-processing like your modeling and so with Hardware course you can get bigger cluster is bigger columnar distributed data stores and with pre-processing you can do Roll-Ups you don't need to calculate yesterday's Daily active users over and over again you can calculate it once and say that and only calculate like today is to save a bunch of time for example okay what about cashing what are some of the different cashing strategies other cash and strategy that you can use
 tube to speed up queries I guess I guess what you just described as kind of a type of cashing but talk a little bit more by cashing so you can cash at different layers of course you want to cash the results of charge themselves right so when you rip so when you reload the dashboard you populate the previous result in immediately and you can refresh the old results in the background but what I think you referring to is cashing his how to get the data into the warehouse right I was talking pretty abstractly so yeah we can go ahead and talk about the talk about cashing in the data warehouse so as we discussed before did it isn't born in the warehouse until you want to get it in there and sometimes it's really easy right there just an event stream it's independently type table and you can grab the new rose incrementally oh that's the most most straightforward approach the streaming data right to the warehouse work It's Tricky is when like you want to load your users table into the warehouse from a like a like a proud replica and now these are table isn't append only it's it's getting new rows of course but old Rose are also changing and soda
 the tricky part is figuring out how do you identify the rose that have changed and update those in the warehouse as fast as possible because it might not be might not be reasonable to reload the whole user table over and over again like every every 10 minutes, I think there are people who are listening to don't know a whole lot about the difference between a data warehouse and a production database can you give a little more color on the difference between those two things sure so just for the sake of the example of compare postgres to redshift to redshift uses a similar dialectics but they're supposed to say for and so the difference is will be more obvious suppose grass is a robe is store when you look up a user's record right select start from users where I'd equals 5 they can do one seek on disc because all the columns in the table are store next to each other in fact the whole users table stored in one flat file on disk and that makes it possible to use multiple indices in the same table use primary key is in that way to hop hop down to the certain
 and really load up a single user really really fast but if you want to find the users with a certain birthday right and you can you didn't have indexes for example you have to scan the entire table right birthday is not it's not to call him to take the index is okay to find the minimum or maximum birthday you have to look at all the user records and doing that means reading the entire table because you can't just read the birthday column cuz it's stored in a robe is storm that's typical for the kind of database you would use to it to host a website for example. I really don't care about all these really care about one user at a time
 Warehouse is quite the opposite Warehouse Curry's almost always care about all the rose but only a few columns because most recent is stored in a separate file that you'll have the the birthday common in the birthday file the id, id file and it means you can't use indices in the same way you would on lactose best database because the files are distributed across notes but you can just read one Collins worth of data as a bonus because you're only storing a single column profile you can split the final too many chunks and distributed around the cluster and then sip those trunks because it's all the same day to type it's all integers are all dates when you say zip you mean like a like just stuck a distributed query like a buck pulling them all together or what exactly do you mean no like I like Jesus you cash it can press the day that because one of the biggest factors in database and Warehouse performance is Ajo and so with rob a store you want to read the whole row contiguously and it Warehouse you want to read the whole column
 wrestling right now I I get that I'm just a little confused on the on the zipping part why is why is zipping unique to the to the data warehousing side of things when the column is all the same data type all integers are all dates are all astronauts compress the file okay right because if you're doing compression always going to get a better I think compression ratio the more common the data within the the Corpus of compression is correct right so for people who don't know much about this columnar versus row based when we've done some shows about this in the past we did a show with a couple guys from dremio which is kind of a stealth date of company but just to just to put a little more color on this cuz I think some people are still probably a little unfamiliar with this you got a typical user database where you got types of data like age name you know
 date of last login those are all different columns in a specific row and so Tom's row is going to include his age his name and so on you can take that database and reconfigure it as just the rose of the database so you just have all the ages of different users but you don't have the other data associated with that particular user and the advantage of that is if you wanted to do something like chart the ages of the users in your database overtime you don't need a lot of there's a lot of data in the database that you don't even want to look at and so internally when a database it you know if you're just have a naive sequel database with all of those columns in it then Aquarius going to take a lot longer because it's got a hop over those areas that it doesn't care about as opposed to if you have a columnar database you can focus explicitly on that data that you're concerned about
 am I articulate that correctly exactly right and you can see the difference in the performers characteristics of very simple Curry's so if you did select star from users where I'd equals 1 a basic like give me all the information for user wine in my database that'll be like 1 millisecond in postgres but I could take 30 seconds on the red shift because I have to open some reason files and data spread all over the place likewise if you want to some the ages of all your users that could take a long time and push press has to read the whole table but in red shift it could be nearly instant because it will just read the age date on
 customers that come to Periscope data and they're like a cow want the dashboarding tool do sure that the best Ash boring tools available I'm going to get Periscope data do they typically have red shift already in their infrastructure or do they just have a sequel database and and Periscope takes care of pulling that sequel database into ratchet what the tip protocol infrastructure that the customer comes to you with it really varies by the date of maturity of the company and so y'all companies who have many different databases and want to join across them and they said they don't already have an internal sort of ETL pipeline set up and some Periscope I can help with that I likewise well customers who already have pretty sophisticated data pipelines and just want to use Periscope as a modeling and visualization layer on top okay so talk a bit about like people who come in they don't have red shift they just got to have got a sequel database as I understand
 you're pulling you will pull their database into redshift to get a cash in layers that correct right that building building of his system and he shall system on top of a regular to the production class database like a postgres you're my sequel can be pretty difficult because the performance really breaks down as you create more domesticated volume goes up and see what we do for customers before they start building dashboards is when we're in all the data from their various databases into a single redshift database and with that they can do their modeling with Interscope and of course joint across all the different databases and build dashboards from there
 and talk more about that Ingress like what happens under the covers when the customer comes in and and you're going to pull their database into redshift it really is that something we can set up the customer said supper table they did they decide how they want to sink in there did a how much of it it there didn't want they want to sink in to so our back end and I can run on any schedule they like based on how their own internal policies run and some customers like to do nightly jobs and customers want to date up-to-the-minute depending on how the tally origin data sources working and of course his priorities between which data source mirrors into the redshift on which schedule can you talk about some of the challenges of getting that process right for sure I think one of the mistakes that can be made is a lease with with other products is they try to do the modeling on the fly right you have an origin date of store you try to model it and then put it into a warehouse and you never really quite sure
 if the error that you find later on is due to not getting the right data or is it in the modeling layer that has would have noticed rounded or is there a problem with the analysis and so what we focus on is mirroring the exact schema right all the columns all the rows everything that you have shown to Periscope we married in so you can know with a hundred percent certainty that the warehouse is starting from a good place and then when you build your modeling layer you can make sure that those models are accurate based on known good information
 and how I guess it describe a little bit more about why that process helps with query latency for people who can't make haven't made a connection yet for sure I'd really has to do with how much Hardware we have behind the scenes right if you if you become a periscope customer and you have a couple billion rows of data may be stored in a my sequel database your cruise are probably taking several minutes to do the basic things like daily active users and retention insurance
 but we're going to mirror that into our wretched clusters and we have thousands of notes about 25 terabytes of ram across all of our clusters that make it possible for us to do the same queries sometimes subside so if I'm a user and I've got these queries that I want to run that I want to turn into into dashboards do you what kinds of a cashing are you doing specifically for the query that I've got several attack I want it I want to have a query that's got the active users on a website on my website any given time does that change the underlying infrastructure does that change what you are cashing more aggressively in red shift in that case will certainly be cashing like your activity tables much more aggressively right because you want to get the activity of the end as fast as possible but there's also a bunch of modeling going on behind the scenes because activity is not the same as an active user you might want to scrub out like your own
 internal users when it might want to start out test and development users and so you want to layer between the the raw data source and the visualization that lets you define what is a user what is an active user and then graph that instead why are using red shift because I feel like I've started to hear more and more about Reggie I guess people have been using it for a while I have not reported on a very much but it seems to really be the data warehousing tool that is most popular explain what makes red shift so useful
 the red shift is why I guess was one of the first I truly hosted like managed warehouses where you could just you could throw date at it and it will help you scale to quite quite a bit and so we invested with The Wretched team quite early we're on a first-name basis with many the folks over there and it's really been a fantastic partnership working with them to switch scale are you stay so long with scaling there there was a shooting there road map and so while there are definitely other day to date or houses out there that that do a good job right now for us and our customers redshift is the best fit and are there any the pricing is it is not an issue for you cuz I hear that sometimes when companies are using red shift or in this is true purpose cloud in general at the economic Times the economics of their business are such that it's it's hard to justify using be expensive Cloud infrastructure but it sounds like the cost is really not an issue for you that's kill the car
 really works out for us and something else to keep in mind is you really don't want to treat all your data equally write some data should be in Cold Storage some data should be in S3 some data should be pulled from S3 when it is needed into a hot resource like redshift and so you want it you want to tear your did Alex so that your cost effectively using your storage in your computer
 and how much tuning does the user have to do of redshift if they're using Periscope they don't do any tuning cuz we handle that all behind the seat if they're using redshift on their own then they want to be setting up a sort keys and discs would affect how did it gets distributed around the cluster and up that of course doing all the usual database maintenance like new vacuuming analyzing on regular basis and when you when I use your comes to you with their data and it's not in redshift is the redshift configuration the cashing layer that you build for them is that all done automatically there are there cases where you have to go in and do some manual stuff
 are we generally don't have to do anything manual the customer just set up a schedule of how they want to marry their didn't and on what frequency and how are the API for red-shifted because it's it's kind of cool that you've been able to build a business around this product I thought I mean I thought of redshift is more of a something that a company would just use short of internally to do data warehouse inquiries but this is like a case where you're actually really using it as a core piece of infrastructure to build on top of their automated fashion
 yes OD like all AWS Services redshift has a really thorough API how you interact with it and how you interact with scaling and and create a new clusters and of course ratchet itself is just a database and so when you communicating with it it's just a jdbc driver and then normal sequel is pricing an interesting discussion here like I imagine you're figuring out the right way to price a user so so that they don't get burned on the cost too much but also you don't get burned on the cost too much medicine It's Tricky how did you arrive at the right pricing yes it is especially tricky for us because I Periscope is a platform it's not just a visualization - 22 it also includes this warehousing component and so there are really two halves to the pricing model because we want to tie the value of the product to the value that the customer see in the product and not at all based on like costs because that's that's not how you operate a pricing a product and so from the or housing
 are we charge based on how much data volume you're using right and that correlates to how much value they're getting out of their warehousing half the product and of course on the bedside we charge based on seat so how many users are actually using the product and getting value from either building graphs are consuming graphs
 who do you use for log management I want to tell you about scalar the first purpose-built log management tool on the market most tools on the market utilize text indexing search and this is great for indexing a book for example but if you want to search logs at scale fast it breaks down scaler built their own database from scratch and the system is fast most of the searches take less than a second infected 99% of the queries execute in less than a second that's why companies like OkCupid and giphy and Careerbuilder use scalar to build their log Management Systems you can try it today free for 90 days if you go to the promo URL which is software engineering daily.com scaler SCA
 why are that's software engineering daily. Com scalar scalar was built by one of the founders of rightly which is the company that became Google. And if you know anything about Google Docs history it was quite transformational when the product came out is this was a consumer-grade you are a product that solve mini distributed systems problems and had great skill ability which is why it turned into Google Docs and so the founder of Riley is now turning his Focus to log management and it has the consumer-grade UI it has the skill ability that you would expect from somebody who built Google Docs and you can use scalar to monitor key metrics you can use it to trigger alerts Scott integration with pagerduty and it's really easy to use it's really lightning fast and you can get a free 90-day trial by signing up at soft
 engineering daily.com scal why are software engineering daily.com scalar and I really recommend trying it out I've heard from multiple companies on the show that they use scalar and it's been a real differentiator for them so check out scalar and thanks descaler for being a new sponsor of software engineering daily
 what I'm sitting in front of my dashboard throughout the day so if I'm a marketing person or salesperson and the traffic patterns on my website are vacillating throughout the day they're going up and down I imagine that's putting different types of load on the data infrastructure throughout the day and that's going to propagate to the query latency naive the naive query latency of my dashboard how does the Periscope data infrastructure adapt to the Thirsty nest in the spikiness in the change in traffic pattern throughout the day that's a really good question of clusters they tend to be multi-tenant so will put multiple customers on to the same Hardware of course separated separated very specifically for security reasons but they're on the same physical machines and we don't put all the customers in San Francisco on to the same cluster
 we have customers all around the world and we have B2B customers and b2c customer is nosy usage patterns are different and see what we actually do is we balance the time zones and the use cases of customers on each multi-tenant cluster so that customers can timeshare a supercomputer and not compete with each other that much all right will let's get into more infrastructure discussion so we've talked a little bit about the connection between visualisations and cashing I want to get a better picture for the infrastructure how you service user requests how you scale I just give me a picture for how your infrastructure looks yes I'm most of our structure in the back end Argo microservices and so they they they operate and they threw talk to each other through various shapes and their managed by kubernetes of course the front end is all JavaScript
 okay so kubernetes running on AWS yes that's correct okay it ain't is there anything interesting there and because if you started in 2012 you must have gone through a migration to kubernetes what were you on before kubernetes Oh the days before coming it is we're not we're not good days we were doing a lot of it and Julie and I and scaling in ways that we're a little a little little rusty around the edges but but now with kubernetes at the system is much smoother can you tell me any nightmare stories about that I can tell you about one of my favorite microservices it was originally called the sorcerer because it's stores data sources and it was built in a world before kuna days before her uncle's Kelly and so it was one box storing all of the the chart data because we would start charging to separately from like the production database or the warehouse and as we started scaling the quell past 800
 stores like it became obvious that this server was not going to keep up and of course you don't want to have all your chart data like stored on one server this is not this is not a good system and so one of our Engineers should go over the project and she built what is now called the Sorceress so along with upgrading and become a car. They schedule my car service had a gender change and now it is it is much more performant and it doesn't really really good job for a customer's great so what are the other SAS tools in the infrastructure tools for the cloud services that you're using dry your infrastructure
 what a my favorite tools is scalar we we stream all of our logs to it and it helps us with production monitoring and alerting my favorite part about it is like searches instant like you can you can query a week's worth of logs which process is a lot of logs and the results come back almost instantly and how does that compare to how you were doing logging but I should I guess I should say scalar is a feather about to be going to sponsor software engineering daily so that's cool but not as like sponsored content like how does that compare to what you were doing with log management company we're logging is pretty important I meant to give it to one of logs cuz there's so much data coming into the system what was your log management solution beforehand
 yeah I think that the thing that really changed for us with skeller is that we could unify are locking and are learning where before they were separate we have like an alerting service that would that would check on various things and then you know Paige people there was a problem and logs Resort elsewhere and dealt with separately but now because we like so much we can also just log Health metrics and then in scalar there are filters that will trigger on certain criteria to help my check is out of whack and then when something touches you but you have a lot of showing what actually happened at the same places where the trigger cause the page
 can you tell me more I want to hear more about this the process of scaling is company cuz you were starting 2012 5 years ago sounds like there were bursts in popularity of periscope data what was the hardest problem in scaling the company that you encountered I think the the challenge has always been identifying and instead of staying true to who are core customer is there are so many tools on the market that are going after less technical users right who want Dragon drop who wants my dream dimensions and we've always had so much success with the Dallas and now the data team that focusing on them and making sure that we're showing them the best tools possible has really been a big part of the challenge and also a big part of the excitement and that's that's why it's Periscope is killing
 so you're you're you're you're putting a stake in the ground you're saying we are not for the less technical users we are for the technical users who can write their own queries the data teams actually I would say that we are about enabling did it seems to serve those less technical users because in a world where you only have is less typical users doing analysis like you run it to challenges where the office might not be correct like for example we're in different time zones and let's say I'm building an analysis for you it's a daily revenue chart and what I sent it to you like what time zone is that Revenue going to be in is that matters how many dollars we made on Tuesday and of course did I include both databases because the Legacy product is in the Legacy database and oh yeah 2 months ago the marketing team ran a campaign and gave away a bunch of licenses and the way that's loud in the system is a bunch of revenue revenue table in a bunch of credits in the refunds table and this like they didn't used to be this hard in this complicated and so the reason
 a professional teams is to deal with that can text Ian make the data accessible to the rest of the organization and so Periscope is really about enabling day two teams to to Wrangle Beckham text Ian Rangel. Volume of data so that the West Oak Hill users can being able to make the best decisions and how is the product management and development teams interact so how do you work where is where are you getting the feedback loop between the customer and your engineering team the whole company is incredibly customer-driven we have a five-second chat response time where any customer can chat with one of our data analyst immediately the sales team is constantly feeding back feedback the account management customer success teams are currently feeding back feedback and it's all goes into our road map where the part of teams that have right angles all these sources of data explorers new questions with some of our strategic customers and turns that into a road map and what about dinner
 9 what role does design play cuz you've got your building at visualization tools got to be well-designed how do designers factor into the process design team is embedded with the product team there should have won Super team and design does a ton of research both competitively and looking for new new usage patterns as well as a bunch of user-testing so once once we've been applied a problem that is where the business that the product team as a whole wide and if I should have who was already doing that who's good at it what can we learn from and then do we do a bunch of testing both internally and externally to make sure that the solution we come up with solve the problem I'd like to hear your thoughts on some other data infrastructure type of stuff she like I think of spark apache-spark it seems like something that could potentially be useful to this type of problem cuz I think of spark as this tool that can pull in can you know distribute
 memory to get a working set of data into into memory and you can get a variety of queries answered against that have you evaluate apache-spark as something useful to use absolutely and that's one of the things will be in a grading later on in the process as we expand out the data Pipeline and it really comes down to operating the right data at the right part the process where were in the beginning we spoke about like connecting to Salesforce right back hose going to be written in some sad we typed language pulled in a regular basis and almost never changed and likes pork chops be written in a more sophisticated language than sequel typically harder to operate cuz you want to be operating at a massive scale and doing Transformers that sense and Spark will be doing a lot of the heavy modeling and another guy hand it off to your signal analysis which what does you're at your groups and accounts in your filters and of course after that you might have a python or are doodoo some Transformations or some projections at the very end of the day you cry
 these these State experiences that can then be filtered and pivoted and drilled into and each step of the way you can you can see how the the engineering gets to have more flexible last less rigid but and the systems became much more rapid to I respond to user feedback and I imagine you see Periscope data as your window into this variety of schools where you can just write a write a query that feels simple on the outside but under the covers it's pulling together an entire workflow of different tools yes very much like the person that form is a workflow platform where will help you go everywhere all the way from ingestion to like visualization and Reporting
 I need workflows today in end up in a big company there I think they're managed by tools like airflow and Luigi is that right this is kind of a very technical tools yes but in in fact air flow is only one piece of the work though airflow handles a lot of the modeling but you have to give David a phone if it's held in Florida put it and so it's missing congestion pieces missing the storage peace and so we what we see long-term is Periscope to either integrating with our flowers need like it so that you can do that style of modeling in the product I put also have the induction peas in a storage peace and visualization piece came out of Airbnb for work flows okay let's just explain from a top-level point of view what is a type of query or type of tasks that a random company like an insurance company for example
 doesn't have the infrastructure of an Airbnb the the engineering team of an Airbnb what kinds of queries are they unable to do or what kind of data infrastructure the unable to build because this type of stuff is not been commercialized it's really about asking the next question right right now they probably don't have access to all of your data and if they did the cruise would take a long time to run and so with the platform like Periscope we want to give him access to all the data in a very rapidly iterative framework this way when they do ask that first question they're not done right they can go ask 10 more questions as part of drilling into that an LS okay so if I'm insurance company what are the what would those question be so I'm guessing he probably has really good data on like their core business but they might not have the ability to connect that data like national insurance plans to the campaign's they're running to the direct mail campaign still running and so letting them connect all those data sources together to realize that
 Direct Mail campaign influence the city which influence this marking I'm playing wish that influence these insurance plans would help them further optimize other operation so it's like you know if you've got to totally disparate databases you've got a mongo database over here and you've got a sequel database over here and Cassandra database over here it should not be hard to do cross database joins rights and that that's part of what are ingestion layer helps customers do is get all those disparate data sources into one place so that when they build the models the most to be fully informed by their whole business not just the part of the business that has easy access to data is it hard to do this cross did bass joins it's hard to do it repeatedly right you can you can dump a CSV from four different data sources Alto that somewhere and then do the analysis but that's almost never what you want the outcome to be right because you're building something repeat
 all your building I answer to a question is going to be asked every day every week every month and so what you really want is a reliable system that can continually consume all the data from various places make it available for not just this question but every future question on a repeatable basis
 do you have to be picky about how much you cash because you know if you were to go hog-wild if somebody plugged in all of their data connectors into Periscope data you could get really greedy at you could like build all kinds of indexes and you know do you have columnar data at than is necessary what's the right amount of data to cash I think it's usually more than you expect because if you if you try to restrict it you only have the day that you need to answer today's questions and tomorrow when the CEO asked you hey this other metric over here what happened if you have to go to the process of establishing a whole new pipe on to get all nude it into the system that are really delay your ability to answer those questions so I think you want as much data as possible as accessible as possible ever have issues where people are saying hey my dad
 words are not loading fast enough and those are button they can click to just like buy more speed out of that conversation go yes there is absolutely that button you can always buy more speed okay give me a little more color on that so if I want to or if I'm going to dashboard is going slow and I click the buy more speed button what happens
 yeah we will increase the amount of Hardware supporting your cruise right we we have these thousands of notes we can just add more to the customer on and within usually about an hour to scallop a custard how you're curious run to Matthew faster okay and what is that is that typically like replication or what exactly is going on under the covers to speed things up so you can scale both vertically and horizontally being that we can scout out to multiple copies of your data on multiple different clusters that usually helps with Skellington currency but if you have a quarry that's taking 90 seconds and you want to take 9 seconds that's killing vertically and that's that's growing the size of the cluster adding more compute adding more RAM so that it can process that much data just so much faster
 and you ever have people who say well I can't afford that so I need to adjust my queries I need to adjust my dashboards to to make them I don't know I guess less demanding is that an alternative for people as well absolutely in fact sometimes there's just not enough Hardware in the pot on the planet to put behind a certain Corey because of the way it's written and so are our support team is frequently helping customers rewrite their court is to be much more efficient and is this the same is the same as what ideas you have in standard software engineering where the the way to make something go faster as to do less work and so that means you to scanning less data or scanning data in the in end time is it of N squared time kind of thing now machine learning is RCU buzzword well depending on who you ask are there any interesting things going on around machine learning your company either ways that you're allowing people to
 learn from their data within Periscope data or are you using machine learning internally to to build I don't know better scalability how do you think about that term machine learning yeah we use machine learning internally to do a lot of scoring on the marketing side like the value Valley the value of the prediction of a contract eviction insurance we don't currently unable machine learning in the products but that is absolutely something will be doing long-term
 okay why no recruiting it has interesting process within Periscope data tell me how you do hiring hiring we try to take a different approach than most companies are pistachios engineering side we never asked folks to invert a tree on a whiteboard this is not a normal thing to do it doesn't teach us anything is not fun for the candidate I don't know why some some companies do that instead we we invite the cat it in and they sit down with our team they sit down in the engineering area and they pair with a few folks during the day to actually build like code together and and the product is is quite fun I think and candidates really good experience for what's it like to be a part of the team what's it like to work on a good base for a Sumo to periscopes what's it like to just feel that the culture in atmosphere at the company I guess you have seen the results of that versus the results of the recruiting process in Microsoft Microsoft
 much more of a invert the tree type of process how does that contrast and outcomes yes so we we have a really kind really positive culture are accept rate for internet offers is over 80% and it's every single time it's because of the people like when you folks doing they say they joined us because of the team because of the interactions they had all their mother on site and so I think that really lends itself to building a company that are you can be part of the culture to be proud of people that work here I versus one where folks are super good inverting trees on whiteboards
 okay fair enough well Tom I know we're running out of time but what are the biggest challenges for periscope data for the future I think that the challenges is really how much can we get done and how soon are we have we have what I think are the best people we have an incredible product in the right space and it's really it's really are still lose his like how how well can execute is the challenge you have any internal I guess personal like personally how do you how do you Seize the Day how do you seize the opportunity cuz that's that's more of a philosophical question you know how did how do you make sure that the company doesn't stumble and fall what are your internal checks against that
 yeah so we're well past the scale where I individually can serve run that process and so it's really been about just hiring really smart really experienced people who have seen some of the challenges we've seen before who know how to grow teams who know how to build the right process for our scale and letting them operate what has become like a very large very powerful machine
 any hard lessons that you've had to learn around the hiring and management side of things human process in the in that sense it is incredibly messy at times and so I think there was any mistakes we've made have had to do with not thinking about the individual human characteristics of a people and how they interact i instead focusing on something else book recommendations for getting better at that psyche psychological understanding or is it just first-hand falling on your face my co-founder learn through books a lot of tree have great recommendations I tend to learn by Falling on My Face all right. I've heard such amazing things about Periscope data I haven't used it first hand but I will be following a company closely and maybe we can do some more shows with different members of your team in the future that sounds great and thank you again for having me on
 thanks to symphono for sponsoring software engineering daily symphono is a custom engineering shop where senior Engineers tackle big Tech challenges while learning from each other check it out at symphono. Com SE daily that's s y m p h o n o. Com SE daily phono for being a sponsor of software engineering daily for almost a year now your continued support allows us to deliver content to the listeners on a regular basis
