Transcription: Google's infrastructure has been the source of inspiration for research papers software projects and entire company's Google Pioneer the idea that we care less about the individual machines that we are running our applications on and more about the applications themselves containers are the abstraction that we used to separate the concerns of the application from those of the underlined Hardware core OS is an operating system built with this paradigm shift in mind in a data center the main job of the operating system is to be a platform for containers to run smoothly on Brandon Phillips is the CTO of coroas and he joins the show to explain what core os does differently to power the application to get deployed on top of it before we get to this episode a few quick announcements if you're interested in advertising on software engineering daily send me an email Jeff at software engineering daily. Com there are more than 14,000 Engineers that listen to software engineering daily on a regular basis
such a great place to get your product out into the ears of developers or to advertise available jobs that you might have your company also if you're an engineer that's looking for an open source project to work on checkout software daily at software daily.com this is an open-source news and information site about software it's being led by Jeff AAA member of the software engineer daily Community you can also check out software engineering daily.com which is the website for this podcast you can find links to the slack Channel my Twitter account my email you can find a link to sign up for our newsletter software weekly and with that let's get to today's episode
getting started with kubernetes can be a challenge if you don't know where to start go to aprender.com se daily there's an upcoming webinar about installing kubernetes as well as an introduction to mini Q which allows you to run kubernetes locally at a printer. Com SE daylight you can also find past webinars about kubernetes covering microservices persistence and multiple clusters as well as the history and origins of kubernetes and if you're looking to move to a cloud made of infrastructure platform that works with your existing applications you should also check out of printer. Com C daily aprender provides Cloud platform software that works with your existing applications and infrastructure so whether you're looking for a guide to installing kubernetes or you're looking for a platform that will give you access to Cloud native infrastructure that works with your existing applications check out aprender.com SE daily that
a p p r e n d a. Com SE daily thanks a friend up for being a sponsor of software engineering daily let's get back to this episode
 Brandon Phillips is the CTO at core OS Brandon welcome to software engineering daily there is a paper that I have heard you reference that was published out of Google called the data center as a computer the idea of this paper is that we have moved to a world where we no longer treat infrastructure with a concern for the individual machines we are deploying to we want to take more in terms of our applications when did you read this paper and how did it change your thinking around Building Systems
 serve read this paper as I was winding on my experience at Rackspace and it was really coming through pretty loud and clear that in the day and age that we started for OS and that we were thinking about these problems that I'm data centers were becoming more about supporting applications and less about Ashley Fielding out the instruction themselves and so yeah it was around the time that we were for me to manage will work on Chrome OS and experience having ran and rim applications and help to run run data centers at Rackspace
 so how did those problems manifest in your work at Rackspace product we had like a globally distributed application and a lot of our time was spent on that but more time was spent getting out which servers to buy out what Hardware configurations were necessary from software that we were using and then inevitably building out that full pipeline of saw where I can speak Russian Management systems that would actually end up Landing copies of the code off of the hardware and so
 miss this is like okay calling pattern for a lot of people say they have a Xbox they want to go but they spend a lot of time thinking through these things that really are pretty common to every products are any soccer product that you build it has a supposed to component of this life cycle management of the application setting up monitoring Ashley planning at the hardware thinking about which pieces of Hardware going to run white white application in the circle thing and two large degree those things and in this day in age or so over the last 2 years I think I can be automated computers are pretty good at Balancing Act calculator
 as we move towards this world where were thinking about the Datacenter as a computer how do containers fit into his way of thinking about Building Systems
 for sure so
 military with the status as a computer idea is that we
 we're thinking about not the individual fight card for not thinking about the individual blade so then because she needs me about taking some capacity some some memory and CPU into network capacity and running the thing that we want across a fleet of machines that we really don't care about the individual server an individual server something that's easily replaceable awesome times will fail without human intervention and that sort of thing so the reason containers are important in this world is because you really similar seamlessly need to be able to move an application between one machine to the other going back to this idea that it in a data center has a computer and a machine May Fail at any time you need to really make sure that it's this application is this Atomic consistent little thing that be moved from Jose and hose B and ran in the exact same way as it was on the
 no machine that's really where the container starts to play its role as it's this consistent Atomic piece of packaging around a piece of software applications in the Datacenter
 what did Docker do that was new about containers how did the docker engine get so popular
 you're so when we starting our last about 3 years ago doctor hasn't been started yet and large containers continue to be isolating and packaging up software you saw a lot of successful companies building on top of my form for building and hosting web applications but the user of us are they aware that we wanted to go to work we need a containers be brought to the Forefront as something that developers and operations people thought about as a way packaging Distributing applications and that Distributing applications part is really the part that
 soccer
 correct and brought to the market was this idea that containers aren't this complex Machinery that you need to kind of custom do on every single it's actually something that you can pack a job push up to the Internet similar to how you do you like they get pushed for your code and pull down on other host let's play trivia Elliott distribute software is really white a successful I'm up to the Forefront of the discussion that we have today around all systems in kubernetes a variety of other container runtimes Google l m c t f y and Cloud Foundry Garden in Mesa had a container specification why were there so many container specifications at what is there something that Docker did that was dramatically
 different than these container specifications or was it more this like virality component the usability component
 I think it's still like things like a garden and let me see in that for you and lxc and a lot of these projects that came with Tanner's none of them really tackle the problem how do I get the container that I want to run onto my host so I can run it and that was fundamental a the difference that the doctor
 a friend of Che at Taco Bell problem and how do I get to soften out and run back and so
 the primary thing is they didn't punch on that problem and they figure out a way packaging of that natural large degree all the other container run times have started to that idea so if it's idea of having a container image The Toasted on the internet that you can download you're starting to see things like Cloud Foundry adopt those clearly things like giving a tease are built on this concept of rely on that sort of concept Mesa started to implement container run times and severe their platform and that's why things we can talk about it later but things like making sure that we standardize how these containers work and so that we can interoperate and that the developer can move their container important
 great and show the degree of standardization the ability to move our container around to different platforms what is the the state of that today how portable are the the containers that we are building
 yeah so it's improving the deportability all just comes down to having some agreed-upon semantics like anything I like they should have key in HTML5 I've been on the web and all of this comes down to it it's actually having some agreed-upon Symantec Symantec start pretty solid at this point everyone has a pretty good idea of how this stuff works out and the Technologies involve to create a container easy to implement that are making progress and that work is ongoing and involves you know a lot of the really important folks in the industry
 eye doctor in Cuero s in Microsoft and Google and redhat etcetera Etc are all kind of involved in that and not effort
 yes I didn't prepare much around the open container Initiative for this interview but I am very curious about the end of their some listeners who are curious could you just talk about what are the main point of debate or discussion or technical development that the open container initiative is intent on solving thing is making sure that somebody who wishes to run a container on a machine understands how that that process will actually be staying up what survey Environment Canada expect. And also a software engineer who is just wanting to package up there software inside of the container how does a package that up into it's not a day that goes along with that image like you know a license in the EuroRail for the homepage and how is that artifact actually symbol using like our format
 pressed out of a sign it how does a cryptographic hash like a unique identity of that thing gets generated that's what if they ensuring that there's well form standards around
 okay so you're talking about the space or a little more broadly just so we can move it towards a conversation recorder iOS the workflow for a developer who is writing code that's Adventure going to wind up on a container is the developer is working on code the developer pushes a commit to a repository and that code whines up in a container deployed on infrastructure somewhere and ideally the developer doesn't have to think much more beyond that but there are obviously a bunch of steps that happen in between those two states of pushing a commit to a repository in that code going to deployment in a container on infrastructure somewhere what are the important steps that are going on in between those two things
 yeah so today there's a lot of different ways that we found customers are actually building that final artifact building that final image
 we have people who are doing a lot of ad hoc stuff so they'll just have the developers built the image using a laptop and push to a central registry like our product queda Ohio we have folks that will like you said
 essentially abstract completely away from the developers the concept of a container being built so they develop
 I get repository will then get triggered by something like Jenkins or some other cicd system that CD ivcc system eventually into a registry like clay and so there's there's various Shades of Grey in that Spectrum but it's actually anywhere from people in Canada adhoc it works on my laptop I built the container it should work in prod to we have a very rigorous existing CID CD environment that our relation is used to using and we adopted that existing testing and build environment to Output a container image a little bit more stations where they had some other deployment style in the past and they're moving to containers for the application moving forward and then obviously people are getting started are much more apps
 to use the it works on my laptop I'll talk to you about the push it to the repository and see what the rest of my Engineers are the rest of my team want to do with that
 how did the responsibilities of the operating system that are containers are running on how does it change when the only thing we're doing with that operating system is simply running containers on top of it
 yeah this is a great question and this gets to why we built things like chorus Linux which was our first product the operating system starts to shrink dramatically before containers we relied on our operating systems to do a really complex set of things for us we were relying on them to keep our databases and the Linux kernel in our bootloader at all these extra pieces software that have each their own life cycle and end we ask them to keep those pieces software update and to never break the api's between the pieces software I'm so all those pieces should work together
 the same time we be making demands like well I want also the latest version of software I want to be running the latest python early its job are the latest postgres database since it's a really hard set of Demands and so with chorus Lennox we ship just a handful of pieces of software on the operating system essentially things are necessary to bring up networking and storage and then run up Tanner and so on chorus you're not going to see things like python a ruby or these things that an application might use to give their unnecessary because those things run inside of the container so we can be a little confusing for people using acronis Linux machine for the first time seeing that you know they're expected tools
 special large degree cordless lennox's is not designed for human beings interact with its design for running containers and that's about it so we have had some shows recently about unikernels and a theme that both of the universe unikernel guests that we had on shed is it the Linux kernel in its current state and most distributions it's been trading off efficiency in order to be compatible with all of the different types of applications that could potentially be deployed on top of it so what is an example of that excess of compatibility that the lens Colonel accumulates rather than writing proficiency I continue to be extremely confused
 fundamentally relies on Hardware abstractions and the hardware abstractions are coming from the Linux kernel and so I really like the in efficiencies the living don't come from don't come from needing compatibility with applications it comes from having to deal with an abstract away Hardware to a large degree and that's what causes likes and that attraction is extremely useful right like we can log onto a Linux machine whether it's manufactured by HP or even if it's running separate architecture it's like I said our machine or an Intel machine or whatever it is and they behave roughly the same in this ati's are roughly the same and so the extraction is still useful now that the Unicorn most rely on that Linux kernel abstraction to the den Implement applications that are utilizing really really efficient
 guys but the attraction that Lennox is providing efficient and extremely useful even if you're you're going a unikernel doing a better job and we are relying on the HPI layer that is so you know now at this point 40 50 years old deposits at interfaces are showing their age of times but it's a pretty effective Hardware Hardware abstraction and it continues to be
 the crew ask is this minimalist operating system and I feel like we should Define what core OS is I'd love to hear your take on how that contrast with what the goals of unikernels are but we should probably start at the basics what is core OS
 for sure so chorus is T-Pain's first is it is a three-year-old company that's building a bunch of Technology surround containers and distributed systems Linux is a
 product that we've built that is a Linux operating system as super minimal so we'll talk about the story of how we got started with us when we started to go down this path of wanting to what we end up calling Google's at the structure for everybody else went down this path of wanting to build products to help people run their infrastructure and related to ship ways we wanted to actually from the very bottom of the stack cray
 setup software open source projects that we would like to consider through correct way and so with coral Iceland x what we wanted to do is we wanted that strip out everything that wasn't necessary in order to run a container reduce sort of the surface area for vulnerabilities the surface area for introduce introduction of complexity and backwards compatibility issues to the very minimum inside of a inside of an operating system
 and then that would freeze up to do some interesting things one of which is that we knew that
 generator system of the moving really really fast and we wanted to have an operating system that could keep up with that case and then also we wanted to solve this problem of security update all software security vulnerabilities it's just a matter of time before they discovered even the most secure thing in the world lockdown burnt into Hardware inevitably get some sort of issues in it and so and that way for athletics differs from a lot of other operating systems bill for servers in that we have a Atomic Auto by default upgrade system and so the recorder iOS machine running cross Linux by default will just automatically upgraded to the latest version of operating system over time
 talking more detail about how there's updates work because chorus has no package manager so just the way that it does updates is quite different than typical Linux system yeah so the way for I was letting says its updates
 is very similar to our phones or updates or how sophisticated networking equipment do their updates so far OS has a copy and a Beat copy of the software on installed and then it will download a new copy of the software and place it on one of the AARP partitions of the disk and then when it's not much signal one way or the other that it's okay to reboot and apply the update it will
 atomically move from one version of software to the other and this has a number of nice properties one is that it's really trivial to cryptographically verify that you were running a cracked version of operating system to is that you know for certain that you're running either A or B version of the OS it's really common problem inside of traditional Linux package manager is to be partially through an upgrade or partially reconfigured on the latest system or partially running the latest version of the colonel because some of the module you're actually using from the new version of the colonel Etc. So there is a number of different Corner cases that we avoid by having this very, very consistent thing and it's really important once you think about managing more than a handful of machines one of my favorite stories about outages when we are maintaining the call monitoring product is that we have this nice
 distributed system 5 data centers
 and we had a Cron job that would do automatic apps update and upgrades when somebody was on duty and so to a large degree with automated the updates we hadn't been paying attention to whether we actually rebooted the machine on the latest Colonel and so there's is terrible linux-kernel bug where
 the machines which is blocked out when the leap second happens and so we lost two of our eye Datacenter simply because not even that we were applying the updates that we hadn't fully up by the updates so I look consistent because the new Colonel packages were there is actually a suspected the kernel is installed was different than the colonel that is currently doing it hopefully automate or at least help pave the way for administrators automate their way out of a lot of that this was accidental situations hear about this Atomic ability to update the software and switch it over is this only applicable in a situation where there's like a an update to something within Linux or is this also how we want to do
 continuous deployment on our infrastructure that is running core OS
 yeah so when we were thinking about fixing problems all the way down to the bottom of the stack and why we started by building Chrome OS is it absolutely this is how application to play should be done to you want to bring up the new version of the application right next to the old version of the application which is something is really difficult to do today say that your new version the application requires Java server currently has Java 6 packages installed it can be a huge rigmarole for organizations to be able to do these sorts of updates always in a safe spot you don't want to incur down time you don't want to give your users a bad experience and this is where container start to solve some really fundamental problems with how we operate it and thought about operations on Linux servers
 and so I using things like your benetti's by using things like containers you starting to have the AP eyes and start have the technology and place that you can say right around version one of the application and then we're going to spin out some copies of Virgin to the application and then we'll slowly move load balancer capacity over new version 2 and you know what if we find out there's a bug in there we can immediately roll back the Persian one because we have hot running copies of version one that aren't taking any traffic but they're available on our host and so
 it is very much a pattern that gets repeated up the stack and fixing it at the bottom of the stack was something that motivated us create or us Lennox to being what
 continuous integration is so useful that I've started using it on my own personal projects snap CI from thoughtworks has the fastest setup I've ever seen I registered for SNAP CI with my GitHub account and after a few clicks I had continuous integration pipelines setup for a no JS application and a rails application that were just sitting on my GitHub account and I don't want my users to experience breaking changes so I want to run a large sweet of test for my application every time I push a change and snap see I will run those tests quickly in parallel on a worker that snap CI spins up and takes care of for me what do you have personal projects like me or you work at a company that is looking for the perfect tool to improve the deployment process go to snap. Co / software engineering daily snap CI and bodies the lessons that thought Works has learned from 20 years of software deployment the same lessons that have been written about Bi-Mart
 Fowler and just humble check it out it's snap. Co / software engineering daily it would support software engineering daily and you would get to check out continuous-deployment snap. Co / software engineering daily thanks for listening
 magic jump ahead but the typical model for when your deployment container on kubernetes as you've got these pods and you allocate a typical amount of space to the Pod and then you spend of a container that takes up some amount of space within that pod So within the core OS model if you have to deploy if you're going to have two simultaneous versions of the application running at a given time and then you're going to switch the traffic between the two versions does this mean that the the out the allocation strategy for the pods in kubernetes if you're using core OS it is does that differ significantly than that then you know if you're not using core OS you say
 JJ application at gig of RAM and 20% of the CPU and then when you stay up the next copy of your application the new version you give it similarly Giga I really it doesn't matter much to kubernetes
 how the operating system is is doing that for like she thinks it's just kind of a boring any other compute node to kubernetes sure before we get into that scheduling stuff let's talk some about that CD Coral West is designed for the host to be able to fail so you're running on this commodity Hardware that happens all the time you have failures what are the different steps that core OS takes to equip an infrastructure that is running core OS for that cut type of failure
 for sure so for some contacts chorus bill it's like a number of different open source projects and the first two that we built our car less than x an STD projects grass and they both get a ton of adoption in various products and projects throughout the Webb Building at CD which is our consensus datastore inspired if you're familiar with some of the Google literature inspired by a Google service called chubby what CD is trying to do is hold on no matter through any of the number of machine failures I hold onto really really important Oster configuration data and data sets are things like water service Discovery information where can I find other services in the cluster secret storage what a username and password to the database that this
 patient supposed to be connecting to
 network configuration where are the routes who are the Gateway Center out to get access to the rest of the cluster you can imagine starting to build other sorts of systems on top of that CD storage I need to know where where the disc is where the log volume is for this particular application Etc
 so what STD does is it it exposes a key value store so you can set some keys used to it and it replicates around multiple machines so if you have in general by default we recommended by Machine cluster it when you right into at CD it replicates the date at between the 5 machines in the XD Foster and does a lot of different things to ensure that that data stage resilient and steaks over time so I have 5 machines that are storing the configuration data for SCD how big of a cluster can that level of replication support
 yeah so a five-member at City cluster is something I can quite easily let's say that the so I had to use a database and patience that uses at CVS in stata base is kubernetes and you can scale up to it a thousand to 3000 Post today using at TD on a five-member cluster and so easily in the near future here as we continue our stealing work inside kubernetes you'll be able to see at t d scaling up and support
 Sammy I've been supporting 10,000 machines inside of a kubernetes cluster this report data even if one of the managers happens to fail have a hardware failure reboot or whatever and that's that's the job of that CD is to ensure that's really important data that's required for the thousand ten thousand machines to continue doing use for work remains resilient and doesn't get lost
 and how should I guess what kind of information am I storing an STD and how does that information get propagated to the whole 1000 mm 10000 node cluster from these five notes that are storing it in a replicated fashion put in by The Operators of a cluster it's like I said it's it's everything that you would imagine is necessary for your application to run so it's network configuration data service Discovery information storage information
 secret stores excetera anything that a human being has to put in the machines know what to work on that that's what you saw renetzky I guess the other pieces like scheduling information in the case of kubernetes as well as being stored inside of that CD and then at city of the number of primitive set make it easy for system administrators to audit what's going in there and back up what's going going or what's being stored in the data store in that sort of stuff to the rest of the cluster 380 eyes at CD is like any other database so
 the other members of the cluster connector at CD through through a network socket Taco race dpuh to be too and all whatever data is relevant for those other machines out of the cluster
 so are the other machines in the cluster periodically paying at CD for changes every single node in the cluster has a particular endpoint that has all the information that node needs to operate and so does each Noble talk to the companion API server which is been back by a CD and on a regular in a bowl say do you have any work for me and if there's any new work but if you guys are bringing back based on what's being stored in at TD and so it's it's pretty Classic Bike leader-follower sort of centralized architecture similar systems Tech CD like zookeeper console what are the design differences between those types of systems in xcd
 yeah so when we started building at CD 3 years ago zookeeper was the really the only incumbent system and the main things that we wanted to do differently with STDs in Zookeeper bunch of operational experience where the Zookeeper previously time does cognitive but that was appropriate for the cloud and one of the things that that meant was that you could dynamically reconfigure the cluster so this means like through the command line while the cluster is still alive I can add her move machines from the XD cluster this is something that until very recently I don't even know if it's made into a release zookeeper was unable to do and it's really important if you want to run these sorts of systems on Amazon or Google compute or somewhere else where machines come and go at least on a weekly basis if not more rapidly
 that's the purses I think it's just just making sure that this was a piece of software that was sort of cloud enabled the next thing is we wanted to lower the barrier to entry to distributed systems zookeeper was always perceived as this really complex hard to use beast and really the concepts that at CD exposes are very straightforward fairly easy to understand I'm confused SVN or CVS or something that has this linear revision history you have a pretty good idea of what a TD is doing and so exposed instead of a proprietary binary API it exposed bhtt where you can curl and play around with the API just using any hdb client off the shelf that your language to your command my gave you and so we
 greatly reduce the buried entry to using these systems and then we saw a huge proliferation of projects starting to
 build on that CD which really any distributed system needs this consensus piece on this this core piece that activity provides and so about Lori Mac buried in tree we found a huge number of new applications for built as pretty pretty impressive and it's been shown to be a pretty good building blocks and that's how things like you're not eating ended up getting built on top of that CD right so it sounds like it's not like you were you did anything revolutionary in terms of the consensus protocol it's RAF based whereas zookeeper access based But ultimately the Breakthrough you made was more in the usability of the API
 yeah there's actually rap was pretty important as well by choosing Raff we got that Dynamic reconfiguration ability I'm one of the things that the Zookeeper zookeeper actually pay some taxes is based on this issue protocol called zap and sad until very recently didn't have the ability to do Dynamic decoration which is a prerequisite for this sort of cloud enable meant that we were going after so choosing rap and the design of rock was actually fundamental as well okay let's talk about scheduling we had a show recently with Adrian cockcroft we talked about scheduling and that word can obviously mean different things in different context in the context of core OS what is being scheduled and how does that scheduling take place
 yeah soap for coral a sweet we have a couple of different scheduler projects that are kind of associated with us we built a container scheduling system called Fleet really early on and fleet was a very very simple scheduler that expose something that looked like a regular in its system like systemd to the user and its system was spread over a lot of posts
 the other scheduling system which we are really deeply involved with is kubernetes and is probably the more relevant one as kubernetes he's got a lot more noise and Traction in the overall ability for a system administrator or set of operations people to run let's say 20 machines and make those 20 machines available for anyone to run their application if they're containerize application on top of I want the scheduler is in charge of getting a pay schedule is in charge of identifying which machines have resources available to run jobs that are submitted by the human beings
 and finding what are the best outfits for those things so ensuring that varies constraints are maintained so constraints might be things like I want to ensure that task A and B you're never on the same physical host or I want to ensure that this task sealant on a house that has a GPU Etc and so that's really a 10 scheduling it's about ensuring that applications land on some sort of host and begin to run based on user input of constraints and resource requirements
 now an insurance system one thing that needs to be scheduled is the update and reboot process we talked about so little bit earlier Cora West provides a tool called a locksmith for this scheduling what is locksmith
 but Sam the operator IDs 20 machines and on those 20 machines are running very critical application that I want to ensure he stays up because I don't want to get paged but I've is competing thing that I also want to make sure that security updates for the underlying operating system continue to be delivered and so I have these two goals that are kind of at odds right and sure that as rapidly as possible I apply security updates but I also want to keep my application up so computers are really good at and locksmith is a tool for automating this process so a locksmith does is allows assistant administrator to set a threshold for how many machines can be rebooting to apply security updates at a time and ensures that before actually taking a reboot the machine toxic locksmith asks a can I take a lock to reboot
 if I lock it is available it takes a lock reboots and then gives the lock back to the service at the end of finding update as soon as this allows for is for system administrators to balance Beach I want to see my application up with the security concerns back to sleep lying updates and it's a system that we were continuing to integrate into Daddy's so people will have visibility into into through the Koran is a t i whatever she is in need of an update and update
 there are a variety of systems that are built on top of that CD or on top of the schedulers that use core OS systems that help with service Discovery what are some of the different approaches that are taken for service Discovery in systems that use core OS
 yeah so
 it's really a runs the gamut of every possible method that's been used in the past so on top of that CD we have a service discovery about like rewriting configuration files for you we have things like Sky D & S which is a DNS server built on that CD which will allow you to do wild card DNS Discovery have a hierarchy of DNS records of the sort of thing
 you have systems like Vulcan which are actually load balancers so um put into the vulkan API what what services exist and then full camel actually watch for the health of those systems and make decisions across the cluster of load balancers about what back ends are healthy and unhealthy in that sort of stuff and so there's a very nice ways that you can piece together service Discovery and then what kubernetes is essentially a meta API for service Discovery and it provides essentially all three of those options so using at CD is back in kubernetes provides services through the balancing 3 DNS or through configuration file rewrites they call secrets are conflict maps and so you kind of have this interesting
 there's sort of the build your own approach that is been taken with Sky D&S and comfy and Vulcan and then you have the sort of middleware platform approach which is why kubernetes is done saying we'll have a unifying extraction and then we'll provide an adapter to each of the layers as people find that their application works better with DNS forces load balancing versus something else and you stop seeing all of those options available inside the exody ecosystem drives people to make particular Selections in how they do service discovery
 yeah it almost all cases you'll see he pull go grab a tool light comes to your siding that's because they already have an application
 it's running fairly well I have an appointment I find that they may be don't want to containerize yet etcetera and so they'll pull something off the shelf that is just the one piece meal component that fixes their problems and put that in today at the structure and so you'll see those sorts of patterns of adoption when people are out of an existing Brownfield application architectural changes adopted when people either can take a little bit more time to think through the architecture of the application
 are building a brand new Greenfield application and want to start from the beginning with the cleaners and and service Discovery as kind of for first-class consideration other operational mode so that for the app
 Chicora West is not just an open source project or series open source projects it's obviously a company would you wear the CTO of and your main product is tectonic what is tectonic
 sure so what are the things that we saw with kubernetes is that it's really excellent open source project it's getting a ton of adoption from users and we wanted to provide have a full solution for Enterprises wanting to adopt this out of way of running a structure so we do with the iconic as we provide kubernetes and I stream of updates for the communities cluster and a whole stack of software then we add in a number of things that you know Enterprises expect from their operational software so we add things like monitoring we had things like identity like hell. Backed identity
 we have a dashboard so that people were less familiar comfortable on the command line or using rest API eyes are able to visually inspected Orient themselves say they're getting paid something's going wrong they can start to drill down there and then we provide Quay which is another one of our products of quay Enterprise so that enterprises have the ability to inspect
 container images build container images of audit and logging and policies around us the painter images scan them for security vulnerabilities in this sort of thing so we take in the eighties pop form that we spend a lot of time working on the open-source side and then added a number of really necessary and critical enabling features that companies who want to adopt this type of its structure require and saw that as a product that we end up calling tips on it so nobody's variety of these commercial kubernetes deployments and services that come with it why do customers or how do customers choose between tectonic and other kubernetes deployments with services
 sure so right now I Connick is the the wine Cuban Eddie's product that is focused on providing kubernetes so we've had a lot of products focus on providing a service Etc. But he's focuses on Middle Ground between infrastructure-as-a-service that you make it from like 8 of us are openstack and platform-as-a-service which you could get out of that Heroku style experience really focusing on providing a really good experience for a kubernetes note Google like infrastructure control plane and so
 there aren't you're right there are products like Google container engine which is a hosted product and really the difference with iconic is that we let you run it wherever you feel like you want to run at so it's not a host a product that I can for us manages it's something that your operations team run behind a firewall or you run on 8 of us are you run on Google cloud and so it's a little bit different sort of delivery model than some of the other kubernetes products understood so we touched a little bit on the unikernel stuff earlier people who work on familiar with his face how would the usage of core OS compared to other girls are in chorus is the strip down operating system that's great for running containers unikernels are these highly specialized at Library operating systems that we we take away all the excess
 and it will use them to just basically specify operating systems that are specific to our applications how do these two bottles compare and why you know you mentioned you were confused by unikernels what is confusing about them to you actually a way of delivering software for too cute to killer language platforms like they Java language platform or something like that or language platform Etc but these is unikernel still fundamentally rely on a real operating system to buy the hardware extraction layer and so
 yes there are efficiencies that can be gained through using a unikernel a day introduce a lot of operational complexity through not being able to leverage existing debugging API the Linux kernel provides of not being able to Leverage The introspection that is provided through say the process tables that game pieces during estrace I'm a lot of the basic fundamentals that the extraction of a process on Linux provide to you and so
 I think the reason that I'm confused I think that unicornos have some time and I need to prove Outback operational model is that
 the deficiencies gain by unikernel are are oftentimes not like
 large changing efficiencies for engineering team so generally changing efficiencies for engineering team are going to be on like orders of magnitude like a 10 x improvement improvement and they're generally not that level then prove it and then you can throw out the operational experiences that your team have with a with a classic deployment weather like in containers for application packaging through the cost for and so did the prusa needs to happen is well if I'm if I'm losing my operational experience because of majority of the cost of running my application and I'm gaining less than an order of magnitude efficiency then why am I actually the plane this technology and I think that's where unikernels going to have to prove themselves as is the cost really worth what you're losing
 okay well that's a great place close off Brandon I want to thank you for your time it's been a great conversation and I appreciate you coming out software engineering daily
 yeah for sure thanks for having me and I just want to encourage anybody who is interested in checking out any of these Technologies we have a bunch of really open great open source guides.com how to get started with containers to help you get started with kubernetes and a really nice Community if you want to check out car us.com community and get involved with any of our open source projects and try it out or if they're looking for a job thanks all things are going to show Brandon
 thanks to sinfonico for sponsoring software engineering daily symphonica is a custom engineering shop where senior Engineers tackled big Tech challenges while learning from each other check it out its symphony.com SE daily that's s y m p h o n o. Com SE daily thanks again siphano
