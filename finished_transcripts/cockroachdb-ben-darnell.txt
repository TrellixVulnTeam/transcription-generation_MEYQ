Transcription: Google has published papers on distributed systems such as big table chubby and the Google file system during this episode we focus on a product that takes inspiration from Google's spanner project a database that is built on a distributed monolithic sortedmap cockroachdb is a scalable survivable consistent sequel database today's guest Ben Darnell is the CTO of cockroach labs and he joins us to discuss Sequel and no Sequel and the trade-offs the different types of distributed databases can make Ben explores these trade-offs between database types and explains how cockroachdb provides many of the best features of both a sequel and and no sequel distributed database
well also providing the survivability of a cockroach after a word from our sponsor will get to this episode of software engineering daily
 as a software engineer you have tremendous leverage in the employment Market every company need software engineers and these companies are all competing for you the engineer a scarce resource but as an individual engineer you don't have much insight into this incredibly opaque Market that's where hired.com comes in her.com assigns you a talent Advocate who works to understand your needs and works to find you the perfect job go to hired.com se daily to try out the platform today and get a $2,000 bonus if you find a job the companies on hired compete for you the engineer and then make bids to win your talent these are companies like Facebook and Uber companies that we know are on The Cutting Edge of engineering because they've come on software engineering daily to talk about their Tech stack
 I have used hired personally and I've experienced firsthand the ease of finding a well-paid enjoyable job this was way better than replying to LinkedIn messages from uninformed recruiters and is also certainly better than using a job search engine to find a great job and support software engineering daily go to hired.com se daily and get a $2,000 bonus upon finding a job check it out it would support the show now let's get on with the show
 cockroachdb is a scalable survivable strongly consistent sequel Database Ben Darnell is the CTO of cockroach Labs which makes cockroachdb been welcome to software engineering daily thanks Jeff for having me today's database systems force you to choose between consistency and scalability explain why this has historically been a trade-off
 so in it in a in a crucial relational database you have a you have a single database server which can serve strongly consist of transactions because everything is everything is stored in one place with the starting in the in the mid 2000 was that people would move to a database system like big table or it said descendants hbase and Cassandra or a document oriented database like like mongodb and this will give you the across a lot of different machines and lots of Cassidy cheaply but it had the downside that these notes weren't that were tightly coordinated with each other and so you would get you would not have strong consistency across the across the different nodes and so for example if possible to for a record to be updated on OneNote without a corresponding record on another note to be to be kept up-to-date and so in cockroachdb
 we are building a system videos strongly consistent and transactional at every level and so that so that the the changes that you make across all the nodes no matter how I have the data is laid out across the different physical machines everything is transactional of anatomic and so did the all the records either update or or don't depending on the state of the transaction
 I'd love to dive a little bit more deeply into this sequel versus no sequel set of trade off before we talk more about cockroachdb so relational sequel databases have some great features like declarative sequel that could support for indexing and transactions but they have poor horizontal scalability why is it difficult to scale a relational database
 so it's
 it's difficult to scale because the relational model is so flexible you can say in a in a sequel statement Century arbitrary Transformations on arbitrary amounts of data and you can see this even in a in a single node there is a huge difference in performance from one sequel query to another depending on exactly how the query is written and how the hell the data is laid out on disc and how it's how it's indexed and so Distributing a relational database is it is essentially just ate a much harder version of this is already hard problem and so that's where that's where no sequel comes in no sequel databases specifically give up a lot of that flexibility and power and then that they reduce the problem to a simpler state that has been easier to solve in a distributed fashion
 so okay no sequel databases in contrast to sequel are scalable to large clusters but there are limitations you don't have strong transactional semantics so it's easy for your data to become inconsistent among shards and replicas explain this in more detail why can no sequel databases lead to inconsistency
 so
 first of all no sequel databases are a broad category and so there there's not apply equally to all of them but in general what most no sequel databases do is they Place either don't support transactions at all or they always support transactions in in a limited fashion for example they only show me support transactions within a single single Row in hbase or Cassandra or a single entity group in some systems that there's usually some some unit of distribution that is also the largest scope you can have for a transaction and so anything that you want to do across anything between to do a cross transaction across what whatever these units is across any groups or whatever they wherever they may be in any given system this has to be a transactional operation and so if the if the database or the client failed
 at an inopportune time then then the transaction may have heard that the operation may have been not completed on one on one note and not on another and so for example if you were if you were building a a bank on top of the database when you want to subtract subtract $50 from account a and add $50 to account B you have to make sure that both of those things happen or neither of them do you don't want to either destroy the money or created out of thin air and so in answer that's it that's a big problem for doing for doing certain kinds of things in a
 it in a distributed database because the date of that you want may not be arranged in such a way that the that the transactions you want to do are possible
 most no sequel systems can only claim eventual consistency what are the kind and for listeners who don't really understand the concept of eventual consistency we've done many shows about consistency in distributed systems in their play of episode you can go back to to learn about this such as a recent episode about react but assuming our listeners know what eventual consistency is what are the kinds of applications where eventual consistency is not good enough yet so first of all I wanted to clarify that separate from the question of sequel versus no sequel because because says sequel databases in in the typical kinds of replicated configurations that you see would you replicate a my sequel or postgres database systems are also eventually can system because the because the replication is done asynchronously and so did the update will be
 8 on the master before it is a synchronicity replicated to the to the secondary and so the eventual consistency is a
 it is a separate separate question from whether the the database itself is transactional in a in a single copy and so it took so back to the question of where when is eventual consistency good enough and when is it not well the thing is that the eventual consistency is it is really kind of a a marketing I'm working it's not it's not really consistent in any in any strong sense of the term and so you have the more accurate term is usually inconsistent or actually one way to look at it is that and eventually consistent system is is not necessarily durable so if you're familiar with the at the acronym acid describing the three properties that you want in a
 in a transactional database on that's atomicity consistency isolation and durability me to the transaction it doesn't really stays committed and that's actually the part that is that is lost within eventually consistent system because the the data could be could be lost before it is for is replicated okay well now that we've had a broad overview of Sequel and no Sequel and some consistency discussion it's going to be a cockroach DB what is cockroachdb at the start of the show it's just so we can get one of the time so it's distributed meaning it runs across across a large number of machines spanning Davis Center Santa and large areas
 it's a transactional meaning that every everything you do is is run as as part of a transaction and and is consistent and and atomic and the three-piece these transactions are also fully distributed so you can have arbitrary transactions over any any subsets of your of your data there's no no restrictions on what can be the heart of a transaction it's also a database or a relational database this is the interface that you use to interact with it this is a work in progress but we're aiming to be compatible enough with the with standard sequel to that you can use off the shelf or M4 and products like like Django were rails or hibernate or things like that and get bees to the point yet and have them work but we are
 what we're aiming for that as a as a compatibility goal so you can sit so you're not having to write write write a sequel queries are doing some work for cockroachdb it's something that that you can use I Off the Shelf at the same time as if you were using my stress or anything else before we get to the implementation details of the are there any other design goal some high-level goals at cockroachdb is trying to achieve one of the one of the other things that we are making a big a big effort in that. I haven't touched on yet is trying to be very easy to deploy and operate we took something that is kind of a necessity when you're running at the at very large scale on muscle machines and it's it's specifically is lacking in a lot of the a lot of the Prada
 and the space right now and so cockroach is it shipped as a single binary with no dependencies you run this binary on the on any number of machines addresses of at least one other note in the cluster or you point it at a load balancer that knows how to find them and then they they talk to each other and yourself organized from there there's no need to statically choose a primary and a secondary or you don't need a separate zookeeper cluster or anything like that everything is is self-contained and once you once you run this is binary on all of your on all of your servers the Cockroach system basically takes it that takes it from there and is responsible for migrating data between that everything is up to your retarded configurations and so on
 so now let's start talking about the architecture and implementation of cockroach DB cockroachdb is architected as a sorted monolithic map explain what this means appetite so that this is actually an architecture of it's not that not that unusual in in the database World in a lot of other databases use a a some kind of strategy in a map means is that all of your data in your table in your tables is ultimately encoded into an ordered key-value store and so for each shut for each record in in a sequel table we construct a key which is any representation of your primary key prefix with the table ID and and that sort of thing and so we have a what is conceptually 1 G
 key Value Store of all of the all of the data in in your neck and your Dame and then indexes are other rows in this key Value Store that are that are represented in a different way so does each each key corresponds to a to a row in a my sequel database sword of oratoria sequel database each each key in this key Value Store currently corresponds to a to a column value in a row at the end of that we may be able to group multiple columns into one key key value to make things that make them more efficient at the at the web when coding but currently currently it stored as a sparse sparse Matrix drawing inspiration from from big table and the end spanner in terms of this this design but so we have
 wake me up in one giant giant sorted map containing all the data in the database and then we take we take continuous sub sub-regions of that map and we we call those ranges and then those are the physical units of storage and distribution him the in the system and so let me know if you got it alphabetically arranged for all of the all of the record starting with a through C and then a second range for I all the records starting with the with d through F and so on and so we we split it up into into contiguous ranges and then those ranges are the
 are the United distribution and so each of those ranges is stored on at least three different notes in the system and the three replicas of arranged are the ones that coordinate to I2 to ensure that all of the changes to that ranch, CLE
 got it so the just to make sure I've got this correct so each the the the map that is distributed across across we are at least three and three replicas defines a key space and each key corresponds to a column value of a specific row in a what would at a higher level of abstraction is a sequel database and so so if you wanted to have a sequel representation of this you would have you know you would conceptually have you know a simple table and each each value within a given row has a mapping Associated to it and in order to find the value of that mapping you would go into the distributed map of cockroachdb
 you would look through this this distributed range distributor range of Chi-Chi spaces and that and the key that you're looking up would correspond to the the value that you want to load into your c equal representation is that correct yep that's right okay very interesting so the highest level of abstraction is a sequel cockroachdb uses a grammar that is similar to post grasp and the layer below that is distributed sorted key-value-store how exactly does the sequel layer interface with the key Value Store layer
 I'm so the the key Value Store exposes an RPC interface that gives you operations like like get put Anchorman delete a conditional put a handful of different a different Primitives and you submit these as as batches into the KV layer the KV layer can actually think I was to petition these at a train bound reason so it figures out which which operations can be sent together too because they fall on the same range and which need to be separated out and so we have a we have one of those kind of system which is something like a client library that understands the key structural and how to split up the requests and farm them out to the two different notes and then it seems The Simpsons the note out to the
 what will send send the request that down 3 raft to ultimately be applied via consensus across all the all the replicas of the range
 data is stored across distributed nodes in this distributed the system of ranges so explain explain what a node is and what a store is to disambiguate that's in describe how stores are nested within nodes right so nodes in stores anode is essentially a cockroach server process running on a machine so you will gym machines are generally synonymous you can run multiple nodes on one machine for testing purposes but it wouldn't make sense to do that for for production use so anode and a machine are more or less synonymous and then in practice you would have one store for each disc on the machine
 so so so in it in it yet simple case you have you have a bunch of notes each with one store if you have multiple multiple disks per machine that are not organized into a raid array or something like that then you would have multiple multiple stores on those notes
 as a cockroach DB deployment scales ranges get broken up across nodes explain how this range distribution works and how ranges get reconfigured as a deployment grows
 right soaz so so initially the cluster is first deployed it only contains one range and so as as data is written into the database and that range grows it eventually hits hits the maximum the maximum Target size for arranged which is currently 64 megabytes and then split operation I'm so in a split we we look at the look at the data in in the rain we pick up pick someplace near the midpoint of that range and then we and then we do a split and create a second range off of it be at the range itself the original range shrinks to cover its left half and then we create a new range to hold the data from the right half
 and so then after this split has occurred then we now have two ranges that can be moved around and rebalanced independently and so the the nodes all all talk to each other using a gossip protocol broadcast information about how how heavily loaded they are how much free space they have and so on and so when notices that I got my my disk usage is is 10% above the median then I'm going to try and shed some of this load I'm going to find a note this music is below the media and and ship them some of my data
 you mentioned the gossip protocol and you've also mentioned raft earlier in this conversation which is another distributed systems protocol and eat maybe I'm not great at understanding how these two types of protocols contrast but I think that something like raft or paxos is a little stronger in terms of how frequently a tries to to synchronize a system versus a gossip protocol which is a little more opportunistic is that accurate yeah that that's accurate so I have to wrap is a distributed consensus algorithm just like paxos and it is a it provides very strong consistency and ordering guarantees and so whenever you make it changed your data this is actually done through a raft and wrapped essentially runs what's like a mini election for each reach change that comes through
 and so so so as I said before each each range is replicated on three different nodes and one of those notes will be elected leader at any given time and so any rights to any rights to that range go to the leader and then the weeder is going to forward that that request to to each of the followers and it's not going to consider it's not going to consider the the the operation complete until it has heard back from a majority majority of followers for a majority of the up the Percocet & Towing in most cases that's two out of three or three out of five depending on how your how your reputation is configured
 yeah so these two different types of distribute systems protocols give some benefits for people who are really unfamiliar with security systems are trying to learn give an idea of like when you would use these two types of protocols like why would you use a rafter packsource protocol to synchronize stuff more aggressively versus an opportunistic gossip protocol right so gossip gossip it is very different from from raft gossip is a it is a very cheap broadcast protocol is centrally and so it's it's a good way to get to get data cheaply across across the entire cluster but it's a provides consistency guarantees at all
 and so in this case the this is why we use gossip for things like for things like the disk usage of a server because it doesn't really matter whether you're basing your rebalancing decisions on the up-to-the-minute disk usage of the server or whether that lags behind by but by some some. Of time and so that basically gossip is it. Just like gossip in the in the real world it's it's undirected and kind of
 not not not fully controlled and so that you know when you put something at the gossip that will eventually make its way across the entire cluster but there's no but there's no strong guarantees of how long that will take it's not. Directed to anywhere in particular brasscraft is very very tightly controlled it's it produces an ordered sequence of operations and guarantees that but that sequence is it is complete and and identical on every on every replica
 how similar is cockroachdb to the spanner database technology designed by Google spanner is is definitely our biggest biggest single inspiration it's kind of the existence proof that tells us that something like this can be rebuilt and and work well at at scale I think the Water Bar high level design points are are similar they're both based on monolithic sorted map for example and the that the also provides things into I will take off fans we call ranges and pretty and put these into independent consensus consensus groups the biggest difference at least in terms of what is widely known and understood about spanner is that Google actually went to Great Lengths to build a Tomic clocks for there for their data
 to provide very accurate time signals for there for their servers and that is something that that we we won't have because cockroach is is designed to run on commodity Hardware in and whatever Datacenter you may have and so we can't rely on exotic timekeeping hardware and so that that that place is certain that certain limitations on on the way the hour that that are transactions can't can work so our transaction model is slightly slightly different from spanners spanners transactions if you're familiar with the concept of isolation levels in Sequel in have transactions that run at different a different isolation levels like that recommitted for people read serializable but that that's just that's just the ones that are typically offered in Sequel databases but that's
 if you look at the literature in in distributed systems design directionally a bunch more different different isolation levels that are possible and so spanner actually provides a very strong I guarantee called linearizable which is faster and even stronger guarantee than than the serializable transactions that are the top level that most most relational databases offer and a lot of that comes from the fact that they have this this atomic clock synchronization for us we don't we don't have that that capability and so our transactions actually are serializable so so slightly slightly weaker isolation then you would get with that with spanner but still no matching the matching be topped here isolation of of other other relational databases
 got it so we will get into this and you know I guess we might as well a good place to start might be to touch more on this this idea of why Google needed to build atomic clocks atomic clock technology in order to have really accurate time stamping and that definitely relate to this linearizability feature of spanner and even though cockroachdb doesn't Implement linearizability I think it's an interesting topic of conversation is to like why you why cockroachdb because it's running on, because he wants to run a commodity Hardware Hardware literally cannot guarantee linearizability so yeah let's let's talk about that a bit you know I don't know where the best place to start is but maybe it's with this term external consistency or I mean what is this term external
 how does that relate to the term linearizability how does that relate to serializability
 I'm not I'm not sure of the exact definition of external consistency so I'm not sure that I can answer that precisely but the maybe I maybe I read too much into that the spanner paper vocabulary this is not my area of expertise in the system so I'm not duh
 all about marketing to to go into the parts of that I'm not completely up to speed on but I can talk a little bit about the difference between transactions and how that how that can come up in in practice and so does the fox doesn't die. Doesn't mean that we can't offer Winnie rice bility it means that that doing so is much more expensive for us than 4/4 spanner because we are because it's Tanner and atomic clocks they know that all the clocks in the in the cluster are synchronized to within I believe 7 milliseconds of each other
 and so it turns out that that an easy way to provide linearizability is to just insert a 7 millisecond sleep in your transaction at certain places and then that is enough to to guarantee linearizability out for us without that without atomic clocks we have to rely on MTP instead for clock synchronization and that gives you much looser guarantees you know you may have to get you may have an upper bound of a quarter second for half a second or something like that for your clock synchronization and so you can be linearizable by but like forming that sleep but sleeping for a quarter of a second is a much less attractive proposition than sleeping for 7 milliseconds and so if you really wanted to run in and when your eyes will mode that is that that is an option but it's not
 that that's something that said that's always appealing but the the difference between serializable and winterize bowl that the difference is very subtle but the best way to explain it is that in a serializable transaction like cockroach provides everything is very strongly consistent between two transactions as long as the two transactions touch at least one key in common if the if the two transactions touch at least one key then that key kind of acts as the access the coordination point to ensure that one transaction definitively comes either before or after the other
 if you have two transactions that don't touch any key in common then those two transactions can be in a kind of ambiguous state where they commit at almost exactly the same time and it's hard to tell which one comes comes before or after and so that this is where linearizability comes in in linearizability it is definitely possible to to assign an ordering for these two transactions even though they didn't even touch the same keys in a in cockroach it's possible for
 it's possible for for three transactions Each of which you know overlaps with each other but but doesn't they don't all conflict with eat with eat different action conflict in interferes with the kind of the neighboring transaction on one key but not not with another transaction sorry I'm not explaining this to get into
 you can get into cases where two transactions were actually kind of Ambiguously ordered where no transaction transaction he comes before transaction fee would come before transaction see but if you consider a and c and isolation then there's that then there was some ambiguity between them with the Define an ordering between between transactions that that don't touch don't touch overlapping Keys okay and so you know for a listener who doesn't know much about distributed databases and might have seemed like we've gone from this strange conversation about distributed databases to this conversation about atomic clocks and time and they just have no idea why we went from one thing to another and I think the key point to make here is that
 versioning is really important to these types of databases and data in in spanner which is like you said today by The Inspirations for cockroachdb where is temporal and version give an overview of how versioning Works how different versions are stored and why clocks are so necessary to this so early when I was talking about the monolithic key value sortedmap eye gloss over 120 tail there which is that the key for all of these for all of these key value pairs there's actually it's actually not a key value pair it's a key time stamp value triple and so eat shot in each value has a timestamp associated with it
 and so that that timestamp is assigned when the when the transaction is written and so that's that that's why it's so important to have the
 did you have consistent clocks across all of your all of your notes to make sure that when a new value is written it has a timestamp it is greater than any other value that was previously written for the same t
 so how many versions of a value are stored at once do you just you just overwrite the previous version or do you keep track of the past so when a good for read-only transactions because read only transactions will have a timestamp assigned when they start and then they can always read the value that was current at that time even as other transactions going on on top of them so this is that this is part of our our concurrency model so that read only transactions can attempt to see you to execute while the while the data is
 it is changing for in the future that they operate at a consistent point in time and so we keep the receipt date around until a garbage collector comes along and delete the old versions currently that defaults to to keep him the old versions for a day this is something that can be configured and you may want to either reduce that value to garbage collect things more aggressively if if there's a lot of churn and and so you're wasting a lot of space on this historical data or you could increase the value if you want to keep the old at around so you can query and say oh what was the value of this of this table as it existed yesterday
 got it so you know you touched on some read some stuff that happens during a read stuff that happens during a ride but I'd like to talk in a little more detail about how a reed versus a right Works in Spanish or or cockroachdb just to give listeners a better idea of just how the transaction ality you work so maybe you could describe a read from top to bottom in a right from top to bottom sure going to start with rights because that's actually slightly yet slightly simpler to explain and and the read the report won't make sense until you see how it interacts with the with the date of this being written cuz that's where the date of the from but so in a right like I said everything goes through raft and so the right always originates on the leader of the range and then wrapped it submits the proposal to Raft
 after conducts its riddle election among the among the replicas and and then comes out with an ordered list of a of operations to apply and show me an all 3 replicas will apply this is operation in the same order and said that that that's what guarantees that the the data stays in sink and so
 so all rights rights are fairly straightforward given the be distributed consensus programmed it was that we were building off of it would be possible to do the same thing you could just get you could just send a read into raft and and then get the get the dog in tree back out of raft and that would give it an order in between so this would tell you which which version of the updated you should read because you know what what order the consensus log came out in but this is pretty expensive involves talking to all 3 replica these replicas have to write to disk and that's that's just way too expensive for
 for a read-only query especially when you consider that in real world systems reads Tim to be far more common than than rights and so we need a more more optimized path for Reeds and so the way we do that is by granting a lease to the leader of the range to say that that that Weider can conserve reads on its own without talking to the other replicas for as long as that Lisa's ballad Deweese is actually managed through three wrapped itself so that so that we know that nothing nothing can go in happen
 before after the police nothing happened before the lease with granted or after it expires and all of the notes agree on that through the order of the raft log and so the that the the Weider has has a lease which allows it to
 to serve these reads directly and it also is responsible for doing. Kinds of bookkeeping to make sure that it won't it won't serve it won't server read that complex with a right that it's already proposed and it won't propose a right that complexed with a read that it does that it's already served
 so I'm sorry maybe I am I said to you say that when you read you provide a timestamp or does it just read based on a timestamp of when the time comes in there yeah so at the at the level of the range by the time it comes down to the range to the Reed already has a timestamp assigned to it as a part of every transaction you can also do a read outside of a transaction that that doesn't have a timestamp associated with it and then it just gets the latest value when it when it comes through to the reader but if it's if it's part of a transaction then it will have a timestamp assigned by the time it comes down to the to the range later
 and the timestamp is kind of a guard on the I guess the level of time-based consistency of of that read yes at the time stamp is the times I was just saying you know get me the give me the value that was current at this time and so the problem so that the reason this is kind of complicated is because rights take some time to come through and so so so it's a you get a right coming in at that time he won so that the right get proposed it's kind of stuck in the in the process of going through raft and then you get a read the comes in at time at time T too so that read can't can't proceed immediately because it no because that leader knows that it just proposed the right at time T 1 and so it can't I can't serve the read until after that that right at T1 has has either completed
 easy to run through the process has been completed or failed and been aborted
 yeah
 so this is a pretty Edge Casey but what happens if I try to read a value at a certain time and I find out that that time has been garbage collected away that would just be an error and so what's the most recent date of it's been it's been let you let you read any further back than that
 okay so prior to spanner there was a Google technology developed called big table and the authors of spanner commented that Engineers who used big table sometimes complained that it was difficult to use what were the usability problems of big table and how did spanner improve upon them so the the fundamental problem with the table was that it was it was not that was not transactional in it and it was it was so you are the same as the problems of most of the sequel databases today which means that your data you have to be very very careful and the way you handle your data because you may be seeing an inconsistent few word your data at at any given time you may be seeing you may see her before the record is it supposed to point to or vice versa
 and you may see different indexes that are mutually inconsistent with each other for example of support for indexing you had to do all of that by hand
 and so you could build you could build systems on on big table as long as you could get you know tolerate a certain amount of inconsistency and get you a lot of applications can if you're getting it doesn't really matter if you know if your bike takes it takes a few seconds to show up for it. Let me know if you're if you're like county is is off on a on a photo when you when you view it but for other things that you like if you have a comment thread where comments are showing up out of order that you know that that that can make things very very hard to follow and so depending on your applications tolerance for inconsistent data it was decided that it was either to try and fit that into the more limited capabilities of a big table
 we started out this conversation talking about the high-level design principles of cockroach DB and weave touchdown some of those we've delve down into the depths of the engineering aspects what are the aspects of the high-level design principles of cockroachdb that we haven't really touched on survivability perhaps yes or what is closely linked to to consistency and yes so I need a base comes from it's a database that is it is as difficult to kill SAS cockroach and so what this means is that because your data is is fully and consistently replicated across the across at least three three copies of everything then you know that you know you can use the machine and everything
 just transparently shifts to work to another another replica that still up when I took root loses a member than the two surviving members of that group will what will running election amongst themselves will be able to pick up right where the right where the failed note left off because everything is is consistently replicated and then and then the repair process will kick in and say okay this this group only has two live replicas I need to find a third one and then it go to it uses the same mechanism as rebalancing to get to reassign that that that data to a new a new host
 okay perfect NSO at a product level cockroachdb claims to offer the strongest survivability story in the industry is there an edge that cockroach Stevie has against similar systems from Amazon or Google or Microsoft
 so so when you're when you're comparing it to to the services that are provided by the by the cloud providers like like amazon-dynamodb or RDS or Google counterparts with their right but they're cloud datastore and their Cloud managed sequel offerings the
 that you know that the level of survivability that you get with these products varies from product product for sample I think dynamodb till recently at least couldn't do cross-region replication you had cross availabilities on reputation but it was still still tied within a single single region with the Cockroach DB you can however however you like you can spread them out spread them out around the globe get you know we don't currently perform very well in that case if your if your replicas are too far apart just because we're not we're not as efficient as we could be in terms of minimizing unnecessary round trips between the replicas but it will work and stay consistent no matter what and one
 advantage that we have in comparison to any offerings from the the big cloud providers themselves is that it is that we were neutral in terms of Hosting situations and so you can take a you can start out with cockroach DB on ec2 and then migrate from there to to cockroachdb on on Google Cloud you can actually run this concurrently you can replicate data from one Cloud into another cloud or have a get no third replica in your own your own physical Data Center and so you have flexibility with the Cockroach DB that that you don't always get with a with a cloud providers managed solution
 so Google is working to build its own cloud platform with externalized versions of internal services that thinks they written papers on lake tensorflow and big table how does cockroachdb differentiate from what Google might be able to do if they built an external implementation of spanner
 so I think
 the biggest the biggest thing is is just what I was saying about that block in you're not you're not tied to to Google's hosting you can take take your data and replicate it out of out of Google's cloud and into it into another and so that that that gives you a daddy I gives you a lot of a lot of flexibility because you should get you nowhere bridge for finding finding the best prices that the other at the other big advantage that we have is that because we were open source at we can cockroach DB can run everywhere from your laptop on up to on up to a huge Cloud deployment and so you're running exactly the same database in your in your development environment as you are in your production deployment
 and so that's something that that that you're unlikely to get to get from Google they were they would have to build a separate local development version as they've done with their app engine datastore but then you were developing against something that's not quite the same as as what you would see in in production
 right so their incentive is always going to be to make some sort of Black Box pass thing that works on top of the Google compute Cloud but does not run on your own laptop where you're not paying for Google infrastructure prices okay what are you working on now a cockroach and what's in the future so as we're recording this we actually just released our our first beta version last week so we hit the hit beta-1 which which it is still fairly early stage of development for a database get you notes people are rightly very cautious and conservative database Technologies so if your first beta of a database is definitely for adventurous early
 doctors but that we've reached a point where it's not compatible changes that will break your break your application if you start start developing against it and so this is weird kind of moved from the from the initial development of the core functionality and features into a stabilization and performance phase of development Denso right now our efforts are all on ensuring that the system works and and performs well under under load and you know degrades gracefully when overloaded it doesn't just run out of memory and die and things like that so are our Focus right now is that is on on stability and reliability and getting from from beta to the kind of 1.0 release that you can but you'll be able to trust your life data with
 well that sounds like a great place to close off Ben thanks for coming out of software engineering daily cockroachdb is a really cool project and you have done a great job of explaining be distributed systems Concepts that stupid database Concepts that difficult to discuss over pure audio so thank you and thanks for having it's been fun
