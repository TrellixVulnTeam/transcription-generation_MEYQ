Transcription: column oriented data storage allows us to access all of the entries in a database call him quickly and efficiently calling her storage formats are mostly relevant today for performing large analytics jobs for example if you're a bank and you want to get the sum of all the financial transaction values that took place in your system in the last week you don't want to either 8 through every row in a database of transaction it's much more efficient to just look at the column for the amount of money and ignore things like time stamp and user ID you don't want to look at every single aspect of a row you just want everything in a single column Julian lady who created parque a file format for storing columnar date on disc Jacques Nado is a VP of Apache Arrow which is a format for in memory columnar representation and they're both part of dremio they join the show to talk about how columnar data is stored how it's processed and how it's shared between systems
spark Hadoop and python this is a topic that is only going to grow in importance in the near future as data engineering becomes a bigger and bigger component of a software companies important stack and I had a great time talking to these guys so I hope you enjoy it too
 to understand how your application is performing you need visibility into your database vividcortex provides database monitoring app for my sequel postgres reddish mongodb in Amazon Aurora database up time efficiency and performance can all be measured using vividcortex don't let your database be a black box drill down into the metrics if your database with one second granularity allows engineering teams to solve problems and faster and ship better code vividcortex uses patented algorithms to analyze and surface relevant insights so users can be proactive and fix performance problems before customers are impacted if you have a database do you would like to monitor more closely check out vividcortex. Com SE daily to learn more GitHub digitalocean and Yelp all use vividcortex to understand database performance at vividcortex. Com SE
 you can learn more about how vividcortex works thanks to Vivek or text for being a new sponsor software engineering daily and check it out at vividcortex. Com SE daily it's a pleasure to have you on board as a new sponsor in the Dem and Joc net or with dremio Julian and Jack welcome to software engineering daily thank you Jeff today we're going to talk about the columnar data format the Apache ecosystem how these things are evolving let's start off by describing the columnar format and how it differs from other formats why this is an important topic Julian could you start us off with that so try to send any data processing started Wiz raw rented and it's just more natural way of treating data but
 has the processors Ivy bold and become more I sophisticated representation using much more efficient way of representing the data for processing and this is because of multiple accusations that processors are doing under the hood and show in contrast to the row oriented set up like a few big bro it's maybe got no Roper user maybe has user ID and username and user purchase ID or something like that but if you're looking at the columnar format you're looking at an entire column in a database so you might only be looking at all of the entries of user ID but nothing else is that accurate so did the natural ways of the royalty like you were saying so you should have 3 columns and they're different types like maybe a string
 integer or a date then when you're represented you're going to enjoy your leave date out different types and it has different drawbacks on how you can if you should be processed at when when you do a call now representation so the way we visualize is two-dimensional we wish visualize rows and columns but when we represented in memory on disk then we have to split it into a linear representation so user right one wrote the time or 1 km at a time so there are oriented is interleaved and you have one you know you ain't really did have different types when we were at a time but you can call him now are you put all the values for a given room at the time together and the advantage of this is that first yeah you would be including beat of the same type so you do can do several optimization ready teaches at
 RK for example when you encode multiple integers together you can use tricks like run dance and coding orbit packing because you ain't getting things of the same type and you can include them together and more efficient Pharma and yeah we get into the system so I can park a and arrow let's talk about the broader ecosystem a little bit more there's there's this problem in the open source Big Data ecosystem where have all these tools you have like spark Hadoop Cassandra all these different systems and they need to share data with each other so how does data sharing typically work among the sister how it worked in the past so I can use one example of that currently has been a problem or is not working as if you should kids you could like
 WhatsApp iPhone that's been one of their popular tools for analyzing data and you have tools like numpy and pandas size tire in memory processing in Python and people are very happy with it and it works well as long as it fits in memory on one machine and then like a portrait of way of working on bigger data and splitting it on multiple machines to use 500 spice Park and in this yusuke's then cuz I have like a drop in performance due to a distributed processing week ahead of communicating from the Juno need to distribute it fashion in joining ballot results together but you get to the extra overhead of serialization deserialization over the wire and so what one
 one of the problem is a spark run some the jvm and inside a Java base process when when python user not Eve code running into guard processes and so they come from two different ecosystems and the certification disorganization of objects on The Wire end up being a very inefficient mechanism to communicate so that the arrow is to at least common representation that we remove entirely ization bee circulation cost because of your presentation become to walking format and so we can use tricks like shared memory and make all that interaction much more efficient are talking about this this difficulty in data sharing we're also talking about this this columnar data format what's the connection between these two
 who issues like help help me understand why we need to frame the the the problem before we get deeply into arrow and and Parkay help me find the problem little bit better where we're talking about both the the issue of data sharing and the issue of the columnar data format yes so those those two prime to a little bit interleaved so arrow is interesting because it's both an efficient representation for data processing and a standard because if you have only a deficient format but if you actually use two different efficient 4 months are between to see Sims then the interaction of the two we have to translate from one to the other and that's where you get cherries ation bee circulation cost if you use
 a common format between things but it's not a form of that is sufficient for your internal representation then you still need to translate between the communication format and your internal representation so that the goal of arrow ear is to be both and efficient internal representation for data processing and we will go later I guess in the details of why columnar is more efficient and soul so efficient and the standard representation so that he can eat cookies from one system to the other without any kind of translation the performance bottleneck that you get without a system like Arrow or a system like parquet is you have serialization and deserialization because you have to serialize it into a generally readable format and then pass it
 to another system like you're passing it from python to spark but with arrow you could just put it in this one for Matt and then put it into a location in memory somewhere from Python and then spark can just consume it in the same format so you don't have that serialization deserialization cost yes that's correct so what are the other guy performance penalties that exist in this this data sharing ecosystem is it is it just that serialization deserialization thing or is there anything else the main goal of moving to columnar is to take better advantage of modern CPUs things like a better advantage of the pipe lining in the CPU opt for the cash locally and things like Cindy instruction which is a single instruction multiple data at which animals are walking faster
 that work faster when using a colon are executions skiing in Victor Isaac the kitchen which depends on a culinary representation in memory majority of data analytics work that's done with these big data tools like spark is it primarily operating on columnar data is like is that the ideal circumstances are there a lot of operations will you still do want to do operations over rows most embarrassing you'd want to do on the columns and a lot of those existing Big Data System to either have already partial vectorized execution are they moving towards it Sparks equal drill planning to move to Victory sized they will actually do it
 royalty to call Marion execution because you have to flip around the way you execute things like a transposition of the other systems budget is the natural way things are going because it goes much faster familiar with this area. Explain that in terms of an example like what would be an operation that would be useful to perform on columnar did and why or why is analytic moving towards this columnar analytics fashion so example of evaluating an expression make maybe you doing a A + B + C / something in a typical volcano
 and that's be Sunday volcano paper that's one of the initial you know sequel execution paper when I was around any more oriented you would take one hour at a time you know I need food goes through the expression evaluation whatever tees expression is when we were at the time and part of this there are lots of virtual colds in a when you you would do her a cool off evolve on this expression that we evaluated self sub expressions and this end up being jumps in the valuation of the code and the reason I'm talking about that is because modern CPUs sorry let me take a step back because I think
 where we're going to go through a bunch of quote that we use I'm not exaggerating one instruction after the other anymore you know it initially first processors that were created when evaluating one instruction after the other you know it was simple you have your program it needs just going to visit you get restriction after the other after while notion of pipe lining which is an optimization that CPUs do which is starting to execute the next instruction before the produce when he's finished so that each instruction is split in a pipeline of steps and modern CPUs have like like 12 steps are these order of magnitude and he's actually a lot of steps like a dozen steps and he started shooting the next one before the previous when he's finished so that the overall strip width of the CPU is faster and
 foods that there's a street that before that the secret we try to predict when the next instruction is so whenever you have a jump in your coat which can be an if statement we can be a loop or a virtual function call they will end up being jumps which are based on a decision and result of an instruction will decide what is the next instruction to Exotic and so the process are we try to predict when it was his one of those jumps is going to try to predict what next instruction happening because he's trying to execute to start executing that next instruction before you does it result of the previous one that determines which is the next which is the result of that is our if we're doing another loop alright the lupus finished and we're going to the next instruction and so because of that the execution
 what's faster because it makes things that are very easy to predict for the presence or
 incapsula is a cloud service that protects applications from attackers and improves performance incapsula sits between customer request and your servers and it filters traffic preventing it from ever reaching our servers botnets and denial-of-service attacks are recognized by incapsula and blocked this protects your API servers and your microservices from responding to unwanted requests to try and capsule it go to incapsula. Com SE daily and get a month free for software engineering daily listeners in capsule is API gives you control over the security and performance of your application whether you have a complex microservices architecture or a WordPress site and capsule has a Global Network of 30 data centers that optimize routing and cash your content the same network of data centers that are filtering your content for attackers are operating as a CDN and
 beating up your application to try and capsule it today go to incapsula. Com SE daily and check it out thanks to encapsulate for being a new sponsor software engineering daily
 play secure company like Spotify for example Spotify has all these analytics jobs that they want to run over all of their data for example like take all of the length of time that anybody listen to a song across Spotify and perform analytics and find like the average length of time that somebody spent listening to a song over the previous day in order to perform an operation like that all that you need would be a single column just to call him of how long did this did a person spend listen to this song so I'm just trying to emphasize give people a better picture for this focus on calling your data because we have all these types of analytics jobs on the back end where you don't want to have to get you don't want to have to force your query engine or your database to look at every single user row and just pluck out the
 relevant call him in that row you want to be able to just say you know what just give me this entire Call of Duty just give me this one specific aspect of the entire database table so that I can perform operations across that entire column that's a good point I think it was things that we may have not entirely talked about is his that really there's these two classes of columnar that can we just call him there and we were talking about in two different contacts right and so there is calling her on disk which is a way to structure data on Dish to make an efficient to pulled it off of disk for analytical purposes and then there is colander in memory and so, nor on disc 1 of the really key things about that is is this situation where you might have a hundred different columns and pick their table time is it at exactly what you were just describing you have a hundred different columns in a table now but you really just want to look at it I'm group in a look at sales by region and so the really the only two columns which is
 virgin and the amount of sales and so if your if your roll eyes on disc it's going to be very expensive is you have to read all of the other columns the other 98 columns in addition to the two that you're interested in and so, therefore mats on disk have become very prevalent with Park Acer absolutely leading the space there and then Julian who's on the chat here being that the creator of that and so that allows you to use your disk efficiently and only pull the things that are out that are relevant and also when you're on disk you have a bunch of benefits around putting things that are like each other next to each other and so you can do a lot of really good dance compression is it in the second category is this, remember stuff or a carpenter in memory stuff like The Arrow stuff where it's about again trying to make things efficient but the concerns that you have in memory are different than concerns you have on disc and sew in most cases you'll be taking, your data off of disc moving that into memory and you already get to the point where you only interacting with data that your is relevant to you
 and then from there it's about how fast can we make how efficient can we make the processing when it is in memory and how can we make it so that we can move that date around between all the difference of loosely coupled analytic systems that use today I make that movement very very so easy and cheap for people like your big data analytics fast-acting probably looks like you have hdfs which is the Hadoop file system at your storing default you have in that file system you have parquet files and Parkay is this like you said this format for storing columnar data on disk and I was looking at the parque for my go into the into the weeds very much when I was researching for the show but it was pretty interesting it's like I'm sure you're familiar with this Julian and probably you too shocked but
 it's this mystery format basically could have given nested tree where the nodes that are above the leave so all of the nose that are higher than the leaves describe the schema of The Columns of your table and in the leaves of the tree are the actual data this is pretty cool because it lets you just irate you know through the tree until you get to the specific column that you want to access it that an accurate description of the Dremel paper as two main contributions one is distributed execution of queries and you're the one is calling our presentation which animals Astoria an AC data structure into a flat kind of nerves in Haitian
 so that they the schema description she Parts structures and list which they called a repeated shields in groups and and the trick is is you you have a repeated structure and you want to report them that in a flat cut on our presentation to do that the general idea if you if you use a flat schema representation you can use a bit a zero or one to represent weather value is Nala are not so you're going to have a call Nora presentation when you put all the nominal values together and then next to it you will have a big field that we tell you for each value whether it's not on that, your representation expense on that notion so you could imagine if you have an issue did I start your where
 multiple levels in your schema instead of storing 01 to say where the value is null you store a smaller integer that store at which level it snow so when you access the field if it's not the first level then you restore zero if it's not the second level you still want any streets Define then it would be the mack The Depths actually the depths of the schema which makes it that Define sewing a simple case you know 0 means it's not Define one means it means it's Define and more General keys off of Misty there a structure anything that is smaller than the depths of the schema means it's not Define and it tells you at which level is now and the maximum value we means that it is actually Define okay so now you've given a brief overview for the park a storage format let's go to Arrow a little bit and then we'll talk about
 pulling data from parque into Arrow Apache arrow is this project that focuses on the columnar in-memory analytics and you're off and pulling your data from parque files on hdfs into a narrow format in memory and then you can perform different operations on that in memory of representation whether you're performing it from pandas and Spark or if you're doing operations on arms are pandas in python or if you're performing operations on spark using Java they can all operate on this one columnar in memory analytics for Matt to give us a little bit more of a picture for what the problems that arrow is solving for the in memory analytics format
 so that it did parque and the arrow Misty representations are slightly different because they optimized for a different Radars so parking is more for persistence as described earlier when you crave did I you usually select a subset of the columns and you want to access of columns faster and make sure I owe is reduced your minimum so you access Only The Columns you need and those columns get compressed using data we're and Coatings Earl optimize more for suffuse Rupert so the cost of a 02 the main memory is much smaller than the cost of IU to disk and so here we go to my smart rewards CPUs Trooper which is why in the in memory representation De Niro we cheap empty slots for null values so that we
 and get faster access by index on things and you have other advantages like random access to data so I can park at you kind of have to eat you're a true old values once you set it to the subset you want any narrow you can run them in the axes by index each individual values in memory systems and how they are optimizing for different things what happens in the pipeline when you're pulling data off of park off of a parquet file and pulling into Arrow so wet when you read from Park in an apple eating to Arrow we need to convert the definition level representation which is the one I was describing earlier defined by the Journal paper we need to convert from those definition levels to in DC's of where to ride them what index to write each value in memory in the arrow vector
 and this can be done for you efficiently as basically it's kind of converting marker of defining defining to indices and this can be done very efficient Nina Victor eyes minor and also we need to decode the values from the park at representation in two more a variable and coefficient Arrow Vector efficiently and driven by what values are Define and what weather we have very few new house or like older guys Define or their nose in cheerleading today. We can switch to one way or another to decode this day. Very efficiently into our we had a show that was entirely about Apache Arrow but I want to give people a picture for how Arrow affect the overall ecosystem cuz I get the sense that it's
 find an important project because of how much the serialization deserialization challenges of interoperability between different systems slows down analytics maybe you could tell me if that's accurate and how Arrow affects the overall ecosystem so maybe we can use an example for that example you have a storage layer which is easier Pocky oh could you which is a columnar database open source project did the boys have a culinary representation and disc and then when you query that data and and they both support projection pushdowns which is sitting on the subset of the column and pretty good pushdowns which is pushing the future down to the storage of the year and they both are able to process City. To reduce how much you read
 add from disk write and do as much as you can I need to start to the year before you push it to the engine which is going to evaluate to Corey about it expressions and salon and they both can do that but they will each provide an API and a certain representation and the default DPI is a raw rented one because most of the existing systems were there you do a plane spot jobs or big job or things like I've not actively there where oriented right if you write a cascading job are spark job which are the typical tools for doing distributed processing the old at use a royalty DPI so so Steve Aoki eyes are oriented but if you use and vectorized execution engine like Sparks equal
 starting to do like drill Apache drill he's doing then you end up writing from a call now representation converting into a r oriented representation for interrupt and then converting backed you have culinary memory representation for fast Victor eyes exaggeration so there's a lot going on when when each system is actually calling are having an error in the middle means that when the storage Lee you're like kudu or arcade read the data into Earl directly they produce dark either the working representation for the execution engine and he's going to be much faster to send it over and process it so please keep totally conversion from the columnar tours oriented and then from royalty back to Cullen are you remove totally these overhead and have a much faster
 execution when each stretch a year can produce Canada in Arrow and use that directly for the execution engine to process is this what you call Arrow based storage interchange exactly so that the interchange and so it seems like who do parque can use it and you can use it also for a d. Cashing dark sea in a fast painter format that you choose for processing so what are the things we seen this kind of came out of this this is a classic case of a good place for people to collaborate an open-source is is that we saw people interacting with these different systems and if you want to read parquet or some of these other Technologies the traditional ways is that each of the system's build its own serve driver Library some way to connect to that source
 but what happens is that when you don't want to do is high-speed processing as possible people start to have to build their own custom interfaces into this type of these different types of storage to get the highest level 4 months and the reason this is the end of the day is is that mostly systems expose a row level API which is kind of sucking through a straw if you will and so the one of the opportunities with arrow is is that I'm rather than everybody having to write their own way to integrate with all these different technologies that there's actually a common format which is high enough performance that they don't need to write custom code to interact with all these lower-level things and so we're actually seen this start to happen which is that we saw activity for the park a community to build an arrow reader which reads from parque into the air representation and now the pandas Community is using that to expose and is working on integration to expose pandis aerostructures stupendous
 and by transitivity also parque access for panis we're talking about a overall model where you've got much of your data stored on disk in parque or Kudo and you pull that data from disk into memory there are also these other systems that are basically say the future is everything's in memory like I've done a show about a Luxio which is basically just everything is in memory is that is that you could you could you do that with the arrow like just use put everything in it have either of you looked at the Alexia system or another entirely in memory system the number of times about what are the two opportunities to allow arrow and Alessio to to work very well together so that I can put a complementary approaches so Alexia provides the sort of in memory grid
 the file system AP High layer which is very powerful is a very common of interphase in so people know how to write to it and use it very efficiently but it says it it it it it's the same situation which is that the on this representation is not necessarily the best representation to do fast processing and so arrow is this is this should have modern API specifically for how to moved it around when it's is its purpose when the purposes is doing analytical processing and so absolutely you could cash huge amounts of data in memory in an air representation and then allow people to interact with that with different systems in Section 1 of the sighting opportunities that we see with arrow is is the ability to use a dataset across multiple systems so the thing to keep in mind right it's just that so if you have a an on disk representation generally speaking you're going to have to transform the data before you can process it
 the transformation is is Canby you know any expense it too expensive to paint on the representation on desk by Jay sounds more expensive than type r k for example and when you do that transformation you then have to hold the data in memory in that transformed version if you maintain your data in memory in an era representation that is data that is ready to be processed as is and that allows you to actually I want to think we're working on it said it is on on system in a cross system using things at the RDMA the ability to do sort of shared memory processing and so what we see with most organizations is a set of hot data sets that are critical to the organization and if you want to work with that today you actually hit this challenge which is that you either have a situation where you have to keep a lowest common denominator version did it in memory and so like say a disk space representation or you have to
 multiple copies in memory that are more efficient for processing is each system has its own representation and so if you have the color representation in memory it still means each system needs to do its own transformation at actually use more memory to hold the alternative representation what is actually doing his processing and so with a common representation that it can be also in shared memory that actually allows each of the tools to actually do their processing directly on the stuff that you're holding and so what your opportunity is is that if you today you know use you no spark and pandas and you know Technologies XYZ to do processing on the class or you're actually typically going to probably hold if you're using for example let's see how you might hold the disc representation in memory of those things going to begin processing you may want to hold the representation of there their internal representation in memory and what happens is you actually into the situation where you're paying for the memory Minimates
 and so many customers that we work with today they can't it's not realistic for them to hold most of their hot many sets in member they can hold a few of them but not lots of them but the problem is compounded by the fact that different tools have to have their own in memory representation which means that you actually may be paying five times to hold the date in memory to satisfy all the different users of that data and so there's actually this huge opportunity with a car which is a if you can have a shared in memory representation which is available via my PC to the different tools then you can actually hold substantially larger data sets in memory with the same Hardware but you some stuff
 so she is an end-to-end data platform that let you focus on driving business value from data with Saji present you can turn your data into a story easily sharing a presentation from your data Lake Explorer data and drill down into details creating rich visualisations with Sochi manager you can easily build data pipelines of jobs mixing spark are Python and more so she is great for Predictive Analytics whether you're predicting the failure of a machine near Factory or production which sales leads will be the most valuable so she helps you take control of your wide variety of data sources and gets them in one place check it out it says g.com SAA gi.com thanks to Sushi for being a sponsor software engineering daily
 if I understand you correctly what you're saying is even in the Alexia world where you have replaced your file system with a memory based file system the file format is still something like parquet or kudu but it's more like just lower down in your in memory hierarchy because you have to do a conversion from parquet to arrow and you will only do that for your hot date assets and then for your less hot datasets you might just keep them in the park a format in memory the the trade-offs are accessing data on disc or in main memory or on the Sony state drives the lattice is different and so the trade-offs of the time you spend spent in retrieving the data versus the time you spend decor
 sing the data the trade-offs we be friends and so you will pick a different representation depending on that all of those variables is is there a a richer hierarchy that would Cash America that we can talk about here because we're talking and somewhat binary terms like you've got this you know these data sets they don't really care about right now that are in parque and then you've got these data sets that are operational irrelevant right now that are in Arrow presumably there is a gradient between those two things so if you could pay me more of a picture for how that gradient exists as field to disc do you know if you didn't fit in memory of thinking that this is becoming
 options that are starting to appear so first now these Keiser Spinning Disk that I've been getting two piece of data and then her highest Rupert's in accessing this continuous stream of data and your silly state drive that are lower licensee to get you something and it's going old sewing memory with non-volatile memory so angry at me is basically using flash memory in in the sockets that tax dim main memory RAM and so the the main reason this keeps the spectrum of different licensees of like Spinning Disk as a higher licensee and then flash drives are faster and then none will let you know memory is faster and then memory is the fastest bus most expensive so you have a graduate's off cost
 ostrich versus lactancia accessing the data labels that are starting to appear and the tread of will be different in their representation of the data use different trade off on how much time it takes to you suck the DS true so you would want better compression and more dense and Coatings versus something that would be faster to process like so arrow is more I did in memory end of the spectrum when park has more the on this end of the spectrum and you have different trade up from one to the other off when you use one or the other to getting the best performance weather I always your bottle neck or CPUs fruit is your bottleneck to that point the other thing I'd note there is this that and that that Spectrum we expect that that will Kim more and more rich in that arrow is the place where we're focused on the interval representation and where
 starting with representation which is a representation that's very useful for a lot of different situations it said it's highly efficient for across the stage and it's reasonably compact but there's actually alternative memory representations that can be beneficial for certain kinds of data right and so we have some options actually already some capabilities already inside of Arrow to support sort of a sparse versus a dance representation for certain types of Surf's Up data structures but we expect to see happen is if we actually have a specification already for dictionary encoding in the context of Arrow and so what we expect to see is is that that whole spectrum of representation it's not going to be parque or Arrow it's going to be several different representations and potentially data sets where portions of the days that you might say hey I'm going to maintain these six columns in a arrow dictionary representation these four columns in an arrow
 Sparks representation and these 12 km in a park a representation in memory and he's 25 parque columns in a park a representation on disc right so the representation and the updated the location of the of data what are media will both be configured based on the requirements of that dated a relationship to your work with you to both work at dremio I interviewed your CEO Tomer I think yeah last year and we talked mostly about drill in that conversation and I think at that time drumeo was 900 still in stealth and we don't need to talk much about drumeo today I'm sure that wind Romeo is on the market I think you're in beta right now I should wait woodwind ring is on the market will do a show about it and I'm sure I would love to at least but I'm curious about what you can't talk about in terms of why the conversation
 around the dream your product has shifted from being focused on drill to a focus on Arrow so we have people here who are involved in a variety of different open source Apache projects and we would make a lot of contributions there and carry pretty much about that I've got doing so that they won't be recognized or what we see is some of the interesting sort of direction we're going is is that we are moving from a situation where we have monolithic systems ride like you think of oracle's that serve very traditional sore monolithic data system where everything has to happen inside of Oracle to what is a very very Loosely coupled analytics ecosystem right where it's not one to I'm using this its 12 and I'm combining together and it's best to breed for the particular use case I have such today it might be sparking tomorrow might be drilling the next day it might be and polish it solves a particular use case very very very well
 and so what open source has been especially good at is building the the common components that connect those things and so that's really where we focus a lot of our time in South Park as an example of a common component is used in many many Technologies we're working on Arrow being incorporated into another these Technologies as well as a common component were also involved in a project call Apache calcite which is a process I query optimization framework used in a bunch of different projects and so really we're drumeo is focusing a lot it's time is on a contributing to the common opponents to make a better version of a better to improve the ability for people to operate in this Loosely coupled ecosystem cuz we all know that Loosely coupled can be very powerful but it can also be very challenging especially if you start to say hey well I can use these tools together but all the sudden is very very expensive to use them together and so what we're really driving towards is this how do we make it
 so that the we should have helped to better achieve the promise of Lisa Capital while reducing the pain of losing a couple who works at redpoint and we had a slight conversation about dremio because he worked on the investment from redpoint and Romeo and we're talking about you know what other Technologies in a graph ql in 2 L or a similar project from Netflix called Falcor with these other projects where it's also working on this data interoperability problem it's except it's kind of at a different layer of the sac like it's more at the the request response stacked like about me user logging on to Facebook Facebook has to fetch all this data from disparate data sources and they've got a bunch of databases and they're going to pull like the likes from one database
 the photos from one database the news articles from one database and such a mess of different things but you can represent all those your question one Big Blob of a Json like structure that is called grapholect graphql structure and then that request hits a graph ql server and it gets Federated to all these different databases and when we had a brief conversation about then the more I thought about was like well you know right now we have this this Paradigm where there's kind of this idea of like offline data and online data there's like the offline data that's the stuff that you're Hadoop or spark or whatever is going to be accessing and then you have to the online data which is like I log on to Facebook and affection my photos and stuff and it's easily accessible but that is like a kind of a false dichotomy because you can imagine a situation where overtime you know your request to a graph ql server gets Federated two systems that could just access Arrow
 data structures and then 10 memory that's fast and then it's like oh my gosh that's a world of possibilities because you take all the offline data and basically make it all my data that's accessible to users that that's a big No-No absolutely although I would say that wasn't did this to sort of the phone it was probably a two by two on this which is that there is the concept of offline dating online and I think that anybody has worked in the space those to those two worlds get very blurred very very quickly and then sort of set of things to look at which is analytical purpose for this is what I would describe as more Opera operational purpose right so people have the need for example to respond to a user request and give them the right information for that request right now which is why I just got to the ferry operational thing I need to tell you you know how many likes this post has or I need to do you know that you know share this message with you from your friend or whatever
 operational purpose you're dealing with small amounts of data on each individual sort of transaction if you will but you have a large quantity of these types of operations right I got a hundred million or billion users are hitting the service all the time right and then you got a second set of scenario which is a lower quantity of interactions of data but typically on a wider breath of data so that way I can call it an analytical purpose where I might be wanting to you don't understand how customers are working in our haven't had nothing by users are using a particular feature and I want understand which ones you know give up I'm using this feature versus those that like to use it a lot and how long do they spend using it and is that some sort of an analytical purpose and so I think that what what's the white what's happening is this kind of you're seeing the traditionally it's been kind of very much like analytical is offline and operational is online it's kind of like the way that that's what chart should it was laid out but I think that you're absolutely right which is these lines are getting blurred in
 so I won analytical like you know you using streaming systems to be able to do what is effectively should have a very short-term RV Resort of present time analytical work is very very common now and these operational workloads are starting to look also very Analytical in there in there in there sir of nature in that you may want to not only see hey this message just heard from you but also how many other people are you know I'm excited about this for a power of interacted with this message which is a little bit more of an analytical type of metric and so I think that you you do see this sort of vision wear these things are coming closer and closer together and having common ways to interact with them is very powerful that being said I think that one of the things I still see a lot as is a split in the types of users for each of these things and you got what are I would say more frequently
 developers who are doing operational types of uses where as you see sort of more analysts business business owners those kinds of people were much more interested in the analytical types of answer so you still see some of those separation in terms of the users but the technology I think I'm getting very very close together for sure shock you Randy distributed systems team at map R and Julie and you let the data analytics pipeline at Twitter in your lives before joining dremio so these are big companies were you have you get a lot of exposure to the chat like the kind of the biggest challenges that exist in the Big Data ecosystem maybe you can each give your perspective on what kind of things you saw what challenges you work
 close to whether it's related to the challenges of columnar formats or not what I see is a an extraordinary amount of technical Innovation happening in in a lot of these places and in data in general right like what's happened in the last 10 years and data has been a problem in terms of not only the rise of of all of these sort of Big Data Systems the rise of the sequel to the ability for developers to be you know magnet is more agile in Croatia of applications and they did what they were able to do you know 10 years ago and democratization of data control from Service central light i t and DBA is to an engineer being will sit down stand up a mug Winston's building application to start working on it so you get this soda explosion of capability in that explosion capability has I think giving people huge amounts of powerful
 abilities I think it's also made developers live substantially better but I've also seen though it says I seen actually a I would say I have some level of degradation of a huge portion of the Alice were closed and we're glad we're glass and the reason is is that you got this day to all over the place in all the different systems recipe from an analyst perspective the monolithic world was kind of pretty nice and pretty friendly and pretty clean rights of the DBA made sure all my data was nice and organized and I knew exactly where all my data was and I could get access to it and the process for dividing find a new project was very was in a very structured so that we always made sure when for we started a project that that the date of that need to be tracked for me a blueprint Alice field do his job was done very well and so what we move towards is this this world where things are much more Dynamic much more agile people can build things much more quickly and I'm and you get these are you know
 the world in you know like 1/10 of time that they used to take however what's happened is that the analyst is kind of not always thought of immediately as part of that and so they often have to deal with to the ramifications of sort of I know I move fast break things in the ant and Alice aside so I need to deal with 20 different versions of data because we keep changing the how the you why is this is storing things are Howard tracking things or whatever and so what I see is I see a huge amount of technology that is trying to help this about that technology is often complicated to use and is requires the the the very very talented resources to be able to use it and so you can you get a lot of success of using that technology in the more sophisticated organizations you like for example a lot of people in the in the Silicon Valley area here they have you know he's amazing teams that have these date mazing day
 engineering and it infrastructure teams which can do all sorts of amazing things with all these Technologies but when you get into a lot of other parts of the eye of organizations that is very hard for them to find the level of talent and ability things together to solve real business problems and so I think that there's a huge amount of Swift pacity and Technology I think that sometimes the the challenges are there at the pieces don't work well together that it isn't easy to get them to solve your actual business problem and so that's really should have one of those for the driving forces that I see and sort of what we're doing I drop me off and let me know how soon we can talk more about this is as we launch but it's how do I make it so that the the that the common user the the the millions of sea plants that are out there are the millions of people who are non-technical who are sitting there using Excel or Tableau that they have the ability to work and achieve their business goals with the power of
 is technology even without having the massive did engineering for a stead someone like Facebook or a or a Uber or a Twitter can can can can stay right so how do we give them sort of the same should have said of capabilities the same sort of power that that the disease that these organizations that don't have that staff as the people what's happened to your how do we how do we get that to other people in the world meaning of those big white companies one of the harder problems used to build a ecosystem for people to interact with data because pretty much the entire company every team in the company in tracks with data and they're not just consuming they're consuming data for using their producing Drive data sets and they all start
 depend on each other and instead of like the old world of the data warehouse where you have a source of trousseau folded it and people use it very much a producer consumer relationship we moving the world where are you have a huge graph of dependencies between different teams inside of companies and they did I keeps evolving and a structure of the company keeps a bowling and you need to be able to last you a little different change to depend on each other reliably and there are many different aspects of case he comes from data collection of knowing what's happening and then you will have a sperm detection team that will consume data to try to figure out what is Spam versus Watson a real user of the system some people will work on a categorizing users so that the service can be better cat or to their specific
 and they will all start using those different source of information than once users have been categorized can be use for sperm detection offer relevance and until you have this complex set of tools and I can be data collection that can be scheduling that can be tools for fast interactive analysis or dashboard producing and all those systems you have these big living entity of which is a company that is doing a lot of things and one of the challenge is enabling everyone to be independent and Ja Rule and do their own work why still depending on each other because the more you have links between team s'mores his chance for things breaking and changing and and breaking everything right so once you be able to hold those dependencies between old Osteen's something breaking wheel impact a lot of people in the company and so that's where you need
 should be true that we make everything more reliable and easier to work in the decentralized minor when people can build those relationships and dependencies in a way that's going to be reliable and it can be a trust and be efficient thing what they're doing alright well that's a great way to close off the show Julian and Jack thank you for giving me your time this is a really interesting conversation and I'm excited to see what develops in the world of Arrow and particularly with drumeo thanks a lot guys
 add for prize is an IOS app where users can create ads to win prizes companies post these prizes as an incentive for people to make ads if you're a creative person and you love to compete you will love ad for prize check it out on the IOS app store only the most creative advertisements win the prizes like $150 cash or $100 gift certificate to your favorite retail companies with ad for prize I am trying to change the way that online advertising works and I'm currently hiring an iOS Developer to lead the engineering of ad for prize if you'd like to work with me send me an email Jeff at software engineering daily.com and if you have an iOS device check out ad for prize a d f o r p r i z e ad for prize one word check it on the IOS app store and I hope you continue to listen to software engineering daily
