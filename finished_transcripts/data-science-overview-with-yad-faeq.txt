Transcription: your bike is a developer working on data science and machine learning projects in San Francisco he was previously featured in episode 0 in which he gave an overview of JavaScript today God is going to be targeting data science yard welcome back to software engineering daily thank you so much thanks for having me back here and I'm glad that our last episode did not end up all those both statements one kind of one disclaimer is that I come from a background of computer science at my exposure to machine learning on Dairy science has been to in over the amount that I've been exposed is definitely less than the amount of software engineering and this is kind of something little bit on favorite for the field folks and if you have a cell that's right I'm going to take any explaining all the things if there is fine I the way I liked your spelling it explain it is the end-to-end process for finding data
understanding what axles did I ask for how we can you know use that training set for the day and make it raining stay out of the day. I'll building an actual model the data coming up with that features the feature engineering when I see you and I literally that that's literally like picking up the problem and tell if this was a business until the actual cost if this was for that patient until the actual patient said that's in a state of science that study all those of It kind of takes over all the regions all the sections for that and I think that's why it makes it a little bit of great though in a lot of people like actually argue about it in different ways I think it's not an apple to Apple conversion Garcia and get a deer stand for something else there's a chance could you rely it does I should rely on the other subfields of in a machine learning and starts and you know different sections of computation
machine learning vs. I think that's it that's an interesting dichotomy I've seen these very different people call themselves data scientist and some of them are working XL or Tableau or some other visualization software and then there are people that are creating jobs in Apache spark or do and so is data science like a big enough field where there are back in and and front end data scientists write so I wouldn't necessarily like that it's not really. Kind of black or white back in her front end shake it depends on the problems write the problem that you are actually the nature of the problem that you were studying that's what we determine what kind of what field of data signs you're focusing on the better way to actually look at this Note 2 is not to actually chosen by what's actually was wrong or right is to actually determinant on which section is reflects on more the for example
if we look at it from a stats kind of perspective the way that says ditions afros your problem is that like a right if we just had this amount of data and if we just understand if we had just this amount of food for this then we will know that everyone's going to be happy and I happily ever laughter we going to live that way or like in the other way more customistic way we are all going to die. He's going to be alive because that's what the famine says that this is like the way it's just more proof for the machine learning at small I guess that's alright I just found out about this amount of data more like this amount of did I have been given to us and this is the start of prediction we want to make so let's see if we can make it a prediction or not that you look at it it's not like one of them can work without the other one both of them interconnect however the way they approach to solving a problem becomes different and this in itself this is like a bigger way to generalize it right but if we going to a more in the more specific overview
we realized XD stats in the machine learning and then there is this other side of the big data and the account for Medics and the actual predictive analysis. Comes into it so these are the things that a Vortex Viper Vortex off all the stuff I like the big topic but you know all the things about the big data is actually the infrastructure it's just the way that you access the data I'm not like you volunteering at that that's what I kind of tried to say like it's actually it was like in the past we just access a sequel database cuz we don't have we don't have the tools and we probably didn't have a day like that pass
the same types of data but just a different Scag back I said that that's why and and when you focus on that technicality I met you know it's like you have a team of engineering right and you have always an engineer. Always want to use one point or 100 one because it has a patch and he knows about the patch but if you realize like that doesn't really matter what matters to me is my customer is actually going to use the future if not now which bachelor you think so it's it's the same thing when you see someone trying to make a calculation using a different framework and like you know Apache and had you been it's not like all the other Big Data stock related each of them are actually good for their own needs each of them me to demand so what types of Statistics does a data scientist need to know how to calculate Are there specific statistics
are there is a lot of people like you talk to them you're like are you doing machine loading or like you're doing and he tells you where she tells me is like oh what are you actually called that if I answer machine learning I call it in a very basic regression because I used to do that in 1992 and you just said that there is that a lot of people who just take it plain cold like I said there is nothing Noble about that I know I agree there might not be no volte into it but it actually is very wide broad section of cats that like all the publication not all but a lot of respectable Publications in stock. Talked about you know it's VM and different other things right now I actually used at I mean I'm at the main approaches and machine learning
the way that actually in a machine learning the way these models work the learning function that takes the data and then turns them into vectors so the projector without representation is of the actual data so I can numerical representations of data that may not be numerical in nature or you know you like it like if we thought of you know a bunch of text books or a collection of emails those don't intuitively seem like collections of numbers but we need to turn them into collections of numbers in order to vectorize damn and perform scientific calculations on it. That's like a rough rough kind of yes that's a rough of frozen to it so the thing is the reader determine the svms that the actual function the learning function that your actual decision that you want to make that it if it's actually a classification or if it's a kind of regression can approach so that the edge
I'm kind of changes depending on that so let me I mean we can we can we can go through a few examples that I can actually tell you how to do this let's do this is a hypothetical exercise so I want you to pretend that you're a CTO of a company that Aggregates data about the weather and performs analytics on that data so you got a bunch of pipes of data coming in and I want you to figure out how you're going to save that data how you're going to process it like let's say let's say your clients are hedge funds and you want to know how how weather is going to affect crops and shipping routes in this other Financial stuff so give me an understanding of how you would treat this as a data science problem or a machine learning problem how would you build your system right
with your actual clients that it say if it was a guy who runs the hatch what's important to him if it's going to rain or if it's going to be funny. That's what I actually the outcome you need to the very first thing you think about is DACA challenge you're tackling. Awesome and you come back to your team and you look at them and there are some of them are happy some of them are not happy because some of them know they need to do a lot of work-related so let's see the challenges to actually make the prediction that we it's going to rain or not going to wreck it if it's not going to be raining it's going to be funny so we need to share respond about it we need to find a day that we need to come up with the training stats as training derossett so if we had already existing kind of label data that makes things way easier if we have labeled data we can just actually put them into a structures labeled
the weather on Maple videos like that says today was I used to degrees so I'm going to take a lot of German listeners that's awesome we have today in Celsius degrees and the wind is like that and it's going to write and we have day-to-day to it's actually 25000 degree and it's not cloudy and the wind is not the humidity is higher and it's going to be really funny unlabeled version of this you have the numbers but it's not determine if it's actually raining or if it's not raining or the unlabeled version could be actually raw yeah like I said for example I Wrong version of
should be free for the example of the nature for the weather
 so it's kind of subjective is it raining or is it just Misty outside so when you say unlabeled it just means that you cannot do proper feature engineering that means you can actually find a future that you can represent to say that I have X and I need to predict why you got a collection of data and it also tells you whether or not it's raining so I'm depending on how big the data is that the kind of the scale that you were going at it and algorithms you're going to use that's where you determine that if you actually going to be using a kind of you going to get it yourself into dealing with a big data in a specific way it was a huge amount and you needed some specific kind of distributor system in other words if it's your they're processing could normally take 20 hours and you want to take it down to 3 minutes then you one
 you use if that was if that's how you're older than wants to do like it severe example you're going to be doing some lexical similarity that said if you were to do that then you would do. Actual activation on top of the data so you going to use some some of those framework to the computation otherwise if we just the basic section that didn't really need that huge amount of data to be used then you will end up actually just straight-up differentiating the day that would you how much of the date are you going to use for training and how much of it is actually going to be used for test on Coors validation the data you can dissect it you going to say 70% of this is going to be for the training and I'm at the 30% I'm going to use for testing running across the meaning that your actual that were than that you going to ride today
 the prediction is going to be using the end of the coefficient of like that. 0 or 1 of decision using to 70% of them out of the theater so I guess in this case we would you use the backlog we have a big backlog of data and we want to train our system with this backlog of data so that every day we can generate reports with the new data so if we wanted to do that what kind of system we want to build to me what would we use Hadoop for spark for what kind of databases will be use how we set the system up without for a second if we focus on the actual what kind of data related part of it the Big Dipper part of it so the thing is that really actually good about it and it's really fast it definitely is none of these gives you one of these are the perfect technology to all that all that
 but the thing with apache-spark is it actually reduces Network input output so it doesn't actually do a lot of request it does a lot of processing using that around that it takes DACA memory it does a lot of things in memory so if you were and it's always a good learning curve for this is easier and these are all kind of factories that comes in because if you have a data scientist that doesn't come from a technical background you want something are you want to give them something that has the least you know what this the easiest learning curve apache-spark Ashley's really good at that the community around that forehead it's in it's really good at some other things but it's not as fast for some of the things that actually Apache in maximizes inputs and outputs that she had your own intranet in your own network you would want to let you know you would use something I could do to create a distribution system to process
 and forget every says it's a deal or technology fees are framework for the daily basis you have all these sorts of stuff that I think that you even had a week about Sunday. We even talked about in the first episode no sequel a big role in this area. Or actually documents and the documents they do have some sort of a relation but not that many and in memory
 we differentiate what is it at what do you mean by some of these are documents versus relations in a basic where you have the tables and they have Frankie's and they have views and you refer to view and then she would represent two other tables that are the relational database could actually function Wildwood the end of the no relational database than those thick wall when I when I say no sequel It's so that we know sequel worse is actually a structure text documents and the structure is would you going to be working with and that's why the nature of Mexico no signal works really well for Big Data is because you don't usually know what the structure if you did it's going to be you have you don't really know what the structure is that you can actually learn you should you would know you know how I'm going to make I'm going to
 officiate my data Avant, Separated or something like that so that that's why no sequel plays a big role and that's how he actually uses it what why is there this connection between Big Data Systems and no sequel is there some reason why no sequel worst particularly well with big data problems yeah I mean absolutely no sequel I'm in the in the in memory databases if I'm going to come back there in a second actually play a big role as well and that you know not not right ass is the one that actually point out to right but at the end memory database is always a play as usual because a lot of these things that you want to mimic the relation and you went to use the fastest existing technology to mimic the relation so that's why you use a combination of that and memory database
 and the no relation Amanda no relational database works really well and they scenario because you can do a lot of thinking of this is goes like the joke that you in a web scale it goes back to a kind of work it has the nature of replicated replicated easier of shorting the database of actually just reading it and making it know that you know another one I said so like the way that I actually had to stay traits that clusters for you imagine that you have these notes is an points my right hand on my left hand right to your right hand is always going to be the cluster that people of rights to and your left hand is going to be the cost of that people read from anyone these two on Note these two endpoints my right hand left hand to know about each other how we do that we always going to be replicate in the database really fast
 without causing any deadlock and all these other things so that's why it actually you know if play some really nice with that and by the way it's not like I actually without headache it's the perfect solution a has a lot of headaches they had a lot of shortcomings in his stuff is easier for doing charting and replication yeah I made it yes because I'm because because of the actual yeah because of the actual Community being around have built around right now she go on Google I don't know I might be wrong if you look if you like I'm starting databases you going to find all these other existing sources to teach you how to do shark tank different scenarios and actual clustering of that because the clustering is actually more about directing on balancing request and a shorting is actually about a dissecting the data or like to replicate
 the different weather problem once you have this data pipeline built what kinds of analytics tools would you have the analytics team using presumably would have the system that's capable of carrying your large volumes of data and then you need some sort of analytics team that works on generating this Daily report for your hedge fund that needs weather data
 so I'm not sure if I quite well would you want to analyze but if you were to do that if we were actually to still do the prediction for the prediction that said you going to pick up like an actual this is literally 101 maybe of probability you going to pick up a recall ever and now you've made a decision and you going to say you know the probability of today being this and that is tomorrow this is the outcome of the problem and it's always fall between the scale of 0 to 1 or like 1202 - 1201 version of Faith like I remember exactly about a year ago my first example of something like that
 simple version of of that other parts of the n o 61 is actually a lot of interesting this is what you want to do it right then it's going to come up with a representation of the density of the data so it tells the name of the group of the clients that sells them you know according to data if it looks like between the month of October to December the actual Revenue going down I mean I mean no shut that because winter it rains a lot for it not to diverge refunded example but the very simple version of this too and I'll text you have on the podcast like whatever that's a speaker comes on that people want to listen to your I know what text Nashville shows out there is more people coming in and maybe whenever I come on there is no one that's in there
 and the interesting part is not just find the actual interesting policy you will find correlation between Tieks so that's why you actually have some graphical models not just mathematical model see how these different kind of model that's it could actually gives you a result Lucian I'm so if your goal with all ways to find if it sunny or rainy or sunny or rainy then you could there is a lot of all the things around that area for that decision that you can find it's not just a 0 and 1 and big data problems the programmer often has to convert abstract data into numbers and we talked about this a little bit earlier with textbooks or emails enter like for example Google needs to serve ads to me on Gmail and initially all they have are my emails which is just some text and so I'm curious what do you think are what are the types of calculation
 that Google is doing to Crunch that data what are the natural language processing and and you know Vector space modeling and machine learning algorithms that they're using to Crunch the data and serve me add that I'm interested in if they have your voice call if your stairs just everything there are some things before I even dive into explaining that just a kind of a KitchenAid you on the previous example in a team of of a data science team sometimes so that's why I the beginning I said the data science term would only reflect that in a good way which speciality they focus on so you have their Engineers to people who are really good at extracting the date on coming up with the aqua Feature Feature engineering saying this scenario
 GoDaddy find semen of course they're doing way more smarter things that I'm going to stay in here but this is the very generic term so it if you were go if you were to go on your email dashboard you'll realize I feel much better right now than I felt 5 years ago why because they realize that there are some certain buttons you never click on so they took all that data and they didn't remove this I order if we do that I Were A B testing is going to have a better feeling than the e x exchange email Solutions and your real life right now you go in Gmail that's almost a different and I thought I can you have a tab e z promotions and Indy's all in itself is a not old version of each of them as a prediction or a guesstimation that actually effects on the other one so this is why you know doing a lot of machine learning over engineering machine learning could actually harm but in this scenario. Why it works
 really well because all the decisions they make are related to each other in other words that suffers alcohol migraine to reduce them so let me let me actually make it pretty picture that turns all their texts and look for a pattern I whenever I recognize the pattern that is a spam then put it into a spam folder right there for the longest time everyone was doing but then World changed we did not we never know that actually we would click on promotion we would like you know weekly Groupon email and in Groupon emails to their standards by to our standards. But this is a new challenge so how come we approach it are we going to actually Bernard our day is Thanksgiving to make that are some detection no let's come up with a new production let's try to call these you know some sort of friendly gift promotion in a different title and let's give it another friendly
 review for the user so you see the prediction goes on and now you go I got to go back into the email that's what you'll realize it's like very friendly to you and not just because that you feel that way because they actually have studied that's on so much amount of data but different kind of scale of hard right now like you have something like mixpanel right or like even the Google analytics at the Google analytics gives you a b testing of you like you say alright nobody even tries to read this so what's your prediction you're like it's because of the text color is really bad so your version of the human version of picture dictionary that I find make the background really dark and if I make the text with you light it will make it easier to read so that's the human version but when you come up with that start with a version of. For a huge scale for everyone else on the globe you know how many people have Isis internet as hard I said that's where they use the machine going again how do I could go in or what I could go in way way way
 deeper
 so I'm curious like how would you explain the problem of of the advertisements like I'm very curious about how Google processes my emails turn those into some storage statistical model and then serves me ads based on that can you describe that from a machine learning problem solving point of view absolutely hopefully they're not doing this but here is hiring the various I really hope it's not like that because I think it works really well on me so let's say I happen to be a game designer okay and all my emails full around the topic of did the game come out when is the game coming out and all the emails I'm getting happens to me from steam.com from I gamepost.com from the RSS on TechCrunch that talks about games so they get a hint this guy is about right
 it taken all the email again this is a very general overview they look for that specific Patcher and that it said this is where the unsupervised learning comes and they don't really know if it's game or if it's actually football or soccer they just know it's it's the thing that keeps repeating in my text I said that's a representation. You know they consider that to be function of something that they want to find out whatever could actually be closest to the function of that terminology for example whatever we could find in our in a googol shop or in all our ad advertisement providers which one of these could be the closest in the coefficient of this thing that the guys always talking about and then by that you can see the density of your email she's actually going to determine the density of a retirement that shows up
 very dumb down version of that I am not sure if I the way if this is still like that but I am going to give you this. It's a very fun things your tryouts go on the Facebook messenger and start talking to a friend and what do you talk to the friend try to dance your conversation into a topic try talking about a mobile games and you realize the most specific game you're talking about that advertisement on your right hand is going to be very very determined to that and when you change the conversation into the friend that is always traveling that wants to go to I don't know Kuba I wants to go to Malaysia after that you realize on your right hand is actually United Airlines advertisement and you're like oh gee I wonder why is that and the way it works is basically like that they take action of that and that's why I know that so they don't really care who you are or where you going to go they just know that there is something in there.
 Define a corresponding add to it and they showed it looks really bad sometimes yeah it's just because sometimes they know this is again goes to the to the nature of Phoenix cats versus machine learning and the staff World they would tell you that they're like if we had this amount of people crossing by this Billboards and if he had this amount of people using this razor and if we do something about a shave cream and on and on this billboard I am sure that this amount of people is going to buy it that's the fax machine learning version of this like that hey I just realized the crew a group of people Cross by here even though there is 1 million Crossing by here but 700000 of them happens to be assessed how about trying to think what ever tiesman could best fit on the billboard
 that's that's dressed in all that time
 okay so what's going to do a more technical term what is deep learning
 where are you seated to answer this question I said I'm sure there's definitely people who could give you much better explanation but I'm going to everyone not everyone but a lot of people is going to give you this answer they're going to say it's the Loosely based on human in a neurons and all that that's all confuses me honestly it always makes me like I always wait wait how is this even based on its from if I if I recall if I'm not sure I think his name is dr. Robert heck Nick heck nothing has Nicholson he's he's one of these he's invented one of the very first New York computer that his version of artificial neural network instead he said that it's basically
 A system that consists off in a that consists of a group of notes or a group of processing elements that are highly connected and the outcome of the processes depend on you know what's the actual existing data is coming in in other words it's up to you know it said layer of of processors and the outcome whatever from the outcome is going to be taken in the outside of the data is going to be presented on that that's the very simplistic version it just said that that the processing that you going to do is not dependent on one process as depending on the interconnection between them and that's why the human version of that I know of the new Ron worked on that because you don't just decide this is pain or this is actually anger because of one neuron exactly billions of neuron
 describing a like a multi-layer network and I read a quote from Jack Ray who works at Google deepmind and he said that deep learning refers to artificial neural networks that are composed of many layers they said it's a growing Trend and machine learning due to some favorable results in applications where the target function is very complex and the data sets are large do you know what he's referring to as a Target function yeah absolutely it's a one layer if you think about the actual one layer it's beyond the serious of processing that you do so that the even one layer have actually serious a processing but when you add the multi-lane this is like it is
 where you start with a set of pics and you can transfer the pixels into a set of edges for the edges and architecture is for building your neurons so there is being in order to speak involutional involutional Network on there is to be believed Network there's all these other different kind of your own nest but overall the actual noodle net in itself I mix the series of trying of a series of the processes that song that goes one after another and it determines of how did it is going to come about it I'm not one of those two different architectures for building your own your own that that's where that's where it is referred to change it because I'm each of them needs to give you a representation of the nature of the data that was for my sister. That's the very first in the basic version
 four different architecture is and different applications of different name we have all these others and that's where the DNA and RNA and the funeral that's where I get recurrent neural network and I'm inside Beast each architectural represents I'm not represent each structure is actually good has an application for a different set of problems so a day you mentioned example of to fix sauce that's about computer vision and you know we always see the example not always but I guess like the one of the most famous examples of the imagenet OK Google that's how you give it an image on you know if I had the representations like all this is a frog or this is yes or this is just a dad uses the Bushnell a different way that that's the computer vision Challenge and you have the actual speech recognition challenge you know Siri Siri listens to different accents
 yeah I understand different accents for NLP you should the natural language processing you tweet you write about stuff and didn't understand some context of what do you write about and you have one of those V hottest among all of them is the bioinformatics which is do you know a compilation of a few architectures and bioinformatics and it's always interesting combination of stats and little bit of machine learning and deep-learning applied to computer vision like what are the different layers in a neural network that would allow Google to build a
 I'd like I'd like a machine learning a deep learning algorithm what it what are the different layers where you know you have an image is being processed and first one what one layer does Axe and then the next layer does why the next layer does Z so you know to make sure that we are not going to lose our our followers so I'm going to I'm going to try to lay to the side as you know that's easier said the reason the reason we actually go with the approach of deep learning or like you know we've been using your own net worth it because that's usually have data that you know and unlabeled day. So we are going to be doing unsupervised learning meaning that meaning that the actual pterodactyl features are we going to have is a representation of the data that we don't really know if this is Locker wider this is green or blue we just know that this is the thing that has this amount of
 occurrence in the data set so that's that's you know that's the reason first of all we are actually done this approach otherwise we could have any of the new stuff again had all these imitation which means that first it's kind of slow and second of all a lot has a high fall tolerance so it's not really good to make a decision on your checkbook but it's actually good to make a decision about the picture of these are kind of things to keep in mind before we even go out you don't know I'm in the example of Google let's say trying to finalize the photo to know if this photo is a photo of a dog or a photo of a car this is how it kind of does it it takes the photo and it's a takes a lot of photos right it takes ten thousand photos and it turns the photos into vectors and a vector account at the very simple and it turns into matrix's of Records
 for the non-technical was out there like that has the Matrix is that imagine your chest for that each step of the board is a number and your truck board technically becomes a vector Matrix that each cell has a number that the vector I am the doors are vectors and your whole chest board is that Matrix that represents something so what they going to do when it says that's that's that's one picture now imagine we can have 100 so what's going to happen we cluster them meaning the groove then we going to say all these chess boards that actually have these numbers on them he's Vector Matrix is actually very close to each other I said this is something so whenever we see whenever we take another picture which one it into a matrix in a bunch of extra similar to test it's similar to this one so that's the
 a simple version of turning the pixels into numbers and what's going to happen next when you give it a pixel that actually turns into same Matrix and it if it has seen a vector similar to that before it tells you tell if it's this is very close to a cat or it's very close to a car but if your algorithm has only learned car and cat has not learned the dinosaur when you give it a dinosaur picture you'll said that it's actually closer to the cat rather than the car why because the numbers the number of his inpatient it's going to be closer of to something that has two legs in the pixels and it has a tail and it has a head lice without a car that's just the box with wheels that's the kind of what you what you understand here with deep learning is that or concept of representation of the teacher because we don't actually have
 I don't really know what what this did I ask and what's going to happen when you when you when we say that we add layers into it basically wants me once we actually get understanding of what the date is the nature of the data we run do you lean your kind of delay in your face to classify the linear regression are processes on them I'm so that's where you add the multi in at the most hilarious into them like you friended fuelers you like alright I know what it is but I'm going to add another layer to determine if this is a set of five classifications and you have all these other kind of Architecture is where it actually is going to be so what do you want to do as to minimize the error rate meaning that even if I give it a sparse data meaning even if I give it a bunch of cats dog
 elephant and car and airplane that's very sparse right none of them are really close to each other I want to minimize the error rate meaning that I don't want the vector representation of East images to actually cause my decision to become gong those are the kind of challenges that you simply try to go after and there is some of the challenges you just see now you take it as a fact you're like I'm not going to be able to ever tell this or if you ever do that so view of how you actually decide to take on entertaining bark on the journey to take unlabeled data to try to make representation for them into different now in a different architecture of noodle net about the Rhythm and and by the way I like the other the other issue or not like the other limitation a side of things
 Dad this is really slow to learn as the actual of black box kind of reason because the way these layers work imagine that's right I'm going to give you this is the basic example even if you have no idea about technology machine learning this filter from the farm farm of the guy or the hash brown. We talked stock the parts would you take the right down from the Weather exam exactly from the weather example we take me to retake that whatever they have from the farm and they put it into a filter right thumb something fixed we know that will be put in there could be very different things that we actually that's the outcome of the year of the farm but we don't really know if it's the truth or if it's actually raining whatever. Thing is that we put it in there and depending on the size or the shape of the actual objects the outcome is going to have something like it might actually mix up the grain with c
 rice with something else you know that because that's how the filter has been built so what we going to do we can add another layer to our filter that will understand that this is actually rise and a different kind of rice and this is actually an apple and this is a banana to see so it's so it's so that's the that's the simplest way to understand how the layers are being out at the problem is that what happens in their the reasoning behind it as you have very least control off like you have very least control of knowing what's the reasoning behind that actually thinks because just because today that says lunar like a class is actually going to snow going to have a very hard and difficult for many other hot challenging problems that's you going to have a hard time reasoning why it makes that decision in the noodle Network
 right so I think we should zoom out a bit and I want to talk about cattle cuz kaggle is this interesting website that market itself as the home of data science what what is cackle how do people use it so personally signed up for Geico but I happen to know about 5 years I don't think so I don't know when it started I may be somewhere at 11 or 10 something like that but the thing is that I can tell you by kygo you decide to her about it it's B Community UConn's really build
 elevation to do anything at least to me that's how I understand that I'll let you have a community around it are we talked about this in the very first episode of 0 weight ask me why would I choose a frame where I can I said you know simply because I look at the people that are using it and I tend to get myself involved the same thing about data science right there is Sciences to be this starting all these different approaches used to be something that people could only access in conferences annually and if you happen to be a part of it at Ivy League school that you could only get a chance to be a part of a community that could actually invite you to this one conference and now you have a website and you have all these really in a drilling my people on the platform not just as a competitor that people who just turned around you comment that comment on challenges they they actually see you you see them you can reach out
 the death of the community is its very very strong and there and you to that you do that a lot of great outcomes I have happened what types of competitions occur on cagel from my understanding again my very short time using a different one so there is the ones that a company comes and that's a with a challenge or resource lap right that's me and you we have the software engineering daily research lab and I will try to figure out how can we already talked to audience from Midwest. You know where like somewhere else that is in a different time zone from both of us so we have a kind of drunk I go about a group of competitors and we put a price for it right the price is going to be like if you could increase our audience we going to interview you are we going to give you something so that's the version of
 it could be a company like that say yes I'm not sure but I think Netflix price was on there I'm not sure I might be wrong I'm not sure why should I saw one from Deloitte and the goal of the competition is to predict rental prices for properties across Western Australia and the price the prize was actually $100,000 which is a pretty significant like are there people who are just doing data science contest on cattle for a living I wouldn't I wouldn't say for a living but it definitely requires you to spend that that's why you see the rash the team's doing it but I could see you know there is one thing about competition like it's a different mentality right it's not the same but if there's a similar to use a correlation and competitive programming you don't actually become a competitive programmer just by programming on daily basis there are certain things
 what to do to become a really good competitive programmer and the same thing about you know these challenges competition so there are people who actually have the interest first in the fact of that competition of the competitive approach 2nd that have the interest in working so that each of them have sex expert in a field so that's the other thing like when you do when you deal with if you if you had label data and even if you're not that familiar with the nature of the data you could still come up with a ticket in a prediction that's because later that they did it labeled but if you had unlabeled data and if you wanted to do an supervised learning on it you better have some expertise in today. I just a matter of time because most all you going to get it wrong because you just don't understand that when the factor of this number goes out that means you have made a terrible mistake
 college kids or I mean actually I I know what I have one of the top 10 top 15 when it one of them. Only got introduced to them are people who are CEOs of companies some of them are there scientists at other companies some of them are just so you know researchers there are students that's how did you see your quiet in a wide variety of competitors on the on that website but one thing that's kind of reoccuring for me at least to me from what I thought I from the people who win the competitions are like the team star have strong skills in one field and they bring it together like you see one guy who's perfect feature engineering because he's just a really strong. Strange that he comes up with the most optimize way to extract the data and get the data out of it
 to the challenge about this competition I like the I my bed like the DLow each one was in a rough and structures I mean maybe there's some structure of the data and that's the first challenge that's figure out how am I going to use this data and how much of it I'm going to throw away
 has chemical change data science I mean a lot of people say that I think I could be a bold statement but I definitely had had a lot of influence Iowa State I mean it the same way if you say let's talk on the floor actually change in a software engineering I mean yeah it's hella fat does that mean I agree and I love it I allow you all right answers and I do get answers on there but we're in there 4 times before their when they're actually they're still there are still far I'm stuck on that you know fall around that could actually have a better actor Dennis Taco Florence and I'm not saying that that's the version of software engineering actually have made a big influence and the Machine learning idea but I am and it has a hidden power you definitely have a similar to it like for me I always thought that every
 cancel 1 year ago what you want yeah exactly like what I thought I saw that everyone who you know what a second guy have not he doesn't even know that's a very big influence because I was excited by something like that you see someone definitely has a hidden potential into it that's because it's so big because it's office throwback you can't really say that I have changed that much about it
 so I have a philosophical question do you think that as machine-learning comes further into our lives and data science comes further into our lives does our world become more predetermined
 oh boy can we do another year when our if you started like they would like we would know what you mean well basically cuz as the volume of data about us increases presumably
 machines are making stronger and stronger predictions about us yeah are they making predictions or are they actually guiding our Behavior you know I have to answers for this and sometimes I prefer one of them in the morning and the other one in afternoon so the only just the fact that I have to ask her for this reason of the two different perspectives and I hope so as if we are this is one of those towards end of the episode I hope that little biased but the machine is perspective of this answer is this yes if you influence your audience if your influence your data is a simple as that you lose your Audience by something and you would actually know what the prediction of outcome is going to be in that in the end of world of machine learning when you have the data that you basically and this happens
 conditions you extensively overfed your dad said to the predictions and you actually eat going to win some dinner somewhere near the conversation you know if you examples of these as when a company tries to mate you buy a product and makes you feel that you are outdated and you're not smart on your not cool so your bias is to Warsaw be having something new cool and smart and the product is nuchal and smart another example of this is the actual I don't I hate getting into politics but this is the very August exam if you voting campaign before any present presidential campaign goes on they do polls they do all these Paws they ask them that the people what's the word that you will leave if you like and guess what I'm in the very very first Obama Campaign a lot of people just wanted something to be different they wanted something to be
 substantial something that's going to be taking over this so what was the outcome of that they wanted something that has that has a relation to the world changed so guess what the campaign came out the main World War changed guess what I actually would know how many votes is going to get because that's what people were referring to but that's a very simplistic version or actually I should say very complicated version of the predetermined data and voting and how you can actually go around it like but the other perspective that I usually prefer that I go with as the cognitive kind of biased boy we are a very sexy cat is saying oh there is no absolutely way that you could actually determine what we going to do that while you're not going to work on that is by ISIS based on social biases in decision-making biases and these are the ones that have you studied and found out a lot of them we still don't even know about them like you know the decision making by us you know you have all these I mean you yourself
 you know a lot about poker so you the bike that you have is very different than me making a decision right way to have a determination on that so that's why I'm saying actually machine learning could have a hard time to have the complete guide to going to replace human thing and you know that's a Hot Topic right now everyone's freaking out that all my God that's not going to happen but yes very hot dog shop down version of the answer with your crush
 that's fantastic I think that's a great place to close yet thanks for coming back on software engineering daily it's always a pleasure conversing with you and kit and got that great
