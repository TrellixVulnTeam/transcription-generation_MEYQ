Transcription: I love being a software engineer because of how easy it is to build side projects and I built the countless side projects I build a Social Gambling application a dating website backend that has zero users stock trading music rhythm application for Android and I think every developer should have side projects so when I have a web app that I want to deploy and share I want to show my friends my most recent project that nobody will care about I use digitalocean to quickly spin up a server and host my projects and who knows if one day one of my apps takes off and goes viral and people actually do care I can easily scale using digital oceans flexible pricing plan companies like taskrabbit that have grown rapidly have used digitalocean for this very reason but until this happens I will happily stick with the $5 a month plan sign up with promo code SE daily to get a free $10 credit as a listener of our show and start
singer apps we would love to see what listeners build so send us an email showing us your project I've also interviewed moisey oretsky who is the co-founder of digitalocean and he mentioned that the ease of use and the flexibility is why they built the service in the first place that digitalocean in interview is one of my favorite episodes moisey talks about bleeding in a Datacenter he says you don't know how hard it is to do a cloud hosting service I'm until you bleed in a data center is really interesting so it's speaking which let's get on with this episode of software engineering daily
Apache drill is a schema free sequel query engine for Hadoop nosql and cloud storage Tober Sharon is the founder of the Apache drill project and the CEO of dremio Tomer welcome to shop for engineering daily thanks for having me I'd like to start off by talking about sequel over the past decade or so we have gone from the exciting introduction of nosql databases to the realization that we still need SQL can you describe this evolution
sure you know I think the I think what really happened here is that no sequel databases came along and basically solved some serious problems that people are having with relational databases so for example they saw the scalability problem they made it easier for developers with very kind of cleaning developer-friendly ati's but I think of the same time what they didn't do is provide kind of a good way for people to analyze that data right and Siegel can I remain that language which and even if a developer doesn't need that for building their web application you know you have millions of analysts and in other business users that no Sequel and tools that no Sequel and then speak Sequel and so now we're getting to that point where people have realized that okay they still need that Corey capability in order to analyze all the data that's going to be systems
can you describe in more detail how no sequel solve the scalability probably refer to end and what other problems did no sequel solve
 sure you're no sequel databases and I I really hate the term no sequel It's because she'll like you said you don't know people are bringing sequel to no Sequel and because they realize it's important you'll never list these the collection of databases things like mongodb in hbase and Cassandra and even Technologies likes Lauren elasticsearch are serving of very very important that purpose since oh and the first problem that they saw this basically scalability problem so traditionally if you wanted to destroy a lot of data in a database you have to buy a very very big machine with a lot of memory in a lot of very high-end storage will just seems like an Oracle database and that was a very expensive to scale up both fell on the hardware and on the softer side also hit the limit at some point you just now no machine that you can buy that's big enough to to do what people are trying to do today with all that the new kind of data that they're using
 and so that was kind of the first problem the solution to that was really easy can a scale-out architecture so it's kind of first pioneered by at companies like Google where it was all about taking more commodity server is standard going to be off the shelf that server is rack-mounted servers and basically pulling together a group of those servers so that you have effectively what's one database but actually physically is running across that many of the servers and sound systems could scale to thousands of the servers
 in the current state of affairs no sequel often means not only sequel you referred to this in some talk see if given I've seen we have polyglot persistence these days or at least we have polyglot data access the fundamentally is there a reason for this polygon is or is this is his variety of data formats is this kind of a case of technical debt across the industry like we could magically get a facelift the entire industry and standardized the data format would we actually want to do that
 you know I think what we're seeing as we are seeing stuff some standardization and kind of the day tomorrow so traditionally the traditional theater model was the relational day tomorrow so you had kind of a fixed set of of columns and then you have many many rows in a table and those columns very rarely change so it's kind of a rigid schema what you're seeing now and especially with Json is that has kind of emerged as a standard way in which applications can save data persist data at weather that's storing Json document in a database things like or, it's it's a virtually every API these days for every SAS application at is based on Json sets kind of memories just standard way too and to represent data
 okay so we have this kind of strange scenario where Jason has been standardized on the form of the restoring data but we still want to access it in the form of SQL so at this point I understand the situation I know what SQL is and I know that my things are being stored in Json so ask you I was a structured query language that I often use to access a database and eventually we're going to get into drill but before we get into Apache drill Apache drill is a sequel query engine can you define more broadly what is a sequel query engine
 sure sequel query engine at the end of the day it starts with the sequel language rice or sequel is a declarative language that people can use an end tools used to describe how to retrieve data right so given one or more at tables of data I can write a sequel statement that basically says that what did I want to retrieve and what kind of Transformations I want to do on that data so an example would be I only want to select some of the columns in those tables are I want to join two tables on a specific key I want to aggregate data by a specific guy so maybe I want to see I want to aggravate the salaries of employees at by a state in which there employed so I'll be an example so it was a language of that basically lets you do that in a very simple way typically paragraph would be the size of a sequel statement in many cases versus have
 how to write a lot of custom code in the language like Java or python so that's kind of what sequels sequel execution engine or a sequel query engine is basically a software technology that takes a sequel statement something that's written in that language and then compiles at and runs that on the data right so it takes the the language and basically gets runs that runs whatever is specified in that statement and Returns the results to the user
 let's motivate the discussion of sequel query engines a bit more with a use case so let's say I'm a business analyst and I'm working at some company like a giant pharmaceutical company to decade-old a huge amount of data in all kinds of different formats scattered across different servers and I'm a business analyst so I don't know much about programming I don't know maybe I just know a little bit of sequel does a query engine like apache-drill fit my use case
 yeah that's that's exactly what drill is is designed for so after some background on Jill what was one of the unique things about Apache drill is that it allows you to run sequel queries on one or more at disparate data sources so if you're an analyst sitting there any update on different locations and you want to join that data or gain insights at from bringing together multiple at data sets from different places you can use drill to run that Corey and then depending on how technical the analyst is they can either use drilled directly and write their own sequel statements and so there are millions of people that know how to write Sequel and then some users and don't know how to write sequel or prefer to use a v i II like Tablo or click or power bi or even Excel and those tools can speak to Apache drill using the sequel language and provide a much more visual experience for doing such a data analysis
 they have this prototypical use case of the business analyst operating a laptop that is pointed at somewhere with head erogenous data heterogeneous data are are there any other prototypical end-users that we can use as an example to think about the uses of Apache drill
 sure yeah I think when you look at Apache drill there are there a number of different personas that at that fine the technology very interesting and useful and so the user that kind of end-user and in many cases is that business analyst the Tablo user for example but many cases we also see the developers or date engineer's using the technology as well and what they're finding is that its use of its useful for them in in one of two ways so the first way is Young many companies that the user's that want to add that that need kind of talk to explore data to analyze data they will she go to developer and say hey can you tell me what the top 10 restaurants are in this region based on the data that we have right and then the developer goes and does a bunch of custom work and gives them the results so for those developers are the engineers being able to provide a self-service solution for the the business analyst is a very valuable thing as if they don't constantly need to kind of provide that the answers right it's more about teaching and fishes
 what's the living on the fish the the other and the other important kind of that yusuke's here for Friday to engineer are developers at the end of the day writing a sequel at Corey is going to be a lot easier and far less code and less work than having to write custom code in order to bring data together for multiple data sources and make sure that data pipeline is always up and running and and do all that data munging if that's something you can just representing us single sequel statement so for a developer or did engineer it just saves a lot of work and it's very easy to to get started
 will get to the direct discussion of drill what Apache drill is based on Google's Dremel system much like Hadoop was based on the the mapreduce paper and other other open Starship technologies have been based on these various Google papers so what is Dremel what was Google trying to solve with a Dremel paper Super Sentai WhatsApp Hachi drills inspired by Dremel not based on Dremel Dremel is an internal system system as a system enables interactive kind of ad hoc sequel queries on large clusters of commodity Hardware so Google already had a mapreduce system and when they develop Dremel mapreduce was really designs for batch processing in so something you would run a job every night that bad kind of work or or or every hour to process a large amount of data at not a very good fit for CNN analyst our product manager summer
 who needs an answer right now and they're sitting in waiting for the answer right so after that you really need to optimize for kind of low latency as opposed to being able to run a 3-hour job and some Google built Dremel as an internal sequel solution that runs on a large cluster servers and can analyze data very fast they come by and kind of a sequel execution engine with a special what's called a columnar format that Google had the dogs internally and called call Mio that allow those cords to run really fast
 okay so this was around 2010 I think that Google publish the Dremel paper and around the aquarium systems were Pig and hi what what are pig and Hive and how did did the Dremel strategy compared to Pig and hive
 yeah so in the end I do one of the great things about I do been in the open source community in general is that you get a ton of innovation from different companies Ryan see a different projects and taking different approaches and a much broader ecosystem then you would get if you were just kind of one company building a proprietary technology and so what happened in that ecosystem as we had a we had a single execution engine called map for you set a time and what people realize was that will be on the just the challenges of how have the execution the time it takes to run a job there were other challenges with mapreduce which is that you really had to have a strong Java development background in order to write one of those Matthews jobs at so it wasn't something that you can take an analyst and have them right now previous jobs and so two layers were basically developed on top of that on top of mapreduce so Facebook built this system called high which basically took a sequel queries and compile them
 into my previous job so it was still at the end of the day my previous jobs that were running but the user was basically submitting a sequel query at Pig was another approach to do something similar it was kind of more of a I knew scripting language that was built on top of mapreduce a lil bit more procedural so you could do slightly more complicated things was great for ETL type or close date of Transformations but again you didn't have to write Java code in order to actually write one of those
 okay so now you giving us a picture of what the query language landscape looks like and then Google publish is This Journal paper and Dremel was eventually like you said the inspiration for Apache drill so it to put a finer point on the important aspects of Dremel that were inspiring what was the initial spec for drill like what were the initial goals of the drill project that were taken away from Dremel or inspired by Drummond so gentle did a few things that were very very interesting very valuable like to introduce kind of an architecture for running a very large single craze on when I want to see large Cory's I mean large amounts of data large cluster of commodity servers and return fast results so that was kind of the biggest Innovation that that that Dremel provided as well
 they're kind of calling or format that was suitable for at complex data as well and so those were kind of the things that we looked at Dremel and we said what we need to bring this to the the open source Community to the to the broader industry as well so that other companies and organizations ranging from small Tech startups to the largest Enterprises and it would be able to have the same benefits that Google has internal system now that also meant kind of understanding that Google had your internally they they could take some kind of relaxing assumptions right they it was very different from Enterprise that had all sorts of Legacy systems and was bringing data from many different than a heterogeneous sources Razz within Google all data was basically being generated by their own internal system it was very homogeneous in NHRA it was all a specific format called that protocol-buffers whereas other companies the other day that wasn't all sorts of formats and all sorts of systems and so required
 a lot more flexibility right and so we and Barks On kind of a journey to build a system that will take the best of Dremel but also provide a much greater level of flexibility that we spent about 3 years at building that until in the middle 2015 we finally had a release that people can start using
 can you talk more about the historical context the anecdotal context of how the ideas from Dremel started to percolate into a project that will become drill like what is the story what were the individuals and companies that were involved at the time I was VP of product map are which is one of the large I do and we were thinking about okay how do we make a Duke more broadly applicable to 12 larger audience and because of end of the day when you were going to companies that didn't have large engineering teams nothing about kind of regional Banks and and those types of kind of meat market companies they really struggled right they had to you had this massive massively parallel very skilled with platform very powerful platform but at the end of the day you need users that are in companies that don't have a ton of resources to be able to take advantage
 pattonsville to analyze data and so when we had seen kind of what Google done with that with dremel and at the time we had we outside of very close to partnership with with Google and we're having discussions with them in kind of made sense to look at how can we take that kind of technology and making more broadly applicable to General Industry
 soap for listeners who still may not understand what drill does one way to frame this is that drill offers zero-day analytics can you define that term what is 0 day analytics sure if you think about what a company has to do today or an organ foreign or an individual or any organization basically has to do today when they want to analyze data in coming from various places they first have to find a way to get all that data into a centralized place right and then they have to go in Define schemas they have to have somebody has to go in to find the structure of that data and all that has to be done before you can actually carry it and so does process how many people refer to it as ETL and that process takes time it takes time because moving all that data takes takes time especially when it's big and also takes time because you need
 have humans involved you need somebody to go and say okay these files that have just arrived from has some internal application or some external data source and this is the structure these are the fields The Columns in them these are the data types and and so what that means is that it can take anywhere from one day to a month and some organizations had to make new data available freezers to Corey that and a lot of times it's not just about a new dataset it's about changes to the structure of data and see what happens in a lot of applications these days is that what you had developers built application WhatsApp web app for a mobile app and add the developer will just go and add some new Fields yeah specially databases like longer to be where it's very easy to do that though just add some new fields and I'll tell the analyst or the product manager sitting next to them I had these Newfield you can you can create a stone that new field so I probably manager goes in there he's like well I'm trying but that's actually not showing up
 and so they have to go to ITN FiOS support ticket and it takes Note 3 weeks and finally they can Corey that new field right and so Julie Ames to solve that problem by saying that basically allowing a user to Corey any data without having to kind of predefined scheme ISO Jewel automatic understand understands what's in the data automatically handle situations where the data structure is changing and evolving overtime as you can immediately query data base the real-time data right inside swimming by zero the analytics you can query data that 0 days old
 right and as you said it it unifies the data format so Jada in the world as you said until we were talking it exist mostly in four different schemas what are those different categories
 so when I mean there's lots of different definitions of schema but when you think about the data that a company has that even little simple case y'all there are lots of files there lots of files that are structured as kind of delimited or text the limited things like log files or CSV files and then you have a lot of Json files that are very popular data source for applications exchanging data as well as Kyle logging in persisting of data so it's another type of format and then yeah relational tables relational databases people still have a ton of data and systems like my sequel on Oracle and sequel server and so that Tina has yet another kind of structure and so drill supports all these structures and kind of crazy unified you that allows you to that korede. No matter what structure it's in and then also being able to join data across these different sources even if they have different structures so I could join a mongodb table with an HP stable and my log files in Hadoop and my customer table in Oracle
 Engineers love Automation and will front automate your investing as a software engineer at there are certain processes do you want to execute no matter what like integration tests during a bill you would execute integration tests manually you would use a continuous integration tool like code ship or Jenkins to automate your integration tests well front is a tool to automate investing just like a continuous integration tool runs your test automatically reinvest your dividends automatically and performance tax-loss harvesting automatically to get your first $15,000 managed by wealthfront for free go to wealthfront.com se daily and get started with wealthfront Slayer of automation on top of your portfolio wealthfront.com SE daily check it out it would support software engineering daily and you will get $15,000 in managed for free if you sign up
 automate your investing get back to the things that you can't on me like writing code
 the relational model of Aquarius engine is it's useful for data that's in CSV or a TSB format but it's typically not as useful for a schema freeform at like Jason or hbase why not what is what are the problems with with a jit with a Json format that makes it harder for Aquarian Ginger I'm in the right there I should a few different problems there so the first TV or just the file full of Json records is that any record could have a different structure to it in practice what you see is that it's not that really every recognize different structure but the records to the structure evolves over time right so just as I mentioned earlier you know the developer has a new field or decides that instead of name there's going to be a first name and last name I ain't worried that the and the time is no longer a string it should actually be a timestamp day that I pray so all these changes are happening rapidly these days as developers are kind of building in update
 applications and so that's the first thing that doesn't kind of fit the relational model II it doesn't fit the relational model is that these structures like Jason support things like nested Fields so you may have say an address field that's not just a piece of text but actually has several Fields nested underneath it like the street the city the zip the state the country right so has that kind of nesting which the relational model doesn't support and then it also has a raise if you look at various Json data sets a lot of times you'll find that there is an array so if it's a data set of restaurants still have that say they the categories of the restaurant so each restaurant having a rate of categories and that's also something that you can't really handle with the relational model 3 Alicia model doesn't really support at rays
 you describe Jason as essentially a superset of all these other format if I'm quoting you correctly can you explain that reasoning sure when you think about all the the kind of different data models that are out there whether it's the relational model which is what you see and say oh my sequel table or a CSV are tsv file or you look at things like Avril or or protocol-buffers that's another type of format that has a schema but supports nested data and then look at things like hbase for example where you don't really have any nesting are raised but you do have you do have basic kind of a sparse sparse table where you don't really like any any records have different fields and so all of these different kinds of structures are basically special cases of the Json structure right so to take an example
 what's a yacht A relational table and so that means that every record in that table has the exact same set of column server Fields right and so you can represent that very easily as a file of a collection of Jason and documents where each of those Jason documents happens to have the exact same Fields right so that's an example we're okay it's very easy to represent that as Jason and all of these other structures and can be at conceptually are logically represented as Jason with various kind of restrictions price of cheese has the most flexible format and what we realized with when we were kind of looking at how to design drill or how to how to build a system that would be very flexible was that if we could build a system that at where Jason was kind of the the t-factor standard data model then we would be able to carry any data in all of these other more restricted or less flexible for us
 and drill allows for sequel on everything which as we've discussed is very useful for this business analyst is typical typical role and seek want everything means sequel across all of these formats what are the high-level challenges that you encountered while you were designing drill while you were thinking about girl just from a high-level what were the challenges that that you were faced with yeah it's a yes Jewels design for like you say sequel on cygwin everything yeah we still have that support add kind of the ability to connect two different data sources right so for example yeah we're working on a connection to say Cassandra right that's not a data source we don't have a connection to that but the engine of drill is designed with that flexibility in mind so adding these sources is possible right and it's also fairly fairly easy and the same goes for a different file format
 gray sew and young Joe has drills very extensible you can add what's called a format plug-in that allows it to Paris any type of new type of file it also has orange bugs that allowed to talk to any kinds of databases and really understand also the internal capabilities of those engines so one of the nice things about drills it will actually push down as much Crossing as possible into the underlying source so if we bring date on S3 or on hdfs then we can we actually drill does all of the execution and because the underlying sister has no cry capabilities at all when Jill is courting mongodb for example it recognizes that mongodb is able to do filters internally and things like that so we can push down as much processing as possible into the engine and same goes for a relational database so if for example you're running a query that joins a mongodb collection with a twerkle table maybe a few Oracle tables at Jewel actually
 do preliminary join inside Oracle of all the deal with an Oracle and push down all the projections and filters into these systems and then only pull out the date of it actually has absolutely has to pull out in order to do the joint across the two systems and so one of the one of the challenges here is understanding the internal capabilities of each of these systems being able to push down as much processing as possible the other challenge of courses the flexibility and in drills engine to even be able to act Corey day. It doesn't have doesn't have a strong kind of rigid schema right so every other sequel engine in the world that's ever been developed and makes an assumption that the scheme is known up front before the Quarry actually runs right so if you think about a relational database or biasi Quan Hadoop technology they all have a schema that's well Define
 known up front it doesn't change for that entire data and and that makes it a lot easier because the engine can then take that schema and compile the Quarry into kind of what's call the Quarry execution plan and then run with drill because the structure that did it isn't known in advance right for example if you have a directory of Jason that formatted log files we actually won't know the structure of the day. Until we actually seen the data right instructor could change it could be that the last record in the last file in that directory has a complete different structure compared to everything else so the engine has to be much more flexible in terms of being able to adapt kind of the execution plan based on the data that scene and that's a much much harder thing to do
 show I think we're getting at here is that one of the big advantages of drill seems to be the modularity it's it's a layer of abstraction that sits over Mongo or I do or whatever your data source is and the access Logic for the end-user is going to stay the same even if you change the underlying storage in this is really useful in a super fast moving Hadoop ecosystem
 absolutely no drill gives people eye unified waiter to unify standard way to explore and analyze data regardless of what system it's in from a drill user's standpoint mongeau collection looks just like a directory of log files and I do but they can be Korean exact same way as also makes it very easy to join these dating sites are those under my systems are very very different obviously drill makes it very easy to work with all them because it's basically the same thing you don't feel learn a different language free system that you haven't in the company
 what kinds of different data sources can drill connect to right now
 Central support it support hdfs it supports Amazon S3 it supports Azure blob storage and basic any distributed file system that's out there are over to Landon's to to find something that's out there it supports some no sequel databases like manga D be like hbase for example it supports relational databases Oracle sequel server by Sequel postgres and people have used it with others like matiza in terms of broad range of data sources that are supported
 can you talk in more detail about how you write a drill connector like whatever whichever whatever would make a good example I don't know I like how did you write that connector for S3 or the connector for whatever whatever other source might make for a good example connectors for kind of databases are already in that will basically work with any distributed file system as well as a non distributed file system so if you're adding support connector if you want to build a connector for say I know sequel database right you basically builds at your building a Java jar file at the end of the day that's that's kind of what your building and you have to implement some interfaces and that are defined by drill and so at the most basic level your defining an operator that knows how to read their pull data from the underlying from that system so given
 given the name of a table and there's kind of a weighted to find the specified names and drill but given a name of a table being able to return an a-11 record after the other that's kind of the most basic level the more interesting kind of work here is really understanding what that database supports in terms of processing capabilities and then being able to push down the processing as much as possible and the storage plugin has the the flexibility or the power to actually override any Optimizer rules and so if you're building a storage plug-in it would look at The Logical execution plan and say okay I see that for example at this this query includes a joint of include to join which has maybe three tables coming from a mongodb example let's say yes and that data source knows that there that storage plug in
 knows that my sequel supports joints internally so we can actually ask Rick actually take that and transform the plan so that the partial join happens within the my sequel as opposed to all the date of being pulled out into drill first and so a storage plug it has a lot of flexibility to rewrite the and the plan
 okay so Wikipedia says drill supports data locality which means it is a good idea to co-locate drill and the data store on the same node can you describe that quote in more detail sure what that what that means is that
 if you run two drills in distributed system right though the way you run drill as there is basic a demon process we called the drill bit you run that on one or more servers and so if your mean use cases you're clearing data and Hadoop or your current date and Mom would you be if you were to run that process the that drill bit on each of those servers where the date of horses running so I'd say if you were running onto Duke running a drill bit on every datanode and if you're running on mongodb running a drill bit on every on every month ago and node in the cluster then when will actually compiles the query and generates an execution plan a physical plan it will optimize that plan for minimizing the amount of data right over the network so you think about the underlying storage system same on Gore hdfs different pieces of data reside on different nodes in the cluster and so by understanding where each piece of data is in the car
 fuel can be more efficient about how it's reading data and so basically each node in each process in the jewel cluster will read data that's on the note on which that's running so we we end up moving a lot less data over the network that's not always possible though right if we recording data from S3 there's no way for the drill processes to run on S3 because S3 is a service it's a black box from now that that Amazon just hosting and that's fine as well in fact I think as networks are getting faster and faster in a lot of companies now you see 10 you use a standard and 40 gigs coming up there in many organizations actually predict that did it will become less important as time goes on
 yeah I did a show with somebody from Netflix and Netflix is entire Data Warehouse in S3 so maybe that is that is that unusual or is that is that is that what you're saying that's the direction we're moving in
 well I mean that the question about cloud is a is an interesting question I was actually I had the opportunity to present 250 ctl's on Wall Street recently and we kind of did a show of hands asking who has more than 5% of their date on the cloud in there none of the hands went up and I'm so I think clouds in there that can really take advantage of the car right it's very hard if you're an on-premise Company If You Can a grown up that way to just wholesale move everything to the cloud because it's kind of the poster child I would say half of an AWS in and using the cloud and they've been very successful doing it but it's very hard for me to move everything to the cloud I think regardless of whether it's caught her on premise and networks are getting faster and I think that's a big Advantage for a kind of it changes some of the Dynamics around distributed systems in
 characteristics of them
 so this is kind of off topic but I mean when I don't know how much you can talk about this but like when these companies say we don't like them they can't go to cloud or they haven't gone to Cloud yet or whatever and they are on-prem are they typically doing this because it's like is actually too hard to move to S3 or is it more like their security concerns
 I think it's a combination of both right now about it logically I will argue that companies like Amazon could probably do a better job securing a Datacenter than anyone else right I mean they have that's that's what they do for a business to be very good at it right I think still think there's some perception around that though that's that company struggle with buy things also just an issue of your legacy to go to an Enterprise today there are there is so much there are so many systems at a better on-premise yes so many applications you one of the companies that were working with has over 10,000 Oracle databases right it's not something that you can just say oh I'm going to throw those away and move everything to you know you see to write and then lot of existing story systems in systems they build overtime and employees of changing moved on and things like that it's just really complex environments right if you are I mean if your a new startup
 you know as soon as we are here at a Dremel then absolutely everything we do is in the cloud I mean we have no way we don't run any infrastructure on framus and it's it's great right any developer going to spend a bunch of servers without using for as much as we want spend them down the most companies aren't don't have that kind of privilege what are you doing
 I can't sell out yet about what we're doing a Dremel we're at we're still in stealth we where he started back in June we're backed by two of the best ski season in Silicon Valley at light speed as well as red point and an Argo really is to make it so that it's easier for people to discover explore and analyze data dramatically easier than anything that's existed before so we'll leave it at that for now
 so I can shows with people from confluent and Ro Khanna and Cloudera and there's there's an explosion of companies that are offering big data services and support that spring out of Open Source projects what is what is the future of the Seacoast Army of business and open source what are the challenges and what do you see looking forward at the cross-section of business and open source
 yeah I think the way in which software and services are adopted as change dramatically over the last 10 years are they used to be that on the first the first time you would learn as a potential customer the first time you learn about something would be when the sales rep called you and you took you out to dinner right and the verses today the way people would want to adopt software and applications is they want to download it or they want to try it out for free and and they'll make a commitment once they see that there's value there for them so they need to be able to realize some value before they actually go and spend a bunch of time talking to it to a sales guy and things like that and I think that's what you're seeing in the context of kind of Open Source as well as premium on those that are out there right and that's why most companies today are y'all doing one of those taking one of those approaches that allows people to see value in and what they're doing before kind of making a big time commitment
 drill what are the biggest challenges of drill that have yet to be overcome
 yeah I think it's it's what you'll see and drill over the next year is a lot of new functionality around new data sources even better performance greater ease of use at the end of the drills and very new project although it's been kind of interesting things about how the open-source World works right or her some open source projects and we started drill and we announce that we were working on drill back in 2012 were there wasn't a line of code written at the time right so it was the work is done in the open right you don't have to sit down and kind of in a stealth mode and kind of environment don't build something for 3 years and then tell the world what you're doing because it's the source code is going to be out there and open source anyway right so why not why not do the developing in the open and people in the meantime could learn what
 says and contribute nelfor example the mongodb support for Jill was contributed by two engineers at into it and so was developed it took about 3 years to develop this and it was released in June of 2015 in rice it's only been about Fiverr I get 6 months now that drills been available at an end usable by and by people and so that's kind of very early if you think about tide the Devolution of a product I think we're seen tremendous girls right now and turns it like and usage downloads if you look at the mailing list you see many questions every day now from different users that are that are using it and trying it out and running into production and so I think the growth is going to continue to explode over the next year and will see a lot of new features built by kind of cord drill developers as well as companies that are trying to use drill and the lad support for their own data formats and their own systems that they have in and how to solve the problem
 they have and so we're definitely seen that kind of growing ecosystem around the project
 what about the ecosystem or brother I like what is changing in the ecosystem that excites you and what would you like to see change more
 why I think
 so super slide separate kind of the the Big Data ecosystem which I think is broader than I do because system right I think what you're seeing with Hadoop is basically a lot of new innovation at all layers of the stack right and see you see a lot of people doing are starting to kind of explain what may have more streaming applications where they does coming in and they're doing something with that data as it's coming in in addition to kind of doing the the standard kind of exploration and analytics of the data as I think streaming is kind of an important use case I think in general you're seeing more and more adoption of of this stack right at the end of the day even though we've been talking about to do Ben and big date of her for a while now most companies are kind of still in the early in the early early days of adopting that technology right the other been some examples of companies that have really taken advantage of it you know some of the large
 retailers largest credit card companies but for the most part they're still on way to go in terms of people really realizing full advantage and so between drill and Spark the others there's a lot of kind of innovation out to eat execution in in the ecosystem I think you're seeing continue navigation at the lower level so in terms of kind of storing of data and then so be on map are released 10 about converge data platform that includes that kind of file system nosql database in streams in one platform at Kiley are released a very interesting at corner store called kudu which Bay City stores at Daytona, format and supports random kind of updates which wasn't possible before using just a file system in calmer files in file system so lot of innovation at the end of stories later as well and I think outside
 that you see things like manga to be in Cassandra in elasticsearch also having lots of new features and so at the end of the day I always joke that if it's a great world to be a developer in right if your developer today you have so many tools and so many options for the most part they're all free they're open source they're very scalable they're very easy to build applications with you think about like Trill Spark mongodb elasticsearch all it's just it's just easy it's way easier than it used to be right on cue do in case listeners are more curious about that project the clothes off to get some historical context there are a lot of long-time database scientist who are involved in drug is like people from Oracle and vertical and so on and I'm curious what it's like working with these people do you feel like databases and data access domains is this a field
 what is changing a fundamental level or is it a case where history is repeating itself and the more things change the more they stay the same
 yeah I think
 I think the best Innovations are the ones that both learn and take advantage of all the existing knowledge that's that's out there in the previous Reacher research computer systems and then build on that existing knowledge and innovate right and it doesn't make sense in my mind to throw away everything everything we've we've learned in the past 10 or 15 years right and then we'll run into the same problems obviously right will do things differently but we're going to the same problems and that's that's been one of the most important things for us and drill and also if you look at the end of the composition of the people working on it and you see people that are y'all very very experienced database engineer as a lot of experience that companies like Oracle and so forth but then you also see people that are kind of moron that distributed systems and kind of fat developer Focus that kind of mindset and so you bring those to audiences together you get to take advantage of all the existing kind of domain
 knowledge has been accumulated in things like optimizers and it's equal and execution technology but you also been incorporate that kind of the flexibility that's required for the flexibility and Agility that are required for Yale 2015
 what's Homer thanks for coming onto software engineering daily it's been awesome talking to you about Apache drill and the Big Data ecosystem I was about to say I do because somebody won't say that anymore so thanks again it's been a pleasure thanks for thanks for having me
