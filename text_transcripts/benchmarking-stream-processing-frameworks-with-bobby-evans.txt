Transcription: on software engineering daily we do our best to keep up with the evolving Hadoop stack stream processing is probably the component of Hadoop with the most variety and assessing the differences between projects like Flink storm and Spark streaming is difficult without an agreed-upon set of metrics to compare them Yahoo's engineering team has created a set of benchmarks to do exactly this on today's episode We compare streaming Frame Works with Yahoo's Bobby Evans but first we're going to take a moment to hear from a sponsor who makes software engineering daily possible
you don't have to settle for a job that makes you unhappy and you don't have to go through the pain of searching through millions of job listings hired.com brings job offers to you the engineer fired is a job market place built for the engineers a few years ago I was unhappy with my engineering job I felt stuck I felt underpaid I felt alone and I didn't know my value in the job market place it's a very opaque market so I used hired.com to find a new position hired connected me with a talent Advocate who guided me through the process like a free concierge it made me feel valued as like a white glove service and this Talent Advocate help me with all the questions I had should I ask for more money should I consider relocating to try to get a higher salary or a better job Attalla Advocate do you get paired with on hire.com helps you with all these kinds of questions
I got several job offers and I found one that was a good fit in fact Engineers who use hired.com get an average of 5 offers from great companies like Facebook Uber and stripe in the companies are bidding on you to try to get the best engineers and so you end up with the higher salary go to hire.com se daily for a $4,000 bonus upon signing up it is completely free to Engineers looking for a job I've interviewed the founder of hired Matt miscovich he also started 99designs and sitepoint he's a software engineer and he understands that Engineers Deserve The Leverage in this super competitive market where great Engineers are at a premium check out hire.com SE daily I'm happy to Advocate higher to my listeners because it puts power in the hands of Engineers now let's get on with the show
stream processing engines have grown in number in recent years and it can be hard to keep up with which Frameworks should be used in which streaming use cases Bobby Evans is an engineer at Yahoo working on streaming Frame Works mostly on Apache storm Bobby welcome to software engineering daily
 thank you let's start from a high-level what is the purpose of a streaming computation engine
 really revolves around having massive amounts of date
 with small amounts of data you get to answer questions about that data fairly quickly you either ask lots of questions and can dig into things or you want to be able to respond to the data very very quickly and that's what streaming really is about is how fast or when the data is first produced to the point that I can respond to it and trying to reduce that latency a lot of use cases are around human interactions and so those late fees are usually on the order of seconds spark and some of the other microbatch systems tend to be able to go on the order of seconds which is fine for human use cases but there's another class abuse cases where you really want much much lower latency in this is usually things that are consumed by a machine so you have a feedback loop and storm and slinked really
 are true streaming engines that that they can respond sub-second and and be able to really reduce that that that latency and I think one thing that is probably worth pointing out is that we're talking about streaming or usually talking well at least as I understand we're talking about streaming across different machines were just talking about your personal machine most of the operations within a single machine would be considered streaming operations I think but in a streaming copy tation engine is generally handling events across a distributed system would you say it's accurate yeah
 you can handle streaming with a single machine but that that's an easy use case that that's not that exciting it just win the data is so big or is coming at you so quickly a single machine just can't handle it on a single machine that's where it should be don't bother with any of these things it's going to waste your time and be slower so we've had batch processing systems in the dupe distributed system space for much longer than we've had effective streaming computation and I want to quickly outline the difference between these two types of systems and the gradient that exists between the batch and the streaming area and make sure I have the definitions correct so in a batch processing system which is how these early Hadoop distributed systems were built you often have data that gets accumulated in some centralized repair
 story like hdfs Hadoop distributed file system and maybe you know once every day like every night you have a job that process is all the events that have run throughout the day but the Desire with these streaming systems is you want to actually get to a situation where instead of just processing all the events on a 24 hour interval or a 12 hour interval whatever I do batch interval you have you instead get into a situation where you have the streaming engine that's always running it's always ready to process a new event and update the overall system to ingest that new events such that the system is aware of that event on a much lower latency time scale would you say that's an explanation
 how fast from when the date arrives do I need to be able to respond to it yahoo lots of other companies have built streaming systems based off of batch the date of aggregated and I know for Yahoo at least we were doing 5-minute file aggregations and so every five minutes a new job would come off it would process that data then there would be other Roll-Ups that were like 15 minutes hourly daily weekly monthly and it's how much data are you willing to batch before the overhead of running that had boob job or the spark job or whatever else becomes too much that is not worth it anymore got it with streaming sites like you said
 we process an event at a time we never take that concept of there's a batch I'm going to process all of this data at once it's every single event shows up and is processed independently got it so you are part of a blog post called benchmarking streaming copy Tatian engines at Yahoo what was the motivation for writing that blog post sure so I want to start off by saying benchmarks are all crap there's some benchmarks that are better than others
 I'm an architect at Yahoo I own several different projects Kafka storm and Spark are the ones that I'm done primarily responsible for and provide them as a service to other teams within Yahoo be able to use so looking at storm we started doing storm several years ago before most of the other streaming engines has really taken off and it's been years we wanted to see how well Storm performed in real world use cases compared to other systems so we went out and we looked at a number of benchmarks that other people had done and like I said all benchmarks are crap but a lot of the other ones had even bigger more serious flaws than the one we have
 never do any kind of micro benchmarks so things like how fast can I process data and I can read the data and do nothing with it how fast can I do a distributed graph where I read in the data I look for a specific substring and throw everything else away these are okay pieces the Builder can be used to build a holistic system looking at them independently especially in streaming is very very hard because you really care about the throughput versus the latency that I have a given group but that I have to be able to handle how many machines is it that I'm going to need how much money do I have to pay to Amazon or whoever else to be able to handle that throughput is going to meet my needs right most comparisons of streaming computation engines they don't they don't take the time or they don't
 get this test of a holistic test against the entire system in use case they don't test against the real world use case and you kind of explain that they yeah maybe take these this piecemeal approach that maybe doesn't exactly capture the whole list decision that you need to do you need to understand the the system we we did a show with Satish Mehta who is from a company in in India called in Mobi which is an advertising company and he talked about that show is actually about evaluating different streaming computation engines but I don't think it was from the standpoint of here's a benchmark we designed and we tested it against the different we tested the different systems implementing these different benchmarks it was more like a theoretical exploration research exploration why is the approach of
 benchmarking of experimentation preferable or what are the trade-offs between that approach and the research-based approach it's because I'm not smart enough to take everything into account that might happen in the real world the real world is a messy place and so
 for us when we see things running in production because we we we do this a lot lately seen on things happen like a top of racks which will get saturated because we scheduled things badly or AAA two different processes running together that they will interfere with each other in really strange way is not necessarily through the CPU but possibly through the cash or other things and you want to to be able to pin these processes to a given CP or whatever to get off Moe's performance and taking these things into account is very very hard to do theoretically very high level you can look at the different approaches theoretically and say in general this approach is going to have lower latency vs. Approach which is going to have better through flu or whatever else a real world use case
 but you know how is going to perform for white for that use case what you really want to do and that's why benchmarks like I said at the beginning really suck is that they are your use case they are doing what you actually want to do and until you try it out there still that uncertainty you just don't know and so having a real world Benchmark gets us a step closer insane this is something that looks more like reality but it still isn't the same thing as taking your code running it on the system and seeing how it performs
 this comes back to the distributed systems enumeration that I wanted to give it the beginning because in a distributed system you know the philosophy registered system is all about the Byzantine failures where you just have these unexpected things that happened that you almost like you admit a front we can't predict all the all the different failure cases and I like the idea of experimentation within a distributed system because it's it's say it's almost like admitting we need to run these tests in order to see that I see the unknown unknowns talking about the actual benchmarking that that you you you ran on these different systems I want to talk about it at a little lower level of granularity what are the commonalities between these two
 I've got we got storm spark-streaming Sam's flank all these other streaming platforms are so many of them what are the set of requirements that every stream processing framework fulfills to some degree is your definition of stream processing and streaming some of these systems by the definition of an event at a time are not actual stream processing systems spark for example Sports training is in micro batch system that they take that original concept of doing a batch of data and they can sit down as small as they possibly can and so
 on the order of a couple II size batches and so you get a second or two seconds or 5 or 10 seconds of patches and they didn't have a very nice layer on top of it that looks like a streaming system layer where they say here are the processes I want to do that look kind of event by event and make sure they're idempotent and process the date of this way but the reality is is is still flowing through as a batch and there are controls at a higher level around all those patches where is you compare that to fling or or storm in those cases it truly is event by event and event comes in its up to either the operating system or the
 the framework itself to decide who's going to process that event give its CPU resources to be able to process that data send it off to some some other thread or some other piece supposed to be able to finish processing it and hand it off in that way and so
 Siri is there really isn't that much of a difference between the two but for the most part at a conceptual level streaming is just
 providing an API to users that says the set of data that your processing is not a single file is not a small piece of data this is an unbounded set of data and what are we batch behind the scenes for you to be able to give you better throughput or not is really the realization that data doesn't end that people looking at your webpage interacting with your app on the phone whatever it is that flow of data never stops it continues constantly and providing a framework the abstracts that away from the user
 right so that describes the similarities and before we can before you go into benchmarks now it's like to talk a little bit about the differences so you touched on this micro batch idea that that's Park Works within and will get into more about the consequences of this but you know what it was what are some other trade-offs where some other different taxis are dimensions that these Frameworks are trading off between that will help us understand why there are so many of them
 use cases and so on
 people have different desires on how they do processing and what they're trying to process in so if you distributed systems are hard by definition did bad things happen to have to deal with failures how you deal with failure easiest way is to just ignore them and say I bought some data whatever who cares and so that
 but then there's some use cases that that's okay with that you're doing kind of just a rough guess as to what it is you're doing some statistics around these numbers putting actual money on the line necessarily or being able to cost more in the compute power than it's worth to get exactly once processing and so you can have something where you say if I lose data I don't care there other ones where you say I want to make sure I process all my data I double count things that's okay but I I want to be absolutely positive I processed everything and then the final one is I want to be like I want to know that I processed every single event once and only once throughout the entire system and so a lot of the difference around these different systems are in how they handle those different situations I processed it
 best effort processing that's easy there really is no difference between the system but they're different ways in how they handled the coordination of exactly once processing or getting closer to exactly once that that at least once processing okay and so will spark and is batch processing is already set up for exactly once tracks everything at the batch level and make sure that the batch was fully processed and committed to wherever the output is following the same Hadoop output formats and that makes spark very easy because it is exactly once processing system they don't have to redo anything around that but then you have other things I can Patch Apex and Link messages and they have periodic checkpoints so every single processing step
 bad that you can send your data to that this one taken event and say filter on that event or or augmented with something else or whatever if that has stayed in it they will check point that state periodically along with information about how many events are what which events have been fully processed and then do a handoff scheme where they they talk to each other know the different pieces communicate back and forth with each other until their positive all of the data has been processed either at least once or it is bad things happen then they can roll back the state and try again that's one way that they deal with it exactly once a separate approach where it is kind of a hybrid between the two
 okay great well so we'll explore those differences a little more I want to get into The Benchmark discussion so we can get a prototypical use case to Circle our discussion around silk there's a prototypical example application that you decided to use for this Benchmark which is this real world example of the advertising application in this is like a really common used case it's it's really important you have several advertising campaigns you have a number of ads for each campaign You've Got High throughput explain why this is a good sample use case for a benchmark primarily because of something that we're familiar with we have lots of people that are our customers at Yahoo the do this exact same thing that they will process data
 this is a very simplified form of it but it's essentially counting how many times in a something was singing Counting who saw this particular add are they interacted with the bad things like that so that you can charge it it better to make sure that this type of acid is interesting to this type of individual so that we can do better targeting towards joint ads that people care about or B I use case that I really liked was just simply counting how many times the ad was shown so that we don't overspend is an Advertiser they come in they say I want to spend $100 on showing the sad well I want to make sure as a as a publisher that I show exactly $100 work if I go over I use and I can charge them for that if I go under I can't charge them for what I haven't shown and so having a feedback loop especially when you get close to the end of that I can
 really becomes important to make sure you can hit the nail right on the head and that's fairly simple amount of math but it's a doing it at scale a very very difficult yeah and 4 if people think yeah maybe the US advertising that's unscrupulous it's certainly something that is analogous to all these other kinds of streaming applications you could have whether it's like a Hadron Collider or you know reading data and space or all these you know if you're reading real-time data in a car or something all these things have very similar situations where the data pipeline is gigantic but the mathematics behind it are somewhat simple and you just have the set of operations need to do so that said there is the set of operations that you said we're basically the The Benchmark you had you when you wanted to do these the set of different things and test it on each framework
 with some given amount of throughput so explain the operations that each framework was going to have to provide and and what what throughput level like give give me an idea of the of the parameters that you were going to use for each Benchmark on a streaming framework
 I want to start out by saying that our numbers and everything we're very preliminary we wanted to get them out but it's just not completely done at this point what we started out by doing was simply saying that that we wanted this basic use case we want to expand the use cases in the future because there are lots of other use cases like the Large Hadron Collider we talk to people about that that that's a very mathematical computational intensive system and so they are there trying to figure out what day did I throw away and the discs can't keep up with the amount of data they produce three different use cases so don't take the advertising use casein assume is going to apply to or anything like that bass case it is the bass case the base case for the advertising is that we take an event that came directly from say an ad server
 oftentimes this is very compressed data that they don't include very much with it because you don't want to have this extra data in the URL or whatever else it is mostly just an ID that is identifying this particular add was shown to this particular user you get those two pieces campaigner or anything you need to augment that Jada and I date is usually stored in an external database and so the first thing you do is you'll take that ad ID and that that event you'll end up with extra data you look up things about things associated with that and try and find all the pieces of information that are relevant about those two pieces
 well sorry I forgot the first at the first step is filter out the event so we don't care about in this particular use case we had extra events of things are likely coming in from the ad server but at least for this use case we don't care about once they've been filtered will go in and will augment the data to be able to have the pieces of information we care about will throw away a few pieces of information that were part of that that begin W don't care about and send them on to do some aggregations are actually very simple where we're simply bucket sizing the data and aggregating channels over a specific range of time trying to see how many times does ad was shown in this situation
 right answer to clarify you implemented this Benchmark within each system so you so in an the different systems you are evaluating worth blank spark streaming and storm if if I'm correct it was just those three that Christ set up two pieces one piece there's a Costco stop streaming state store in a streaming way so it's kind of similar to hdfs but it's different because it's not in files it's in a is kind of like you're telling the end of a file as things get written into it and so we pick one because that seems to be where most everybody is coalescing there are a few other ones there that people use but a lot of people seem to really fall around Kafka
 and then we set up a Renaissance times just because it's very fast in performance be able to act as the database we could have set up a spacer or some other nosql store it was just we wanted something to be able to be there in some code to be able to publish data to Kafka for these to be processed to populate that database and then we also have a piece of code to check the correctness of the result when everything's done and that is independent of whatever streaming System is using this the goal here was to be able to have an external piece that the streaming system is a black box so that we can be positive that is doing the right thing and then also get high performance numbers out no matter what the system is some of these systems have much better monitoring than other systems to and so we wanted to be able to to collect the metrics independent whatever that system was okay
 storm think we've had a few pull requests for other systems since then but yes they're those were the three we picked to start out with
 I love being a software engineer because of how easy it is to build side projects and I built countless side projects I build a Social Gambling application a dating website backend that has zero users stock trading music rhythm application for Android and I think every developer should have side projects so when I have a web app that I want to deploy and share I want to show my friends my most recent project that nobody will care about I use digitalocean to quickly spin up a server and host my projects and who knows if one day one of my apps takes off and goes viral and people actually do care I can easily scale using digital oceans flexible pricing plan companies like taskrabbit that have grown rapidly have used digitalocean for this very reason but until this happens I will happily stick with the $5 a month plan sign up with promo code SE daily to get a free $10 credit is a listener of our show and start
 singer apps we would love to see what listeners build so send us an email showing us your project I've also interviewed moisey oretsky who is the co-founder of digitalocean and he mentioned that the ease of use and the flexibility is why they built the service in the first place that digitalocean interview is one of my favorite episodes noisy talks about bleeding and a Datacenter he says you don't know how hard it is to do a cloud hosting service I'm until you bleed in a Datacenter it's really interesting so it's speaking which let's get on with this episode of software engineering daily
 so what were the bottlenecks that you expected in this pipeline of operations what it what did you expect to be the most time-consuming or processor-intensive computational long this Benchmark pipeline leak computational intensive that's the thing it's very very simple computation compare and just throwing some pieces out there's a quick database look up there is some aggregations Century summation the results back out to a database operations and part of the goal with this was to see how much of the overhead I mean this is fairly common that most of the computation we do is very simple computations and so the overhead of the system
 things to dominate in a lot of these use cases but I wanted to see in a real world use case that wasn't word count it was similar to work count how much did the system actually dominate there was one of the things I was really curious on
 so correct me if I'm wrong so I understand what you just said the the actual operations of the what the framers had to do not very complicated the bottlenecks that you expected to emerge were at the where they at the points of interest graichen like the point of integration between the streaming framework and Kafka or the string trimmer can the database or on the same box but we also separate boxes and
 so do you stay so how fast can I read data and throw it away is actually a very valid use case because it's measuring what that overhead is that was one of the first benchmarks we wrote when we were value waiting storm was the speed of light test how fast can I do nothing out of measuring overhead of the system itself
 you are trying to do things and then you're doing everything on a very small amounts of quantitation can add up very quickly and so I wanted to see how much that overhead was and how that compared between these different systems
 I'm still not clear because I thought that the distributed elements like the coordination between different processes across different boxes I thought that was kind of like standardized on zookeeper and zookeepers takes care of all that and you don't even need a bad and I so I guess I'm totally mistaken about that
 spark streaming is doing microbatch behind the scenes and by far the bench there the bottleneck and Spark streaming for us was the the overhead of processing about the scheduler better than the computation involved with in the scheduler just became more and more and more to the point that actually refreshing to you I slow down the processing because the patches were just so absolutely tiny to try and get some second lane since we just do it increase the batch size to the point that it could keep up and it was doing things well was much much much higher than the other systems storm I just finished doing a bunch of a benchmarking and performance improvements to try and improve the throughput there and previously the vast majority of the overhead was actually
 contact switching because we're processing everything event by event in a thread would wake up it would process that event it would be done with it and it off to another red and the amount of context switching the overhead in the OS to be able to switch back and forth between making a thread sleep and making another one wake up was the thing that dominated by far that I was able to more than double the through foot and still reduce the latency by playing around with patching and other things to be able to to stop that extra overhead within the operating system very similar things we also with the network that's why on HPC systems are the supercomputers they actually bypass the operating system and even the CPU in many cases to be able to do communication that's what RDMA is all about is remote direct memory access and so the the network card is tied directly into the memory it can do
 from another box on another piece of the network and shove that data directly over to where it needs to be without the OS being involved at all in the OSS doing all kinds of things that are important to keep us safe from viruses and other things that want to just deal that date or behave badly but it also slows things down drastically OKC you said a lot of interesting stuff there and I want to delve into one specific thing so you know kind of getting back to a a basic conversation of this Benchmark and the comparison between different framers do you mentioned a comparison between spark and storm there and I'd like to kind of roll back to a little simpler level of the conversation
 how did the implementations of this advertising Benchmark differ between how you implemented it in storm and how you implemented it in spark streaming
 yeah so give me the big difference was around the database access
 weather in in storm in in Flink because the process is the bolts and spouse or functions or whatever at their long-lived I was able to variable to create a connection to the database leave it up to stream things through we had to make sure that those processing was idempotent so that if we did it multiple times especially for Flink it won't matter and so for read that's not a big deal for the right that's where things got a little bit hairy but it really wasn't that bad so we were looking for exactly once processing and so will you be kind of punch it on that for spark spark is doing micro batch and so it becomes a little bit Harrier with how they do that and so the game we played was to be able to use a function the process is the entire batch in to establish the connection at that point for Paredes
 we just have to read this process the entire micro batch and then we have to reestablish it simply because of how spark works it could have scheduled that prop bet that one bit the processing on a completely different host the next time also cashing that connection really is difficult but on the final output if we were going to do things truly the proper way with sparkly would have had to have written a format that could write into redis
 that was a bit more complicated than we wanted to deal with and so we kind of punch it on it and it's a bit of a half the way we did it but we essentially Road another processing element that would write the data into redis button it wasn't forced it to run and so it wasn't a true output format it wasn't doing things truly the right way but with spark it's essentially you get exactly once processing only there is no way to turn it down and so it is
 that was really the big difference was just being able to understand how those different systems handled the failure cases we could communicate with the database in an actual official play and so one of the consequences of the way that sparkworks cropped up in the results of of your benchmarking and that was it you know what I kind of threw put the hundred thousand messages per second for example you said spark was not able to keep up and you had some modifications and you touch on this little bit earlier but could you just clarify why couldn't spark keep up and what were the alterations that you made in order to allow spark to keep up so back to what I talked about before is with the scheduling that when the batch size is very very small then you get very low latency
 but you also have a set amount of overhead around each batch and so did the bottleneck very much was not in the processing of the data in the scheduling of all those patches as the throughput increased
 the batches are not on a prayer time basis I don't we didn't dig into the exact details of exactly what was going on so I can't answer why is the throughput increased it really start to slow things down that's something we'd like to look into more but increasing the batch size increase the efficiency of being able to do those processing and everything else associated with it and so as we were going along we hit a point where we couldn't process the data fast enough we're falling behind so we increase the batch size and raible to go on until you get kind of a bit of a stair step pattern in the lady sees
 yes you also increased parallelism can you talk about that so we experimented with increasing parallelism so
 hey there a number of different optimizations that these different streaming systems can do and like I said we're storm had a lot of problems with context switching where would hand the data from one friend to another thread to another thread swink and Spark try to avoid this and storm in some cases by taking these different processing and having them happen sequentially seriously on a single thread
 so by default IP systems will look at the parallelism of Kafka does Taco self is a distributed system and they will allocate as many threads as there are threads and Kafka different discs in Kafka that the date is being written to increasing the parallelism within spark to be able to be more than the number of threads and Kaka but that actually had a negative impact simply because we weren't CPU found within the actual processing itself it was the overhead of the scheduling and increasing the parallelism added extra overhead because data had to be sent off to other machines and also because the scheduling became more complex
 okay I understand so we've delved into spark with touched a little bit on storm we haven't really talked too much about flank accept tangentially so let's let's talk a little bit what what is the big draw of Apache Flink and how did you know what are the features that you get from Apache Flink that you don't get from spark streaming or storm so it is kind of position yourself as a stream processing system that can also do batch because if you think about it batch is a subset of stream is just in the Stream ends at some point so you're done
 and I haven't done much with link prior to the Benchmark we were very curious about it it's getting a whole lot of Buzz and we wanted to evaluate whether storm was anywhere in the ballpark compared to what link can do and
 it's not what we needed what steps we need to take to be able to get there or if it was just way off what steps we need to take to move to plank cuz that we're here to provide a streaming service we're not here to provide storm we're here to provide streaming to Yahoo should I really wanted to know what was happening and so slink experiments really provided a very nice Middle Ground we haven't done anything with the patch and so as far as how well it actually performs in batch is up to you to evaluate but from what I read it seems to perform very very well from the street side of things it seem to be able to keep up as far as the latency and to be able to provide very similar through foot and so it definitely seems like a very nice solution but again I am finished finished with our benchmarking one of the things we really were
 to do is to go towards a much
 a much bigger cluster with much more processing going on because we provides a hosted multi-tenant cluster to are our users and are clusters are
 an order of magnitude larger than what we tested with
 and so we wanted to be able to see you want to be able to see how these things handle when there are multiple racks multiple different pieces that are involved there
 so it's definitely an interesting thing I don't think it's quite as mature as storm is especially from an Enterprise standpoint with security and other things at this point but it's it's definitely very interesting technology sure okay so but talking more about the idea of as you said flank looks at batch as a subset of streaming whereas spark looks at streaming as a subset of batch I think I got that correct what is storms point of view on this and if you want I mean so I think you can touch on it on the The crucial advantage of blank is that you can it sounds like you can write your batch jobs in your streaming jobs and get performance out of flank that leverages the fact that the system understands that you're in a batch mode versus a streaming mode is that is that correct
 I don't know exactly all the details of the internals of Link but yes that's that's what they're trying to do okay so what is storms perspective on this and how does you know why can't storm take advantage of the fact that it's you know it in in a given situation says oh I am in a batch processing mode I can leverage I can leverage that knowledge does not provide a batch processing API
 storm was written to be a strain processing system we haven't extended it to be able to do batch and quite honestly I don't really see that much of a need to at least at this point they're all open source project so as the community gets new ideas and decides to go in New Directions that's perfectly fine that's what they do but from my standpoint what I want to do I want to add in features to storm that are really specific just during processing and can make it the best stream processing system out there so for example the Duke has this concept of a distributed cache
 so you can take arbitrary files ship them off to all the different nodes and then you're mad or reduced job or whatever else can take advantage of those they can open up the files use them for something and this is really comment and say doing the map side join so if you want to do a big join this distributed you can either do the shuffle which is expensive or you can ship a very small amount of data to every single mapper and have it look it up in that table as the join streams through very similar things future we put in some extra extensions that are very specific so I can update that data on the fly without ever having to tear down what was there
 and so for me the store but I can concentrate on what makes stream processing great without having to sacrifice something that if there's a trade-off between batch and straining I don't have to make that trade-off it's a streaming-only system that's not necessarily great for all users there are lots of people swear they want to process some massive amount of data use that as part of their string processing and so if you don't have batch with that it's more complicated to know I have to do with two separate systems to be able to unify those
 is badge something that is like are we always going to want to do some form of batch processing on our Big Data Systems or is this like a relic of the past the we're eventually going to move Beyond so what is there two different pieces of latency the people care about when is The Delancey of how long it takes for me to get a question answered and so this is why there's Impala this is why they're all those other wonderful analytics system like Druid and everything else I have a set piece of data and I want to ask lots and lots of questions and dig into that data it doesn't matter that much of the date is a few days old I'm trying to as a human dig into that data and find patterns or find something that actually makes sense and machine learning is is one of those at least most
 learning algorithms to fall into that case to how much can I buy a large piece of data and massive numbers of queries on that day. How fast can I get those queries back where is the other use case is I know what I'm looking for I know exactly what I'm looking for but I need to respond to that date of very very quickly and so I see those two different use cases as being very separate
 you can get some more out of optimizing both of them cuz both of them are some data shows up I want to go very quickly but did I date is the query or whether that data is the actual date of your doing you get some very similar things between those two but they are very distinct use cases so okay we bounced through this discussion of these different systems I'd like to get an idea of what the conclusions that you took away from this benchmarking exercise were
 the first conclusion was the store was actually doing fairly well I was a bit nervous about that just because it is several years old has been getting depressed and storm beef Lincoln latency which I was quite happy with the other thing that I took away is that my performance optimizations which amazing things are actually kind of crappy on a real world use case that we ran both prior to my performance optimizations and then after my performance optimizations and I did all kinds of things that when I ran was word count more than doubled and drop the latency and on this particular use case did not go up one lick the latency did drop slightly and that was good but the actual throughput that was one of my goals then to actually make much of a difference which is another reason why benchmarks are grateful
 I optimize the wrong thing apparently I spent a bunch of time I got it to work really really well and well now worked out you can go very fast with storm but you can't necessarily by any means but it wasn't necessarily the best use of my time
 he was technically expectedly spectraflame to be fast because it was a streaming system and we expected spark to have lower Lanes used but have decent throughput and that was what we we saw
 yeah I find it funny that it's like you know you create is Benchmark and it's just like as controlled an experiment as you can have and it's still got all these kind of caveat sand like it did we actually accomplish what we wanted to and you know additional questions raised at the end of it maybe said something about the nature of distributed systems systems to all of these we tried to run them with the out-of-the-box configuration and since then data Artisans which runs Flink has given us a pull request to try and improve the performance and drop the latency a little bit we've talked with people in spark and say that the things were awesome and not really what we wanted that what they wouldn't have like preferrably and so they're all kinds of the small little things
 you can't we can and get much better performance potentially out of but as a normal user who doesn't
 doesn't even deathly into distributed systems and become an expert in these things out of the box felt like the best way to start out with spark I'm sorry storm was kind of the it seems like the winner in this situation would probably have the best results Yahoo has a lot of experience working with storm is there any chance that the positive results from Storm were due to implementation expertise and an understanding of how the system works of course this is not unbiased we try to do things in a very unbiased way but these things will creeping and so that's why we released it is open-source we really want people to give us feedback cuz we're not the experts
 absolutely everything no spark really well we know storm really well sleep was new to us we want to explore all those other systems by Sam Apex and whatever else they might be out there but we don't have the expertise or the time to be able to do all of that yeah you know I don't think you're ever going to get a job as a experimenter at a pharmaceutical company cuz you're you're pretty honest with your latest biases
 it's not actually the three or four different things in a test to see if it kills the bacteria individually individually will crap shoot yeah I understand yeah so what's been the response from the different open-source communities for the different project since you publish the findings I think it's been fairly positive I haven't heard much from spark as you might expect flinches actually how did it a fair amount I've seen them retweet things quite a bit done feedback from both communities though on how to tune the systems Apache Apex some people from that Community came over and offered to give us a
 there are to help develop a version of The Benchmark for that and so I think it's been very good I hope it continues to keep it up that as new versions of these tools are released we want to be able to keep running it and seeing how things change so this lack of attention from the spark Community Park is kind of more at least from like a fundamental use case perspective it's more for the data scientist who needs to load a working set into memory and then do these different operations are working set which seems somewhat orthogonal to to a streaming methodology that will do will do
 Exploration with it some transform link type processing mode processing
 nothing on the streaming site we had a few people play around with it for streaming but not that much
 in is that is that kind of because well so is there is there a reframing that that may need to take place for for spark streaming to kind of fit both of use cases are at Jimmy do you get the sense that spark streaming is trying to fit a square peg in a round hole or are there are there kinds of synergies that you could see emerging from the way that spark does things to to have a good used case for spark streaming it's all about the light like I said if you're doing something where your latency can be 10 seconds
 Sports training is a fabulous solution especially if you're using Sparkle ready you don't have to learn something new you don't have to deploy something new it works really well at 10 22nd Lane C's
 you need something to sub second spark is not going to be able to handle it now. That's in its current form I know a lot of people I date at date of bricks Anna Cloudera and a map are in other places that are throwing look at IBM and how many people they're throwing a spark if they really want to get to the point that they have these these some II Lanes actually is going to be very different difficult to do with what they have right now but you throwing us more people at it I'm sure they can come up with something and so
 by any means but yeah I don't know how much of their bread and butter comes from that at this point either okay super interesting question how will the world of streaming Frameworks evolve going into the future
 I think a lot of it is around ease-of-use and adding in some new Concepts natively so Google has been doing processing for a very long time and you look at their Cloud dataflow API you can tell from that they have really thought through how you stream processing should work one of the really difficult pieces with stream processing is late data in the real world all your data is coming in a batch but the first event to arrive was not necessarily the first event produced things happen out of order and so being able to get really truly good accurate results in a very fast way that deals with this out of order data is a difficult problem has been trying to tackle that
 storm has been trying to tackle that spark has been trying to tackle that I think that's kind of one of the very interesting pieces that that we all have to grow up a little sleepy eyes and after that a lot of it is around maturity
 it all distributed systems getting visibility and what's going on is very difficult in batch you always have the know if something goes wrong it's not the end of the world because I can rerun it on another machines or I can just wait until that batch is done and try again the next time is very Define computation stream processing systems like storm in Apex
 you don't get that that that that piece is still long-lived it's not going away at any point and so you have to be absolutely positive that the scheduling was right if you want to have very good performance you have to be able to get really good visibility in what's happening at that point and
 can get very expensive
 expensive situation and Yahoo I like to say that we have a big meditative problem that one of our biggest problems is dealing with logs what logs we keep wood logs me throw away lots of people when they're debugging something want to log every single event that came in on a map reduce you can kind of do that erase the local desk and then I can look at it stream processing That Never Ends how much data do I keep how do I provide people good visibility in Spanish becomes very difficult to be able to deal with all that provide that in a good clean way so I think there's a lot of maturity that needs to come simply from the Enterprise standpoint productive does ability maintainability
 security all kinds of things around that
 cool well that that sounds like a great conclusion Bobby Evans thanks for coming on software engineering daily it's been great talking to you I I really like that blog post and you know if you guys have any other interesting findings in the future feel free to come back on
