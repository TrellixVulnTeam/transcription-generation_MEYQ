Transcription: in a typical data analytics system there are variety of Technologies interacting hdfs for storing files spark for distributed machine learning pandas for data analysis in Python each of these different Technologies has a different format for how data is represented and there are plenty of other data formats represented in this ecosystem as well serialization and deserialization between these different formats causes significant latency across the overall system Apache arrow is a tool for improving performance of in-memory analytics systems and today's guest you corn explains how Arrow enables these systems with interoperability that adds a lot of speed this show goes into the weeds of serialization and deserialization in these types of systems and how data is represented it's a very good show for somebody who wants to get an in-depth understanding
how this big data ecosystem represents data and how things can get sped up
 if you're interested in subscribing to the software engineering daily newsletter you can go to software engineering daily. Com also if you're interested in collaborating if you want to host a show if you want to get involved in contributing to the outlines and I prepare for the shows you can click on the link to collaborate on software engineering daily.com on the website you can also find links to the slack channel to my Twitter and to my email address software engineering daily at gmail.com I would love to get your feedback and with that let's get to this episode about Apache arrow and data representation in Big Data Systems after a quick message from today's sponsor
 if you want to learn about kubernetes go to aprender.com se daily today you can find a webinar about kubernetes covering microservices persistence and multiple clusters you can also find links to a past webinar covering of the history the origins and the technical architecture of kubernetes the webinars at aprender.com se daily are hosted by Joseph Jack's a member of the cloud made of computing Foundation who was previously a guest on software engineering daily if you're looking to move to a cloud native infrastructure platform that works with your existing apps you should also check out a printer. Com SE daily aprenda provides Cloud platform software that works with your existing applications and infrastructure so very lucky to migrate to Cloud native what you want to maintain your existing applications you might want to check out kubernetes and Brenda thanks to aprenda for being a sponsor of software engineering daily
 do you want to learn about kubernetes while supporting software engineering daily go to aprender.com se daily that's a p p r e n d a. Com SE daily thanks again aprenda
 who pakorn is a contributor to Apache Arrow a tool for improving performance of in-memory analytics systems welcome to software engineering daily
 so I want to start by talking about two types of data processing row processing and column processing and we'll have an overview of just kind of the processing landscape and eventually will get into discussing what Arrow does but let's start by defining these two types of processing row processing and call it processing
 repressing if the typically used in a black marker so where you look at that table of information where you have Collins pipe and row switch all the time to get home I have information to say meditate and example of constant of the block the title of the blog how many comments as it has where is in, you're more interested in seeing what you want to call him like you want to sell a all blog post on the same date or a really large chunk of users
 right and so colander processing has become more prominent in the last 10 years as we've gotten more more aggregations of giant amounts of data whether we're talking about coyote or weather data or aggregation of social media data just High volumes of metrics where we want to do a big aggregation or the average or find the minimum of one specific column in a database so like in your blog example maybe we have an extremely popular blog and we want to do analysis on all of the comments all of the text fields of all the comments you have to aggregate all the data from just that column what are the challenges of that columnar processing what are the canonical challenges of colander processing the we've encountered over the last 10 or 15 years since her
 do Pez knighted this big data Revolution
 set the main problem you don't want to have too many that you just interested if you want to do any programs that you only access to single column and it's not like there is no Standard whey a vacation day tomorrow and how to pass it on to another application
 okay so when I load a data set into memory with a data analysis tool like apache-spark how is the data set typically arranged if I'm if I if I want to do some columnar processing in spark for example how is that data arranged and how might that differ from other systems that I might pull it into memory with
 because you want to do operations on each of the scollins anomaly
 assorted memory of John Corbett junk but damn it's on its own and Central like hashmap or dictionary right
 why you have the name of the call him and appointed location memory where the Columbus store
 okay and so the weird thing is the way that data is typically represented in memory is row wise so if you have some if you have like a bunch of Rose where you have strings in memory interspersed with some integers and you want to only process the integers this is a problem to explain why that's a problem and how and how the system might do columnar processing despite this row wise representation
 call mi princesa on this item and only look at even though if you have code Rhythm the taxes this year in each row yes if you normally go at once and then don't belong to call them because Chick-fil-A at CPU will fetch not the actual Foster agency so even though you in the background your CPU already fix a lot more memory at you just waiting for this moment to arrive and for each next to you have to wait again the latency to get the road again
 a contrast to that if you already had it call nice in memory
 did Shakira tour with the value for the first row but also would be faxed to the valley for the next road which also we get you a significant speed up
 okay so just to recap the data is represented in memory stupidly represented memory in a row wise fashion and if you have a bunch of strings interspersed with integers the processing system is going to have to step over if it wants to get all the integers in all in these collection of Rosie of loaded into memory it has to step over the strings in order to analyze each of these integers could you talk in more detail about how much I like how does this slow down the CPU how much are what kind of penalties are we paying in order to do this cottbus columnar processing on row wise representations
 Freddy huge number of CPU Cycles which M number to get if you do simple operation to see if you would take one or two CPU Cycles but just waiting for hundreds of thousands of CPU sockets why you waiting and just wasting CPU time so
 okay and what's going on in the other let's talk a little bit more about the the interaction between main memory and the CPU cache because the kids were getting an arrow that the the difference between main memory in the CPU cache is going to be important how does information typically get from Main memory and into the cache of the CPU normally after think when you touch something for memory starts with an instruction to see if you would say so K I want this from memory is okay and it stinks probably because I give you many more fights at once
 but only at the cost of latency of one memory request an SS kind of fits into the registers of the CPU your CPU will start but if you have access to his memory locations all trees neighbors is already in the cash and you don't have to go to memory again
 and how does the speed of main memory compared to the speed of the CPU cache
 I can't exactly how much for a significant it's like a hundred times faster so we really want our data in the CPU cache
 yeah okay so with that said what is Apache Arrow
 Apache error in its Essence is La specification how your store, dated memory and unified but
 what's up applicable to a lot of applications
 just started, Dayton memory care package already comes with kind of implementation for Java and 5658 on disk and have a simple way to integrate this into your application
 and just a recap for people who still maybe a little confused what problem is Apache Arrow solving
 it's fucking mainly the problem of integration of two different call my BAE Systems because I don't know each of their systems has its own which is normally but not exactly the same system if you want to Fox's Brooklyn by providing a provider, and you can pass along today that without any of that
 could you give an example of a pair of systems that would want to be utilizing the same data and contrast what they would have to do without Arrow versus what they can do with arrow providing interoperability
 example for this is if using apache-spark and python Library pandas at the moment apache-spark has its name which is different to that of pandas and did you also have separation that one is running in the stadium and the other one is running mate is code city running a different memory specifications and directions to need those two and integration of a griffin CNN stock futures kind of
 performance penalty you got free T3 code we would have a common Spanish house Spa
 even started trying to remember me or you just like it's memory soda kind of screwed already use it and didn't have to convert from python objects to Native objects to type of spam
 so you're talking about the serialization and deserialization penalty that you have to pay in order to two commonly share these these objects data around and change the metadata for different applications
 what kind of implementations does does pandas have to actually have a quick and what is spark and why are why is this a canonical example how do these two things together to be complementary
 Apache spark is a distributed system for executing computation on large data sets and under contrite pandas kind of thing the same but it's not distributed it gets the words together with scikit-learn machine-learning python really fast native back on and it's very popular because you can just easily startled and
 your actions are going to write a Griffin's to both it's kind of nice that you write down the pandas and then just hopefully integrate that part into spark executed directly and
 don't have to have the overhead to reap lamenting at the same spot just to get it in an honest performance Beast
 what has been how much work does a programmer have to do to integrate pandas and Spark today is there is there significant work or is it more than that the work is not the very hard it's just a latency penalty or Pang
 remember to work some Fosters already in Japan dysfunction Spock and the problem is just that simple
 does this case instructive cheer like they keep healthy
 okay so when you're talking about getting a harmonious interoperability between pandas and Spark for example what kinds of implementation details do those involved in the respective systems
 introspective systems you would have to look at your facial structure is maybe already doing that to make us back that normally match then you need to mislead you just need to pass that along to our library in a case for your stock just I'm not exactly the same. They need to make some friends on missions which have your application to Aaron
 if there's no overhead for you to implement you can just block them together and you only have to do this once and up for FBI application
 okay so arrow is this this interoperability between a variety of different systems do each of these different give some more examples of different systems debt things like things like pandas and Spark would want to integrate with
 calendar systems Wichita, date of Life stl100 engines like Impala or databases like they have two faces which you can use language they want to have to take care of to get the cute kid Friday's libraries to understand how to make a phone stand up in comparison to how they do it internally
 Cher now talking about data storage as well as data processing how is data typically stored on disk in a big data processing system
 I'm typically if you just looking at like 5 from which era stop it it's also really the default files to. Call me later and Becky and it's quite simple but also if you look at the old store
 Eva memory on this in Khulna formant because the other team just call my rice processing
 show data in Daisy Duke ecosystem is often stored in hdfs you're saying in a binary format like parquet or a text format like CSV it's stored in a file or perhaps it could be stored in an online storage system like Apache Cassandra or hbase or kudu these online databases if we're ignoring it looks a little ignore arrow for a second just just to give people idea of the status quo when a computation engine likes park or Impala requests data from one of these storage systems what happens
 Timothy what's happens is the storage system it's at which they could you need to stay to enter memory with to suffer together with the uses of the software mind
 Traxxas Aton 2 weeks API and into a pie or driver maybe this data structures transferred to the one which is retrieving today. Which is this could be apache-spark
 okay and when wet stored in the format like parquet or a format like CSV how does the loading into the beach and memory systems how does that go is it is it complicated to do a translation from the on disk format of something like parquet or something like CSV into the internal storage into the internal in-memory representation and something likes park
 disappearing from like pocket it's not very hot today because he already has an ocean of call him later and you already have somebody like me and my stations in parque today to own this store similar equivalent like it is stored in memory for solution using contrast shisui's really more complicated because we have call me later but seriously is a revised Foreman
 so we need to read the state of Rhode wise determine its type of email on the Fly and have to adjust out of the structures in memory and decides to sleep so sweet potato road by road and we don't know beforehand how much do we have to read
 why do we need Arrow when couldn't we just store the could we just have the data in memory in the same format as parquet
 okay have some optimization so it's really good on disk in there is you can store it in my phone which is similar to error it also has some to my Sation to start a bit efficiently hidden codes for example strings with a dictionary and it only starts at 8 sorry I have to file and and specifies codes in to check codes for it stinks that stinks. I hate this is really good if you do doing a scan over the data or start restoring it from you if you do run a Nexus on this file to the dictionary Orchard index pick up the jump back to the dictionary look what is this thing for the integer and you would have to jump front for from the beginning to the back and the computation on this really in a fishing resident
 Arrow you already have call my data which is stored in memory so you can have access to the data
 okay so with that said what is being done to 2m to give these systems is different systems like spark or pandas what is being done to give them the ability to have data represented in the arrow format rather than something else
 for example of tennis real relationship and I've never laid out on the converter that converts so we just copying like 2.2 today though but just 40 some to code which just wants one or two pieces where they test the same that's a bit of conversation of civilization
 yeah she don't want me just need to implement like the pointer Studebaker and pass along their emitted
 how much does the programmer have to know about Apache Arrow
 how to format a specified as long as it may be some jvm to make of coke and contrast development was just using Spa passenger ejected from Brunswick to Annapolis first speech as part of the user
 wealthfront is an automated investing services that saves you time as an engineer your time is valuable if you want to invest but you don't want to spend significant time evaluating your portfolio and allocating assets take a look at wealthfront go to wealthfront.com se daily to open an account today and get a special offer for software engineering daily listeners $15,000 manage for free when you open your account with traditional Investment Services there are many humans in the loop that are doing things that well front automate away when you pay the commission's an account fees of these traditional Investment Services you're paying for work that could be done by a computer so don't pay commissions and account fees maximize your gains with wealthfront set-it-and-forget-it investment automation check out well front.com SE daily it would support software engineering daily and it would introduce you to the world of automated investing
 wealthfront.com SE daily thanks to wealthfront for being a continue to sponsor of the show now let's get on with the show
 as a data scientist which you are how does your workflow change how does your workflow improve when you have these systems that are enabled by Apache Arrow
 the thing is it's really hot tea estate system to another at the typical choice is an atheist, disc format today at the office system which is really expensive because those two programs F1 private which passes it from Houston to another but you still need to have the separate code of each pair of assistance which process data from one system to another
 with arrow does that mean that on the same box you're going to do some spark processing in the data is going to be passed from the memory of spark to something like pandas or how exactly does that work
 ask Candace and spark on a personal assistant
 soda Confidant in bumper stickers but it's really not easy but there's already some implementation in the airport check which temp has data on the same machine from one Princess to the oven without coughing
 and then spot versus would write its take on a specified which and just can't take on this memory and work on that so that there's a from Austin right to enter SO2
 that seems pretty cool does that mean that you get to have minor changes that it in the past or in the present perhaps you have to have different machines that are dedicated to running these different types of processing systems or maybe you can tell me that's wrong does does Arrow end up cutting down on the number of machines you have running because you need after your machines dedicated to specific systems
 arrow is not cutting down on the number of machine city of them you have to run it on the same system at the moment it's just cutting down on the overhead which you have with weapons list of different systems on one machine
 okay
 up to in memory data representation has been a desirable goal for a while and each system has been handling it differently why has it taken a while to get something like Arrow going
 I think it's this tree out of the leads to that you always have to write driver's panel two systems to gather a systems the most you have the most distance to support if you make any system and its already 4 months to each pile of work you have to do if you want to have better Education Office systems and it's got to be patient with them for free and start that you have to write your own try if I can
 what I keep thinking about is the fact that
 each of these different systems whether it's spark or pandas or storm has to implement the functionality to have the data represented in Apache Arrow
 how laborious of a task is it to implement the specific formatting and representation for each of these different systems
 it really depends on the type of the system like for my example and it's really really Construction Specialties maybe like you have a difference between representation and I need to change the Sphinx but typically because you already have so many representations how you could make it calling wife when you're eating meat just till I have the right coach to passivate that wall
 so prior to Arrow was the only way that data got moved between these different systems the process of you serialize it you write it to desk and then the other system that wants to consume it pulls it from disc deserialize is it and converted into its own format that were doing it a memory
 how to convert a memory loss and it's not always fixed like maybe you're already systems which have the same structure that's easy to pass along the system you have to say memory representation as an APA system while maybe later on
 so you have to write so that your data always
 the ecosystem is mostly java-based and Utah shuns little bit Arrow improves interoperability with languages like R and python explain how it improves that interoperability and why that's so important
 this is reading public assistance which scientist use if they looking at small or medium which can fit in your memory on one machine Dallas and two tickets. It's one of those jbims tools
 5 Arrow to see the freedom to you still use those to come into the overhead and
 devil
 we have to have kind of code that supports Mary management between jvm and native code because of the moment they tracked them and this is our life we will handle if you gym that you kind of memory which tap has the ball between those two
 what are some of the other advantages of this interoperability
 the overall interoperability of Arrow like just you know I understand at this point Arrow provides a vast number of interoperability features but what are the other macro advantages of disinter operability
 it's already this is a chance to have like a tree starting with a new software and you're not sure why you're already executed
 that's kind of like yo if you're starting a implemented and you're unsure if you going to run it in stock or if you just using python if you can get you in future to benefit did you confirm that it forms and I sent just wanted one of those two two systems isn't that it's a memory
 most of the data that we deal with today is not perfectly formatted sequel data it's likely to be some denormalized Jason and we might have you know we might have Jason log data from one system that we're riding over here like what they were writing weather data in one system and the in system a and then we're riding weather data in system b and a slightly different Json format and then we might want to aggregate the data between these two Json format despite the fact that they're not normalized so they might be on disc in a format that that's not normalized what are some challenges of dealing with
 building an in-memory columnar processing format for this type of data
 what is type of datum the basic thing you need to have is that you're supposed to be kind of nesting does Jason kind of tree like stir structure
 kind of cooked vegetables that you can come from one phone to another phone case then you can use the same facial structures or different different different memory presentation between those two formats then yet you have the right to vote
 okay
 show Arrow emerged out of the Apache drill project could you share some history behind that and how Arrow came out of that project
 I'm sadly not Sitting Bull in that but all I could say that they're looking Heidi could integrate all the languages into the system and have support functions does Apple languages needed to look for Hammond
 informant how to cook pasta days from Boston to Napa
 let's talk a little bit more about the lower-level internals of this we touched on this little bit in the beginning
 explain now that we've had more overview how is in the current systems that are not using Arrow why is the CPU being underutilized if you end up using ever and you having like no. If they had called and told his beginning and then you have to wait a lot longer time until they take extra memory to your CPU because you're reading a lot more dated any use or you just have to wait malls in front of main memory because he just fell off my memory access and
 okay and modern processors have an operation type called simdi which is a single input multiple data could you explain what SIM di is and how Arrow helps just take advantage of that likely be taking the same time as we both just doing on one while you important for this to start light included in the continuous action you shouldn't be any gaps with me today then you just load which is hopefully already in your past your computer and start a restart of that again in the country music
 okay and what is what are the steps that you need to take in order to be able to take advantage of Cindy when you're writing Arrow
 basic things just rights do they tell me another one
 okay and could you talk more about how Aero improves the relationship between main memory and the CPU cache and how Arrow encourages more than enough cash finity is the right word more data in the CPU cache that you actually want to be accessing
 Heritage Define segue Heights. Call me later if you have the right alignment so that is starting at 6 if you want and like symptoms can only meet on instructions don't meet too many exes II Men in one CPU cycle or one starts of a CPU and whichever.
 nothing much happening but that's also a bit of the ascent you have a simple memory access and Anna CPU can use all its internal parts just how are different companies contributing to this project in different organizations like what is the status of the arrow open source community
 just communities quite some of it already because it's not really a way you could already use it in the stable so it's the chance to get to have it found like at the moment but if there is
 what does small number in worked at the moment already some sort of laying Cinder which I have pot of really big large
 companies and Country videos to
 what are the other types of real-world applications that Arrow will help speed up
 I'm just actually already one kind of we would application wherever is used for that which is which country area that carries the fight for March 7th from West McKinney which is used to have Faust interchange between our and Tyson
 distinctive form at speeds up and wait for the state of friends to take the data exactly so you can load it
 the spot really quick interchange between those two systems like loading and saving experience without having to wait on civilization overhead or being limited by the CPU time it takes to write to format
 okay
 what are the further reaching implications of Apache Arrow what are the future systems that it will enable
 the future difference between the littlest typically the use of like a difference back they started taking application to Applications but she's calling us data but you don't have to think about so much about how he just writes a rapid rate to spike to compress take that long
 okay well what what aspects of Arrow are you working on today I may be working on and support us because my point I'm so I want to have to have to pass it along to the Embassy or cat in Jupiter and workflow without having to go dancing to jvm even though I'm bad with our you can use a novice Tech to pass along any time to work for you
 Porta-Potty part it's just really interesting to have a good format where you can start last time and you have to read like fast way to get data from This to Memory in Pakistan
 okay can you talk more about what is involved in writing that
 at the most things you need to care about your is Jack if it matches that matches the spec an hour if it matches the I respect we can just pass along the daytime if it take care of that like doing a conversion so that you can pass beta from Pike Road and back again you have to look at you do with least amount of data to duplication in memory and efficiently so that you see if you can read it passes a law in converted really fast it's already very costly and you want to keep it to a minimum
 so we're talking here about the CPU bottleneck that is the the the problem that arrow is ultimately solving what are the other bottlenecks that exist in the ecosystem
 which kind of dressing is the CPU to memory which can become quite significant if you have a lot of you and Back Again
 if you having addresses to people but afterwards it
 normally discomfort at work why you have to stay around
 what is the finished product of Arrow going to look like and how far are we from from having everything that needs to be implemented for this project to be a success implemented at the finished product which episode of Code which is in code so you can use from java
 the biggest conversion which means to the typical life is already have like from turn on pie or two and also the coach that you can pass along when we start in the jvm to native code or memory stored in one person to another process hair salon image of metadata see the one receiving versus knows where to access the memory
 fantus also made important part that tested so you can be sure that when you pass memory from the jvm to make his coaches wear work
 turo wise and columnar wise representations might seem like these are the only types of representations that we're going to want to have for our big data workloads but I can imagine something like tensorflow perhaps
 standardizing or changing in your changing how we look at RR workloads such that we would want an entirely new format of of of representation do you think that's plausible and an how would we how would we adapt to do I should just have to do you think that's possible it's like having to work sample that you have data stored in memory which is not an air compressor on killed his phone we have frost on a Nexus
 affects how you can start the memory of a rifle called for fast and the Nexus that's because we didn't hear what kind of like the 2D table but our memory is Linnea so we'd have some form of linear transformation for contrast of applications for you have two large matrices which a spot maybe just a good information just to store data points from XD at the value minutes. Start a large chunk of mouth
 for the point of Arrow will most interested in Baytown without any compression are there any types of workloads where arrow is not a great fit
 tickle you state to hide from the beginning like with a raffle and then yeah but they have eyes in the back at which store do they have drove by because otherwise you would have to patch a lot of columns and a lot of things about it just kind of free space, Texas not really fitting fitting
 cool thanks for coming on the show I think this is been a great overview of the Apache Arrow project is there anything else you'd like to say about the project in conclusion
 a condition where where to get state where we just finding out if we're having starting
 imitations going on what in Java and it's code and the thing is not we're still not there it's not really good points to have a look at it and see how it matches your work late and maybe help us because now it's the time where we just need to the finish line for a specification
 great well Uber thanks for coming on the show I appreciate it
 thanks to sinfonico for sponsoring software engineering daily symphonica is a custom engineering shop where senior Engineers tackled big Tech challenges while learning from each other check it out its symphony.com SE daily that's s y m p h o n o. Com SE daily thanks again siphano
